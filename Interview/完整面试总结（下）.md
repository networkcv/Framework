https://juejin.cn/post/7049177505671938056

# **五、Redis篇** 

### WhyRedis

​		速度快，完全基于内存，使用C语言实现，网络层使用epoll解决高并发问题，单线程模型避免了不必要的上下文切换及竞争条件；

|        | GuavaCache  | Tair       | EVCache      | Aerospike         |
| ------ | ----------- | ---------- | ------------ | ----------------- |
| 类别   | 本地JVM缓存 | 分布式缓存 | 分布式缓存   | 分布式nosql数据库 |
| 应用   | 本地缓存    | 淘宝       | Netflix、AWS | 广告              |
| 性能   | 非常高      | 较高       | 很高         | 较高              |
| 持久化 | 无          | 有         | 有           | 有                |
| 集群   | 无          | 灵活配置   | 有           | 自动扩容          |

​		与传统数据库不同的是 Redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。另外，Redis 也经常用来做分布式锁。除此之外，Redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。

#### 1、简单高效

​		1）完全基于内存，绝大部分请求是纯粹的内存操作。数据存在内存中，类似于 HashMap，查找和操作的时间复杂度都是O(1)；

​		2）数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的；

​		3）采用单线程，避免了多线程不必要的上下文切换和竞争条件，不存在加锁释放锁操作，减少了因为锁竞争导致的性能消耗；（6.0以后多线程）

​		4）使用EPOLL多路 I/O 复用模型，非阻塞 IO；

​		5）使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；



#### 2、Memcache

| redis                                 | Memcached                  |
| ------------------------------------- | -------------------------- |
| 内存高速数据库                        | 高性能分布式内存缓存数据库 |
| 支持hash、list、set、zset、string结构 | 只支持key-value结构        |
| 将大部分数据放到内存                  | 全部数据放到内存中         |
| 支持持久化、主从复制备份              | 不支持数据持久化及数据备份 |
| 数据丢失可通过AOF恢复                 | 挂掉后，数据不可恢复       |
| 单线程（2~4万TPS）                    | 多线程（20-40万TPS）       |

**使用场景：**

​	1、如果有持久方面的需求或对数据类型和处理有要求的应该选择redis。 
​	2、如果简单的key/value 存储应该选择memcached。	



#### 3、Tair

​	Tair(Taobao Pair)是淘宝开发的分布式Key-Value存储引擎，既可以做缓存也可以做数据源（三种引擎切换）

- MDB（Memcache）属于内存型产品,支持kv和类hashMap结构,性能最优
- RDB（Redis）支持List.Set.Zset等复杂的数据结构,性能次之,可提供缓存和持久化存储两种模式
- LDB（levelDB）属于持久化产品,支持kv和类hashmap结构,性能较前两者稍低,但持久化可靠性最高

**分布式缓存**

大访问少量临时数据的存储（kb左右）

用于缓存，降低对后端数据库的访问压力

session场景

高速访问某些数据结构的应用和计算（rdb）

**数据源存储**

快速读取数据（fdb）

持续大数据量的存入读取（ldb），交易快照

高频度的更新读取（ldb），库存

**痛点**：redis集群中，想借用缓存资源必须得指明redis服务器地址去要。这就增加了程序的维护复杂度。因为redis服务器很可能是需要频繁变动的。所以人家淘宝就想啊，为什么不能像操作分布式数据库或者hadoop那样。增加一个中央节点，让他去代理所有事情。在tair中程序只要跟tair中心节点交互就OK了。同时tair里还有配置服务器概念。又免去了像操作hadoop那样，还得每台hadoop一套一模一样配置文件。改配置文件得整个集群都跟着改。





#### 4、Guava

​		分布式缓存一致性更好一点，用于集群环境下多节点使用同一份缓存的情况；有网络IO，吞吐率与缓存的数据大小有较大关系；

​		本地缓存非常高效，本地缓存会占用堆内存，影响垃圾回收、影响系统性能。

**本地缓存设计：**

​		以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况，每个实例都需要各自保存一份缓存，缓存不具有一致性。

**解决缓存过期：**

​	1、将缓存过期时间调为永久

​	2、将缓存失效时间分散开，不要将缓存时间长度都设置成一样；比如我们可以在原有的失效时间基础上增加一个随机值，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。

**解决内存溢出：**

​	**第一步**，修改JVM启动参数，直接增加内存。(-Xms，-Xmx参数一定不要忘记加。)

　**第二步**，检查错误日志，查看“OutOfMemory”错误前是否有其它异常或错误。

　**第三步**，对代码进行走查和分析，找出可能发生内存溢出的位置。



**Google Guava Cache** 

**自己设计本地缓存痛点：**

- 不能按照一定的规则淘汰数据，如 LRU，LFU，FIFO 等。
- 清除数据时的回调通知
- 并发处理能力差，针对并发可以使用CurrentHashMap，但缓存的其他功能需要自行实现
- 缓存过期处理，缓存数据加载刷新等都需要手工实现

**Guava Cache 的场景：**

- 对性能有非常高的要求
- 不经常变化，占用内存不大
- 有访问整个集合的需求
- 数据允许不实时一致

**Guava Cache 的优势**：

- 缓存过期和淘汰机制

在GuavaCache中可以设置Key的过期时间，包括访问过期和创建过期。GuavaCache在缓存容量达到指定大小时，采用LRU的方式，将不常使用的键值从Cache中删除

- 并发处理能力

GuavaCache类似CurrentHashMap，是线程安全的。提供了设置并发级别的api，使得缓存支持并发的写入和读取，采用分离锁机制，分离锁能够减小锁力度，提升并发能力，分离锁是分拆锁定，把一个集合看分成若干partition, 每个partiton一把锁。更新锁定

- 防止缓存击穿

一般情况下，在缓存中查询某个key，如果不存在，则查源数据，并回填缓存。（Cache Aside Pattern）在高并发下会出现，多次查源并重复回填缓存，可能会造成源的宕机（DB），性能下降 GuavaCache可以在CacheLoader的load方法中加以控制，对同一个key，只让一个请求去读源并回填缓存，其他请求阻塞等待。（相当于集成数据源，方便用户使用）

- 监控缓存加载/命中情况

统计

**问题：**

​	OOM->设置过期时间、使用弱引用、配置过期策略



#### 5、EVCache

EVCache是一个Netflflix（网飞）公司开源、快速的分布式缓存，是基于Memcached的内存存储实现的，用以构建超大容量、高性能、低延时、跨区域的全球可用的缓存数据层。

E：Ephemeral：数据存储是短暂的，有自身的存活时间

V：Volatile：数据可以在任何时候消失

EVCache典型地适合对强一致性没有必须要求的场合

典型用例：Netflflix向用户推荐用户感兴趣的电影

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmapdnh0yaj30ku0aigmc.jpg" alt="image-20210103185340548" style="zoom:50%;" />

**EVCache集群**在峰值每秒可以处理**200kb**的请求，

Netflflix生产系统中部署的EVCache经常要处理超过**每秒3000万个**请求，存储数十亿个对象，

跨数千台memcached服务器。整个EVCache集群**每天处理近2万亿个**请求。

EVCache集群响应平均延时大约是1-5毫秒，最多不会超过20毫秒。

EVCache集群的缓存命中率在99%左右。

**典型部署**

EVCache 是线性扩展的，可以在一分钟之内完成扩容，在几分钟之内完成负载均衡和缓存预热。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmapg99q8lj30ix0f3jrw.jpg" alt="image-20210103185611516" style="zoom:50%;" />

1、集群启动时，EVCache向服务注册中心（Zookeeper、Eureka）注册各个实例

2、在web应用启动时，查询命名服务中的EVCache服务器列表，并建立连接。

3、客户端通过key使用一致性hash算法，将数据分片到集群上。



#### 6、ETCD

​	**和Zookeeper一样，CP模型追求数据一致性，**越来越多的系统开始用它保存关键数据。比如，秒杀系统经常用它**保存各节点信**息，以便控制消费 MQ 的服务数量。还有些业务系统的**配置数据**，也会通过 etcd 实时同步给业务系统的各节点，比如，秒杀管理后台会使用 etcd 将秒杀活动的**配置数据实时同步给秒杀 API 服务各节点**。

![image-20210418174251742](https://tva1.sinaimg.cn/large/008i3skNly1gx2tg9qs00j30ta0g40vc.jpg)



### Redis底层

#### 1、redis数据类型

| 类型   | 底层      | 应用场景                                       | 编码类型              |
| ------ | --------- | ---------------------------------------------- | --------------------- |
| String | SDS数组   | 帖子、评论、热点数据、输入缓冲                 | RAW << EMBSTR << INT  |
| List   | QuickList | 评论列表、商品列表、发布与订阅、慢查询、监视器 | LINKEDLIST << ZIPLIST |
| Set    | intSet    | 适合交集、并集、查集操作，例如朋友关系         | HT << INSET           |
| Zset   | 跳跃表    | 去重后排序，适合排名场景                       | SKIPLIST << ZIPLIST   |
| Hash   | 哈希      | 结构化数据，比如存储对象                       | HT << ZIPLIST         |
| Stream | 紧凑列表  | 消息队列                                       |                       |



#### **2、相关API**

> http://redisdoc.com

|        |       |        |           |       |         |        |          |       |           |
| ------ | ----- | ------ | --------- | ----- | ------- | ------ | -------- | ----- | --------- |
| String | SET   | SETNX  | SETEX     | GET   | GETSET  | INCR   | DECR     | MSET  | MGET      |
| Hash   | HSET  | HSETNX | HGET      | HDEL  | HLEN    | HMSET  | HMGET    | HKEYS | HGETALL   |
| LIST   | LPUSH | LPOP   | RPUSH     | RPOP  | LINDEX  | LREM   | LRANGE   | LLEN  | RPOPLPUSH |
| ZSET   | ZADD  | ZREM   | ZSCORE    | ZCARD | ZRANGE  | ZRANK  | ZREVRANK |       | ZREVRANGE |
| SET    | SADD  | SREM   | SISMEMBER | SCARD | SINTER  | SUNION | SDIFF    | SPOP  | SMEMBERS  |
| 事务   | MULTI | EXEC   | DISCARD   | WATCH | UNWATCH |        |          |       |           |



#### 3、redis底层结构

**SDS数组结构**，用于存储字符串和整型数据及输入缓冲。

```java
struct sdshdr{ 
  int len;//记录buf数组中已使用字节的数量 
  int free; //记录 buf 数组中未使用字节的数量 
  char buf[];//字符数组，用于保存字符串
}
```

**跳跃表**：将有序链表中的部分节点分层，每一层都是一个有序链表。

​	1、可以快速查找到需要的节点 O(logn) ，额外存储了一倍的空间

​	2、可以在O(1)的时间复杂度下，快速获得跳跃表的头节点、尾结点、长度和高度。			

**字典dict:** 又称散列表(hash)，是用来存储键值对的一种数据结构。 

​	Redis整个数据库是用字典来存储的(K-V结构) —Hash+数组+链表

​	Redis字典实现包括:**字典(dict)、Hash表(dictht)、Hash表节点(dictEntry)**。

​	字典达到存储上限(阈值 0.75)，需要rehash(扩容)

​	1、初次申请默认容量为4个dictEntry，非初次申请为当前hash表容量的一倍。

​	2、rehashidx=0表示要进行rehash操作。

​	3、新增加的数据在新的hash表h[1] 、修改、删除、查询在老hash表h[0]

​	4、将老的hash表h[0]的数据重新计算索引值后全部迁移到新的hash表h[1]中，这个过程称为 rehash。

​	**渐进式rehash**

 	由于当数据量巨大时rehash的过程是非常缓慢的，所以需要进行优化。 可根据服务器空闲程度批量rehash部分节点

**压缩列表zipList**

​	压缩列表(ziplist)是由一系列特殊编码的连续内存块组成的顺序型数据结构，节省内容

​	**sorted-set和hash元素个数少**且是小整数或短字符串(直接使用) 

​	list用快速链表(quicklist)数据结构存储，而**快速链表是双向列表与压缩列表**的组合。(间接使用)

**整数集合intSet**

​	整数集合(intset)是一个有序的(整数升序)、存储整数的连续存储结构。 

​	当Redis集合类型的元素都是整数并且都处在64位有符号整数范围内(2^64)，使用该结构体存储。

**快速列表quickList**

​	快速列表(quicklist)是Redis底层重要的数据结构。是Redis3.2列表的底层实现。

​	(在Redis3.2之前，Redis采 用双向链表(adlist)和压缩列表(ziplist)实现。)

**Redis Stream**的底层主要使用了listpack(紧凑列表)和Rax树(基数树)。

​	**listpack**表示一个字符串列表的序列化，listpack可用于存储字符串或整数。用于存储stream的消息内 容。

​	**Rax树**是一个有序字典树 (基数树 Radix Tree)，按照 key 的字典序排列，支持快速地定位、插入和删除操 作。



#### 4、Zset底层实现

​		跳表(skip List)是一种随机化的数据结构，基于并联的链表，实现简单，插入、删除、查找的复杂度均为O(logN)。简单说来跳表也是链表的一种，只不过它在链表的基础上增加了跳跃功能，正是这个跳跃的功能，使得在查找元素时，跳表能够提供O(logN)的时间复杂度

​		Zset**数据量少的时候使用压缩链表ziplist**实现，有序集合使用紧挨在一起的压缩列表节点来保存，第一个节点保存member，第二个保存score。ziplist内的集合元素按score从小到大排序，score较小的排在表头位置。 **数据量大的时候使用跳跃列表skiplist和哈希表hash_map**结合实现，查找删除插入的时间复杂度都是O(longN)

​		Redis使用跳表而不使用红黑树，是因为跳表的索引结构序列化和反序列化更加快速，方便持久化。

**搜索**

​		跳跃表按 score 从小到大保存所有集合元素，查找时间复杂度为平均 *O(logN)，最坏 O(N) 。*

**插入**

  选用链表作为底层结构支持，为了高效地动态增删。因为跳表底层的单链表是有序的，为了维护这种有序性，在插入前需要遍历链表，找到该插入的位置，单链表遍历查找的时间复杂度是O(n)，同理可得，跳表的遍历也是需要遍历索引数，所以是O(logn)。

**删除**

  如果该节点还在索引中，删除时不仅要删除单链表中的节点，还要删除索引中的节点；单链表在知道删除的节点是谁时，时间复杂度为O(1)，但针对单链表来说，删除时都需要拿到前驱节点O(logN)才可改变引用关系从而删除目标节点。



### **Redis可用性**

#### 1、redis持久化 

持久化就是把内存中的数据持久化到本地磁盘，防止服务器宕机了内存数据丢失

Redis 提供两种持久化机制 **RDB（默认）** 和 **AOF 机制**，Redis4.0以后采用混合持久化，用 AOF 来**保证数据不丢失**，作为数据恢复的第一选择; 用 RDB 来做不同程度的**冷备**

**RDB：**是Redis DataBase缩写快照

​		RDB是Redis默认的持久化方式。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为dump.rdb。通过配置文件中的save参数来定义快照的周期。

​	**优点：**

​	1）只有一个文件 dump.rdb，方便持久化；

​	2）容灾性好，一个文件可以保存到安全的磁盘。

​	3）性能最大化，fork 子进程来进行持久化写操作，让主进程继续处理命令，只存在毫秒级不响应请求。

​	4）相对于数据集大时，比 AOF 的启动效率更高。

​	**缺点：**

​	数据安全性低，RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。

**AOF：持久化**

​		AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。

​	**优点：**

​	1）数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次 命令操作就记录到 aof 文件中一次。

​	2）通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。

**缺点：**

​	1）AOF 文件比 RDB 文件大，且恢复速度慢。

​	2）数据集大的时候，比 rdb 启动效率低。



#### 2、redis事务

​		事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。

**Redis事务的概念**

​		Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。

Redis的事务总是具有ACID中的**一致性和隔离性**，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。

Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的

**事务命令：**

**MULTI：**用于开启一个事务，它总是返回OK。MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。

**EXEC：**执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。当操作被打断时，返回空值 nil 。

**WATCH ：**是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。（**秒杀场景**）

**DISCARD：**调用该命令，客户端可以清空事务队列，并放弃执行事务，且客户端会从事务状态中退出。

**UNWATCH**：命令可以取消watch对所有key的监控。



#### 3、redis失效策略 

**内存淘汰策略**

1）全局的键空间选择性移除

​	**noeviction**：当内存不足以容纳新写入数据时，新写入操作会报错。（字典库常用）

​	**allkeys-lru**：在键空间中，移除最近最少使用的key。（缓存常用）

​	**allkeys-random**：在键空间中，随机移除某个key。

2）设置过期时间的键空间选择性移除

​	**volatile-lru**：在设置了过期时间的键空间中，移除最近最少使用的key。

​	**volatile-random**：在设置了过期时间的键空间中，随机移除某个key。

​	**volatile-ttl**：在设置了过期时间的键空间中，有更早过期时间的key优先移除。

**缓存失效策略**

​	**定时清除：**针对每个设置过期时间的key都创建指定定时器

​	**惰性清除：**访问时判断，对内存不友好

​	**定时扫描清除：**定时100ms随机20个检查过期的字典，若存在25%以上则继续循环删除。

#### 4、redis读写模式

​	**CacheAside旁路缓存**

写请求更新数据库后删除缓存数据。读请求不命中查询数据库，查询完成写入缓存

<img src="https://img-blog.csdnimg.cn/20200806194316539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6eF92aWN0b3J5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 15%;" />

<img src="https://img-blog.csdnimg.cn/20200806194300826.png" style="zoom: 15%;" />

​	业务端处理所有数据访问细节，同时利用 **Lazy 计算**的思想，更新 DB 后，直接删除 cache 并通过 DB 更新，确保数据以 DB 结果为准，则可以大幅降低 cache 和 DB 中数据不一致的概率

​	如果没有专门的存储服务，同时是对**数据一致性要求比较高的业务，或者是缓存数据更新比较复杂的业务**，适合使用 Cache Aside 模式。如微博发展初期，不少业务采用这种模式

```java
// 延迟双删，用以保证最终一致性,防止小概率旧数据读请求在第一次删除后更新数据库
public void write(String key,Object data){
	redis.delKey(key);
	db.updateData(data);
	Thread.sleep(1000);
	redis.delKey(key);
}
```

高并发下保证绝对的一致，先删缓存再更新数据，需要用到**内存队列做异步串行化**。非高并发场景，先更新数据再删除缓存，**延迟双删**策略基本满足了

- 先更新db后删除redis：删除redis失败则出现问题
- 先删redis后更新db：删除redis瞬间，旧数据被回填redis
- 先删redis后更新db休眠后删redis：同第二点，休眠后删除redis 可能宕机
- java内部jvm队列：不适用分布式场景且降低并发



​	**Read/Write Though**（读写穿透）

​		**先查询**缓存中数据是否存在,如果存在则直接返回,如果**不存在**,则由**缓存组件负责从数据库中同步加载数据.**

​	<img src="https://img-blog.csdnimg.cn/20200806194334623.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6eF92aWN0b3J5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 50%;" />

​	先查询要**写入的数据在缓存中**是否已经存在,如果已经存在,则**更新缓存中的数据**，并且由**缓存组件同步更新**到数据库中。

​	<img src="https://img-blog.csdnimg.cn/20200806194346642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6eF92aWN0b3J5,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 50%" />

​	用户**读操作**较多.相较于Cache aside而言更适合缓存一致的场景。使用简单屏蔽了**底层数据库的操作**,只是操作缓存.

**场景：**

微博 Feed 的 Outbox Vector（即用户最新微博列表）就采用这种模式。一些粉丝较少且不活跃的用户发表微博后，Vector 服务会首先查询 Vector Cache，如果 cache 中没有该用户的 Outbox 记录，则不写该用户的 cache 数据，直接更新 DB 后就返回，只有 cache 中存在才会通过 CAS 指令进行更新。

​	

**Write Behind Caching（异步缓存写入）**

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gorlsg74i6j31950e3dhs.jpg" alt="img" style="zoom:35%;" />

比如对一些计数业务，一条 **Feed 被点赞** 1万 次，如果更新 1万 次 DB 代价很大，而合并成一次请求直接加 1万，则是一个非常轻量的操作。但这种模型有个显著的缺点，即数据的一致性变差，甚至在一些极端场景下可能会丢失数据。



#### 5、多级缓存

**浏览器本地内存缓存：**专题活动，一旦上线，在活动期间是不会随意变更的。

**浏览器本地磁盘缓存：**Logo缓存，大图片懒加载

**服务端本地内存缓存：**由于没有持久化，重启时必定会被穿透

**服务端网络内存缓存**：Redis等，针对穿透的情况下可以继续分层，必须保证数据库不被压垮

**为什么不是使用服务器本地磁盘做缓存？**

​	当系统处理大量磁盘 IO 操作的时候，由于 CPU 和内存的速度远高于磁盘，可能导致 CPU 耗费太多时间等待磁盘返回处理的结果。对于这部分 CPU 在 IO 上的开销，我们称为 **iowait**



### Redis七大经典问题

#### 1、缓存雪崩

​		指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。

​	**解决方案：**

- **Redis 高可用**，主从+哨兵，Redis cluster，避免全盘崩溃
- 本地 ehcache 缓存 + hystrix **限流&降级**，避免 MySQL 被打死
- 缓存数据的**过期时间设置随机**，防止同一时间大量数据过期现象发生。

- **逻辑上永不过期**给每一个缓存数据增加相应的**缓存标记**，缓存标记失效则更新数据缓存
- **多级缓存**，失效时通过二级更新一级，由第三方插件更新二级缓存。



#### **2、缓存穿透**

​		https://blog.csdn.net/lin777lin/article/details/105666839

​		缓存穿透是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。

​	**解决方案：**

​	1）**接口层增加校验**，如用户鉴权校验，id做基础校验，id<=0的直接拦截；

​	2）从缓存取不到的数据，在数据库中也没有取到，这时也可以将**key-value对写为key-null**，缓存有效时间可以设置短点，如30秒。这样可以防止攻击用户反复用同一个id暴力攻击；

​	3）采用**布隆过滤器**，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力。（宁可错杀一千不可放过一人）



#### **3、缓存击穿**

​		这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库

​	**解决方案：**

​	1）设置**热点数据永远不过期**，异步线程处理。

​	2）加**写回操作加互斥锁**，查询失败默认值快速返回。

​	3）缓存预热

​		系统上线后，将相关**可预期（例如排行榜）**热点数据直接加载到缓存。

​		写一个缓存刷新页面，手动操作热点数据**（例如广告推广）**上下线。



#### 4、数据不一致

​	在缓存机器的带宽被打满，或者机房网络出现波动时，缓存更新失败，新数据没有写入缓存，就会导致缓存和 DB 的数据不一致。缓存 rehash 时，某个缓存机器反复异常，多次上下线，更新请求多次 rehash。这样，一份数据存在多个节点，且每次 rehash 只更新某个节点，导致一些缓存节点产生脏数据。

- Cache 更新失败后，可以进行重试，则将重试失败的 key 写入mq，待缓存访问恢复后，将这些 key 从缓存删除。这些 key 在再次被查询时，重新从 DB 加载，从而保证数据的一致性

- 缓存时间适当调短，让缓存数据及早过期后，然后从 DB 重新加载，确保数据的最终一致性。

- 不采用 rehash 漂移策略，而采用缓存分层策略，尽量避免脏数据产生。



#### 5、数据并发竞争

​	数据并发竞争在大流量系统也比较常见，比如车票系统，如果某个火车车次缓存信息过期，但仍然有大量用户在查询该车次信息。又比如微博系统中，如果某条微博正好被缓存淘汰，但这条微博仍然有大量的转发、评论、赞。上述情况都会造成并发竞争读取的问题。

- ​	加**写回操作加互斥锁**，查询失败默认值快速返回。
- ​	对缓存数据保持多个备份，减少并发竞争的概率

​	

#### 6、热点key问题

​	明星结婚、离婚、出轨这种特殊突发事件，比如奥运、春节这些重大活动或节日，还比如秒杀、双12、618 等线上促销活动，都很容易出现 Hot key 的情况。

如何提前发现HotKey？

- 对于重要节假日、线上促销活动这些提前已知的事情，可以提前评估出可能的热 key 来。
- 而对于突发事件，无法提前评估，可以**通过 Spark，对应流任务进行实时分析**，及时发现新发布的热点 key。而对于之前已发出的事情，逐步发酵成为热 key 的，则可以通过 Hadoop 对批处理任务离线计算，找出最近历史数据中的高频热 key。

**解决方案：**

- 这 n 个 key 分散存在多个缓存节点，然后 client 端请求时，随机访问其中某个后缀的 hotkey，这样就可以把热 key 的请求打散，避免一个缓存节点过载

- 缓存集群可以单节点进行主从复制和垂直扩容

- 利用应用内的前置缓存，但是需注意需要设置上限

- 延迟不敏感，定时刷新，实时感知用主动刷新

- 和缓存穿透一样，限制逃逸流量，单请求进行数据回源并刷新前置

- 无论如何设计，最后都要写一个兜底逻辑，千万级流量说来就来

  

#### 7、BigKey问题

​	比如互联网系统中需要保存用户最新 1万 个粉丝的业务，比如一个用户个人信息缓存，包括基本资料、关系图谱计数、发 feed 统计等。微博的 feed 内容缓存也很容易出现，一般用户微博在 140 字以内，但很多用户也会发表 1千 字甚至更长的微博内容，这些长微博也就成了大 key

- 首先Redis底层数据结构里，根据Value的不同，会进行数据结构的重新选择
- 可以扩展新的数据结构，进行序列化构建，然后通过 restore 一次性写入
- 将大 key 分拆为多个 key，设置较长的过期时间



### Redis分区容错

#### **1、redis数据分区** 

**Hash：（不稳定）**

​		客户端分片：哈希+取余

​		节点伸缩：数据节点关系变化，导致数据迁移

​		迁移数量和添加节点数量有关：建议翻倍扩容

​		一个简单直观的想法是直接用Hash来计算，以Key做哈希后对节点数取模。可以看出，在key足够分散的情况下，均匀性可以获得，但一旦有节点加入或退出，所有的原有节点都会受到影响，稳定性无从谈起。

**一致性Hash：（不均衡）**

​		客户端分片：哈希+顺时针（优化取余）

​		节点伸缩：只影响邻近节点，但是还是有数据迁移

​		翻倍伸缩：保证最小迁移数据和负载均衡

​		一致性Hash可以很好的解决稳定问题，可以将所有的存储节点排列在收尾相接的Hash环上，每个key在计算Hash后会顺时针找到先遇到的一组存储节点存放。而当有节点加入或退出时，仅影响该节点在Hash环上顺时针相邻的后续节点，将数据从该节点接收或者给予。但这又带来均匀性的问题，即使可以将存储节点等距排列，也会在**存储节点个数变化时带来数据的不均匀**。

**Codis的Hash槽**

​		Codis 将所有的 key 默认划分为 1024 个槽位(slot)，它首先对客户端传过来的 key 进行 crc32 运算计算 哈希值，再将 hash 后的整数值对 1024 这个整数进行取模得到一个余数，这个余数就是对应 key 的槽位。

**RedisCluster**

​		Redis-cluster把所有的物理节点映射到[0-16383]个**slot**上,对key采用crc16算法得到hash值后对16384取模，基本上采用平均分配和连续分配的方式。



#### **2、主从模式=简单**

​	主从模式最大的优点是**部署简单**，最少**两个节点便可以构成主从模式**，并且可以通过**读写分离避免读和写同时不可用**。不过，一旦 Master 节点出现故障，主从节点就**无法自动切换**，直接导致 SLA 下降。所以，主从模式一般**适合业务发展初期，并发量低，运维成本低**的情况

<img src="https://s0.lgstatic.com/i/image/M00/80/25/Ciqc1F_QgPOAaL8TAAC5EiNlvo4795.png" alt="Drawing 1.png" style="zoom:50%;" />



**主从复制原理：**

​	①通过从服务器发送到PSYNC命令给主服务器

​	②如果是首次连接，触发一次**全量复制**。此时主节点会启动一个后台线程，生成 RDB 快照文件

​	③主节点会将这个 RDB 发送给从节点，slave 会先写入本地磁盘，再从本地磁盘加载到内存中

​	④master会将此过程中的写命令写入缓存，从节点**实时同步**这些数据

​	⑤如果网络断开了连接，自动重连后主节点通过命令传播**增量复制**给从节点部分缺少的数据

**缺点**

​	所有的slave节点数据的复制和同步都由master节点来处理，会照成master节点压力太大，使用主从从结构来解决，redis4.0中引入psync2 解决了slave重启后仍然可以增量同步。



#### 3、**哨兵模式**=读多

​	由一个或多个sentinel实例组成sentinel集群可以监视一个或多个主服务器和多个从服务器。**哨兵模式适合读请求远多于写请求的业务场景，比如在秒杀系统**中用来缓存活动信息。 如果写请求较多，当集群 Slave 节点数量多了后，Master 节点同步数据的压力会非常大。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gluq6vlvglj30nw0e076f.jpg" alt="image-20201220231241725" style="zoom:50%;" />

当主服务器进入下线状态时，sentinel可以将该主服务器下的某一从服务器升级为主服务器继续提供服务，从而保证redis的高可用性。

**检测主观下线状态**

​	Sentinel每秒一次向所有与它建立了命令连接的实例(主服务器、从服务器和其他Sentinel)发送PING命 令

​	实例在down-after-milliseconds毫秒内返回无效回复Sentinel就会认为该实例主观下线(**SDown**)

**检查客观下线状态**

​	当一个Sentinel将一个主服务器判断为主观下线后 ，Sentinel会向监控这个主服务器的所有其他Sentinel发送查询主机状态的命令

​	如果达到Sentinel配置中的quorum数量的Sentinel实例都判断主服务器为主观下线，则该主服务器就会被判定为客观下线(**ODown**)。

**选举Leader Sentinel** 

​	当一个主服务器被判定为客观下线后，监视这个主服务器的所有Sentinel会通过选举算法(raft)，选出一个Leader Sentinel去执行**failover(故障转移)**操作。

​	**Raft算法**

​	Raft协议是用来解决分布式系统一致性问题的协议。 Raft协议描述的节点共有三种状态:Leader, Follower, Candidate。 Raft协议将时间切分为一个个的Term(任期)，可以认为是一种“逻辑时间”。 选举流程:
 	①Raft采用心跳机制触发Leader选举系统启动后，全部节点初始化为Follower，term为0

​	 ②节点如果收到了RequestVote或者AppendEntries，就会保持自己的Follower身份 

​	 ③节点如果一段时间内没收到AppendEntries消息，在该节点的超时时间内还没发现Leader，Follower就会转换成Candidate，自己开始竞选Leader。 一旦转化为Candidate，该节点立即开始下面几件事情:
​		--增加自己的term，启动一个新的定时器
​		--给自己投一票，向所有其他节点发送RequestVote，并等待其他节点的回复。

​	 ④如果在计时器超时前，节点收到多数节点的同意投票，就转换成Leader。同时通过 AppendEntries，向其他节点发送通知。

​	 ⑤每个节点在一个term内只能投一票，采取先到先得的策略，Candidate投自己， Follower会投给第一个收到RequestVote的节点。

​	 ⑥Raft协议的定时器采取随机超时时间（选举的关键），先转为Candidate的节点会先发起投票，从而获得多数票。

**主服务器的选择**

​	当选举出Leader Sentinel后，Leader Sentinel会根据以下规则去从服务器中选择出新的主服务器。

1. 过滤掉主观、客观下线的节点
2. 选择配置slave-priority最高的节点，如果有则返回没有就继续选择
3. 选择出复制偏移量最大的系节点，因为复制偏移量越大则数据复制的越完整
4. 选择run_id最小的节点，因为run_id越小说明重启次数越少

**故障转移**

​	当Leader Sentinel完成新的主服务器选择后，Leader Sentinel会对下线的主服务器执行故障转移操作，主要有三个步骤:

​	1、它会将失效 Master 的其中一个 Slave 升级为新的 Master , 并让失效 Master 的其他 Slave 改为复制新的 Master ;

​	2、当客户端试图连接失效的 Master 时，集群会向客户端返回新 Master 的地址，使得集群当前状态只有一个Master。

​	3、Master 和 Slave 服务器切换后， Master 的 redis.conf 、 Slave 的 redis.conf 和 sentinel.conf 的配置文件的内容都会发生相应的改变，即 Master 主服务器的 redis.conf配置文件中会多一行 replicaof 的配置， sentinel.conf 的监控目标会随之调换。



#### 4、集群模式=写多



​	为了避免单一节点负载过高导致不稳定，集群模式采用**一致性哈希算法或者哈希槽的方法**将 Key 分布到各个节点上。其中，每个 Master 节点后跟若干个 Slave 节点，用于**出现故障时做主备切换**，客户端可以**连接任意 Master 节点**，集群内部会按照**不同 key 将请求转发到不同的 Master** 节点

​	集群模式是如何实现高可用的呢？集群内部节点之间会**互相定时探测**对方是否存活，如果多数节点判断某个节点挂了，则会将其踢出集群，然后从 **Slave** 节点中选举出一个节点**替补**挂掉的 Master 节点。**整个原理基本和哨兵模式一致**

​	虽然集群模式避免了 Master 单节点的问题，但**集群内同步数据时会占用一定的带宽**。所以，只有在**写操作比较多的情况下人们才使用集群模式**，其他大多数情况，使用**哨兵模式**都能满足需求



#### 5、分布式锁

**利用Watch实现Redis乐观锁**

​	乐观锁基于CAS(Compare And Swap)比较并替换思想，不会产生锁等待而消耗资源，但是需要反复的重试，但也是因为重试的机制，能比较快的响应。因此我们可以利用redis来实现乐观锁**（秒杀）**。具体思路如下:

1、利用redis的watch功能，监控这个redisKey的状态值 
2、获取redisKey的值，创建redis事务，给这个key的值+1 
3、执行这个事务，如果key的值被修改过则回滚，key不加1

**利用setnx防止库存超卖**
	分布式锁是控制分布式系统之间同步访问共享资源的一种方式。 利用Redis的单线程特性对共享资源进行串行化处理

```java
// 获取锁推荐使用set的方式
String result = jedis.set(lockKey, requestId, "NX", "EX", expireTime);
String result = jedis.setnx(lockKey, requestId); //如线程死掉，其他线程无法获取到锁
```

```java
// 释放锁，非原子操作，可能会释放其他线程刚加上的锁
if (requestId.equals(jedis.get(lockKey))) { 
  jedis.del(lockKey);
}
// 推荐使用redis+lua脚本
String lua = "if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end";
Object result = jedis.eval(lua, Collections.singletonList(lockKey),
```



**分布式锁存在的问题**：

- **客户端长时间阻塞导致锁失效问题**

​	计算时间内异步启动另外一个线程去检查的问题，这个key是否超时，当锁超时时间快到期且逻辑未执行完，延长锁超时时间。

- **Redis服务器时钟漂移问题导致同时加锁
  redis的过期时间是依赖系统时钟的，如果时钟漂移过大时 理论上是可能出现的 **会影响到过期时间的计算。

- **单点实例故障，锁未及时同步导致丢失**

  **RedLock算法**

1. 获取当前时间戳T0，配置时钟漂移误差T1

2. 短时间内逐个获取全部N/2+1个锁，结束时间点T2

3. 实际锁能使用的处理时长变为：TTL - （T2 - T0）- T1

   该方案通过多节点来**防止Redis的单点故障**，效果一般，也无法防止：

- **主从切换导致的两个客户端同时持有锁**

  大部分情况下**持续时间极短**，而且使用**Redlock在切换的瞬间**获取到节点的锁，也存在问题。已经是极低概率的时间，无法避免。**Redis分布式锁适合幂等性事务**，如果一定要**保证安全**，应该**使用Zookeeper或者DB**，但是，**性能会急剧下降**。



**与zookeeper分布式锁对比**

- redis 分布式锁，其实**需要自己不断去尝试获取锁**，比较消耗性能。
- zk 分布式锁，注册个监听器即可，不需要不断主动尝试获取锁，ZK获取锁会按照加锁的顺序，所以是公平锁，性能和mysql差不多，和redis差别大





**Redission生产环境的分布式锁**

​	Redisson是基于NIO的Netty框架上的一个Java驻内存数据网格(In-Memory Data Grid)分布式锁开源组件。 

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glurlfrrp4j30qk0g876c.jpg" alt="image-20201221000119586" style="zoom:67%;" />

但当业务必须要数据的强一致性，即不允许重复获得锁，比如金融场景(重复下单，重复转账)，**请不要使用redis分布式锁**。可以使用CP模型实现，比如:**zookeeper和etcd。**

|            | Redis    | zookeeper  | etcd       |
| ---------- | -------- | ---------- | ---------- |
| 一致性算法 | 无       | paxos(ZAB) | raft       |
| CAP        | AP       | CP         | CP         |
| 高可用     | 主从集群 | n+1        | n+1        |
| 实现       | setNX    | createNode | restfulAPI |





#### 6、redis心跳检测

在命令传播阶段，从服务器默认会以每秒一次的频率向主服务器发送ACK命令:

​	1、检测主从的连接状态 检测主从服务器的网络连接状态

​			lag的值应该在0或1之间跳动，如果超过1则说明主从之间的连接有 故障。

​	2、辅助实现min-slaves,Redis可以通过配置防止主服务器在不安全的情况下执行写命令

```yaml
min-slaves-to-write 3 (min-replicas-to-write 3 )

min-slaves-max-lag 10 (min-replicas-max-lag 10)
```

​		上面的配置表示:从服务器的数量少于3个，或者三个从服务器的延迟(lag)值都大于或等于10 秒时，主服务器将拒绝执行写命令。

​	3、检测命令丢失，增加重传机制

​		如果因为网络故障，主服务器传播给从服务器的写命令在半路丢失，那么当从服务器向主服务器发 送REPLCONF ACK命令时，主服务器将发觉从服务器当前的复制偏移量少于自己的复制偏移量， 然后主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区里面找到从服务器缺少的数据，并将这些数据重新发送给从服务器。



### Redis实战

#### 1、Redis优化

![img](https://tva1.sinaimg.cn/large/008eGmZEly1gorm5m7b4gj30uy0hjwfp.jpg)

**读写方式**
	简单来说就是不用**keys**等，用**range、contains**之类。比如，用户粉丝数，大 V 的粉丝更是高达几千万甚至过亿，因此，获取粉丝列表只能部分获取。另外在判断某用户是否关注了另外一个用户时，也只需要关注列表上进行检查判断，然后返回 True/False 或 0/1 的方式更为高效。

**KV size**
	如果单个业务的 KV size 过大，需要分拆成多个 KV 来缓存。拆分时应**考虑访问频率**

**key 的数量**
	如果数据量巨大，则在缓存中尽可能只保留频繁访问的热数据，对于冷数据直接访问 DB。

**读写峰值**
	如果小于 10万 级别，简单分拆到独立 Cache 池即可
	如果达到 100万 级的QPS，则需要对 Cache 进行分层处理，可以同时使用 Local-Cache 配合远程 cache，甚至远程缓存内部继续分层叠加分池进行处理。**（多级缓存）**

**命中率**
	缓存的命中率对整个服务体系的性能影响甚大。对于核心高并发访问的业务，需要预留足够的容量，确保核心业务缓存维持较高的命中率。比如微博中的 Feed Vector Cache（**热点资讯**），常年的命中率高达 99.5% 以上。为了持续保持缓存的命中率，缓存体系需要持续监控，及时进行故障处理或故障转移。同时在部分缓存节点异常、命中率下降时，故障转移方案，需要考虑是采用一致性 Hash 分布的访问漂移策略，还是采用数据多层备份策略。

**过期策略**

​	可以设置较短的过期时间，让冷 key 自动过期；也可以让 key 带上时间戳，同时设置较长的过期时间，比如很多业务系统内部有这样一些 key：key_20190801。

**缓存穿透时间**
	平均缓存穿透加载时间在某些业务场景下也很重要，对于一些缓存穿透后，加载时间特别长或者需要复杂计算的数据，而且访问量还比较大的业务数据，要配置更多容量，维持更高的命中率，从而减少穿透到 DB 的概率，来确保整个系统的访问性能。

**缓存可运维性**
	对于缓存的可运维性考虑，则需要考虑缓存体系的集群管理，如何进行一键扩缩容，如何进行缓存组件的升级和变更，如何快速发现并定位问题，如何持续监控报警，最好有一个完善的运维平台，将各种运维工具进行集成。

**缓存安全性**
	对于缓存的安全性考虑，一方面可以限制来源 IP，只允许内网访问，同时加密鉴权访问。



#### 2、Redis热升级

> 在 Redis 需要升级版本或修复 bug 时，如果直接重启变更，由于需要数据恢复，这个过程需要近 10 分钟的时间，时间过长，会严重影响系统的可用性。面对这种问题，可以对 Redis 扩展热升级功能，从而在毫秒级完成升级操作，完全不影响业务访问。

热升级方案如下，首先构建一个 Redis 壳程序，将 redisServer 的所有属性（包括redisDb、client等）保存为全局变量。然后将 Redis 的处理逻辑代码全部封装到动态连接库 so 文件中。Redis 第一次启动，从磁盘加载恢复数据，在后续升级时，通过指令，壳程序重新加载 Redis 新的 redis-4.so 到 redis-5.so 文件，即可完成功能升级，毫秒级完成 Redis 的版本升级。而且整个过程中，所有 Client 连接仍然保留，在升级成功后，原有 Client 可以继续进行读写操作，整个过程对业务完全透明。









































# 六、Kafka篇

### Why kafka

消息队列的作用：**异步、削峰填谷、解耦**

**中小型公司**，技术实力较为一般，技术挑战不是特别高，用 **RabbitMQ** （开源、社区活跃）是不错的选择；**大型公司**，基础架构研发实力较强，用 **RocketMQ**（Java二次开发） 是很好的选择。

如果是**大数据领域**的实时计算、日志采集等场景，用 **Kafka** 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gmfiyienm0j30zu0hago7.jpg" alt="image-20210107225921930" style="zoom:50%;" />



**RabbitMQ**

RabbitMQ开始是用在电信业务的可靠通信的，也是少有的几款**支持AMQP**协议的产品之一。

**优点：**

- 轻量级，快速，部署使用方便
- 支持灵活的路由配置。RabbitMQ中，在生产者和队列之间有一个交换器模块。根据配置的路由规则，生产者发送的消息可以发送到不同的队列中。路由规则很灵活，还可以自己实现。
- RabbitMQ的客户端支持大多数的编程语言，支持**AMQP**协议。

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gmfjicxzb2j30u80hx0uw.jpg" alt="image-20210107231826261" style="zoom:40%;" />

**缺点：**

- 如果有大量消息堆积在队列中，性能会急剧下降
- 每秒处理几万到几十万的消息。如果应用要求高的性能，不要选择RabbitMQ。 
- RabbitMQ是Erlang开发的，功能扩展和二次开发代价很高。



**RocketMQ**

借鉴了Kafka的设计并做了很多改进，**几乎具备了消息队列应该具备的所有特性和功能**。

- RocketMQ主要用于有序，事务，流计算，消息推送，日志流处理，binlog分发等场景。
- 经过了历次的双11考验，性能，稳定性可靠性没的说。
- java开发，阅读源代码、扩展、二次开发很方便。
- 对电商领域的响应延迟做了很多优化。
- 每秒处理几十万的消息，同时响应在毫秒级。如果应用很关注响应时间，可以使用RocketMQ。
- 性能比RabbitMQ高一个数量级，。
- 支持死信队列，DLX 是一个非常有用的特性。它可以处理**异常情况下，消息不能够被消费者正确消费而被置入死信队列中**的情况，后续分析程序可以通过消费这个死信队列中的内容来分析当时所遇到的异常情况，进而可以**改善和优化系统**。

**缺点**：

​	跟周边系统的整合和兼容不是很好。



**Kafka**

**高可用**，几乎所有相关的开源软件都支持，满足大多数的应用场景，尤其是**大数据和流计算**领域，

- Kafka高效，可伸缩，消息持久化。支持分区、副本和容错。
- 对批处理和异步处理做了大量的设计，因此Kafka可以得到非常高的性能。
- 每秒处理几十万异步消息消息，如果开启了压缩，最终可以达到每秒处理2000w消息的级别。
- 但是由于是异步的和批处理的，延迟也会高，不适合电商场景。



### What Kafka

- Producer API：允许应用程序将记录流发布到一个或多个Kafka主题。
- Consumer API：允许应用程序订阅一个或多个主题并处理为其生成的记录流。
- Streams API：允许应用程序充当流处理器，将输入流转换为输出流。

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gme95cirjfj31000kb41j.jpg" alt="image-20210106203420526" style="zoom: 40%;" />



**消息Message**

​	Kafka的数据单元称为消息。可以把消息看成是数据库里的一个“数据行”或一条“记录”。

**批次**

​	为了提高效率，消息被分批写入Kafka。提高吞吐量却加大了响应时间

**主题Topic**

​	通过主题进行分类，类似数据库中的表，

**分区Partition**

​	Topic可以被分成若干分区分布于kafka集群中，方便扩容

​	单个分区内是有序的，partition设置为一才能保证全局有序

**副本Replicas**

​	每个主题被分为若干个分区，每个分区有多个副本。

**生产者Producer**

​	生产者在默认情况下把**消息均衡地分布**到主题的所有分区上：

- 直接指定消息的分区
- 根据消息的key散列取模得出分区
- 轮询指定分区。

**消费者Comsumer**

​	消费者通过**偏移量**来区分已经读过的消息，从而消费消息。把每个分区最后读取的消息偏移量保存在Zookeeper 或Kafka上，如果消费者关闭或重启，它的**读取状态不会丢失**。

**消费组ComsumerGroup**

​	消费组保证**每个分区只能被一个消费者**使用，避免重复消费。如果群组内一个**消费者失效**，消费组里的其他消费者可以**接管失效消费者的工作再平衡**，重新分区

**节点Broker**

​	连接生产者和消费者，**单个**broker**可以轻松处理**数千个分区**以及**每秒百万级的消息量。

- broker接收来自生产者的消息，为消息设置偏移量，并提交**消息到磁盘保存**。
- broker为消费者提供服务，响应读取分区的请求，**返回已经提交到磁盘上的消息**。

**集群**

​	每隔分区都有一个**首领**，当分区被分配给多个broker时，会通过首领进行**分区复制**。	

**生产者Offset**

​	消息写入的时候，每一个分区都有一个offset，即每个分区的最新最大的offset。

**消费者Offset**

​	不同消费组中的消费者可以针对一个分区存储不同的Offset，互不影响

**LogSegment**

- 一个分区由多个LogSegment组成，
- 一个LogSegment由`.log .index .timeindex`组成
- `.log`追加是顺序写入的，文件名是以文件中第一条message的offset来命名的
- `.Index`进行日志删除的时候和数据查找的时候可以快速定位。
- `.timeStamp`则根据**时间戳查找对应的偏移量**。



### How Kafka

**优点**

- **高吞吐量**：单机每秒处理几十上百万的消息量。即使存储了TB及消息，也保持稳定的性能。
  - **零拷贝** 减少内核态到用户态的拷贝，磁盘通过sendfile实现**DMA** 拷贝Socket buffer
  - **顺序读写** 充分利用磁盘顺序读写的超高性能
  - **页缓存mmap**，将磁盘文件**映射**到内存, 用户通过修改内存就能修改磁盘文件。
- **高性能**：单节点支持上千个客户端，并保证零停机和零数据丢失。
- **持久化**：将消息持久化到磁盘。通过将数据持久化到硬盘以及replication防止数据丢失。
- **分布式系统**，易扩展。所有的组件均为分布式的，无需停机即可扩展机器。
- **可靠性** - Kafka是分布式，分区，复制和容错的。
- **客户端状态维护**：消息被处理的状态是在Consumer端维护，当失败时能自动平衡。

**应用场景**

- **日志收集：**用Kafka可以收集各种服务的Log，通过大数据平台进行处理；
- **消息系统：**解耦生产者和消费者、缓存消息等；
- **用户活动跟踪：**Kafka经常被用来记录Web用户或者App用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到Kafka的Topic中，然后消费者通过订阅这些Topic来做**运营数据**的实时的监控分析，也可保存到数据库；



### **生产消费基本流程**

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gmeb1cw09gj313m0kgwgb.jpg" alt="image-20210106213944461" style="zoom:40%;" />

1. Producer创建时，会创建一个Sender线程并设置为守护线程。

2. 生产的消息先经过拦截器->序列化器->分区器，然后将消息缓存在缓冲区。

3. 批次发送的条件为：缓冲区数据大小达到**batch.size**或者**linger.ms**达到上限。

4. 批次发送后，发往指定分区，然后落盘到broker；

   - **acks=0**只要将消息放到缓冲区，就认为消息已经发送完成。

   - **acks=1**表示消息**只需要写到主分区**即可。在该情形下，如果主分区收到消息确认之后就宕机了，而副本分区还没来得及同步该消息，则该消息丢失。

   - **acks=all （默认）**首领分区会等待**所有的ISR副本分区确认记录**。该处理保证了只要有一个ISR副本分区存活，消息就不会丢失。

5. 如果生产者配置了**retrires参数大于0并且未收到确认**，那么客户端会对该消息进行重试。

6. 落盘到broker成功，返回生产元数据给生产者。



**Leader选举**

- Kafka会在Zookeeper上针对每个Topic维护一个称为ISR（in-sync replica）的集合

- 当集合中副本都跟Leader中的副本同步了之后，kafka才会认为消息已提交

- 只有这些跟Leader保持同步的Follower才应该被选作新的Leader

- 假设某个topic有N+1个副本，kafka可以容忍N个服务器不可用，冗余度较低

  如果ISR中的副本都丢失了，则：

  - 可以等待ISR中的副本任何一个恢复，接着对外提供服务，需要时间等待
  - 从OSR中选出一个副本做Leader副本，此时会造成数据丢失



**副本消息同步**

​	首先，Follower 发送 FETCH 请求给 Leader。接着，Leader 会读取底层日志文件中的消 息数据，再更新它内存中的 Follower 副本的 LEO 值，更新为 FETCH 请求中的 fetchOffset 值。最后，尝试更新分区高水位值。Follower 接收到 FETCH 响应之后，会把消息写入到底层日志，接着更新 LEO 和 HW 值。



**相关概念**：**LEO**和**HW**。

- LEO：即日志末端位移(log end offset)，记录了该副本日志中下一条消息的位移值。如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0, 9]
- HW：水位值HW（high watermark）即已备份位移。对于同一个副本对象而言，其HW值不会大于LEO值。小于等于HW值的所有消息都被认为是“已备份”的（replicated）



**Rebalance**

- 组成员数量发生变化
- 订阅主题数量发生变化
- 订阅主题的分区数发生变化

leader选举完成后，当以上三种情况发生时，Leader根据配置的**RangeAssignor**开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进**SyncGroup**请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。



**分区分配算法RangeAssignor**

- 原理是按照消费者总数和分区总数进行整除运算平均分配给所有的消费者。

- 订阅Topic的消费者按照名称的字典序排序，分均分配，剩下的字典序从前往后分配

  

**增删改查**

```bash
kafka-topics.sh --zookeeper localhost:2181/myKafka --create --topic topic_x 
								--partitions 1 --replication-factor 1
kafka-topics.sh --zookeeper localhost:2181/myKafka --delete --topic topic_x
kafka-topics.sh --zookeeper localhost:2181/myKafka --alter --topic topic_x
								--config max.message.bytes=1048576
kafka-topics.sh --zookeeper localhost:2181/myKafka --describe --topic topic_x
```

**如何查看偏移量为23的消息？**

通过查询跳跃表`ConcurrentSkipListMap`，定位到在00000000000000000000.index ，通过二分法在偏移量索引文件中找到不大于 23 的**最大索引项**，即offset 20 那栏，然后从日志分段文件中的物理位置为320 开始顺序查找偏移量为 23 的消息。

<img src="https://img-blog.csdnimg.cn/20191230225447849.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMjMzNzA2,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />





**切分文件**

- **大小分片** 当前日志分段文件的大小超过了 broker 端参数 `log.segment.bytes` 配置的值
- **时间分片** 当前日志分段中消息的最大时间戳与系统的时间戳的差值大于`log.roll.ms`配置的值
- **索引分片** 偏移量或时间戳索引文件大小达到broker端 `log.index.size.max.bytes`配置的值
- **偏移分片** 追加的消息的偏移量与当前日志分段的偏移量之间的差值大于 Integer.MAX_VALUE 



### 一致性

**幂等性**

保证在消息重发的时候，消费者不会重复处理。即使在**消费者收到重复消息的时候，重复处理**，也

要**保证最终结果的一致性**。所谓幂等性，数学概念就是： f(f(x)) = f(x) 

![image-20210107000942286](https://tva1.sinaimg.cn/large/008eGmZEly1gmefdeas1vj315i0bgmya.jpg)

**如何实现？**

​	添加唯一ID，类似于数据库的主键，用于唯一标记一个消息。

```bash
ProducerID：#在每个新的Producer初始化时，会被分配一个唯一的PID
SequenceNumber：#对于每个PID发送数据的每个Topic都对应一个从0开始单调递增的SN值
```

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gmefjpeet8j317e0cgmyp.jpg" alt="image-20210107001546404" style="zoom:80%;" />

**如何选举**

1. 使用 Zookeeper 的**分布式锁选举控制器**，并在节点加入集群或退出集群时通知控制器。
2. 控制器负责在节点加入或离开集群时进行分区Leader选举。
3. 控制器使用epoch`忽略小的纪元`来避免**脑裂**：两个节点同时认为自己是当前的控制器。





### 可用性

- 创建Topic的时候可以指定 --replication-factor 3 ，表示不超过broker的副本数
- 只有Leader是负责读写的节点，Follower定期地到Leader上Pull数据。
- ISR是Leader负责维护的与其保持同步的Replica列表，即当前活跃的副本列表。如果一个Follow落后太多，Leader会将它从ISR中移除。选举时优先从ISR中挑选Follower。 
- 设置 acks=all 。Leader收到了ISR中所有Replica的ACK，才向Producer发送ACK。





### 面试题

#### **线上问题rebalance**

> 因集群架构变动导致的消费组内重平衡，如果kafka集内节点较多，比如数百个，那重平衡可能会耗时导致**数分钟到数小时**，此时kafka基本处于不可用状态，对kafka的TPS影响极大

产生的原因：

- 组成员数量发生变化

- 订阅主题数量发生变化

- 订阅主题的分区数发生变化

  **组成员崩溃和组成员主动离开是两个不同的场景。**因为在崩溃时成员并不会主动地告知coordinator此事，coordinator有可能需要一个完整的session.timeout周期(心跳周期)才能检测到这种崩溃，这必然会造成consumer的滞后。可以说离开组是主动地发起rebalance；而崩溃则是被动地发起rebalance。

  ![img](https://tva1.sinaimg.cn/large/008eGmZEly1gooe9o07fvj30p00btju1.jpg)

解决方案：

```properties
加大超时时间 session.timout.ms=6s
加大心跳频率 heartbeat.interval.ms=2s
增长推送间隔 max.poll.interval.ms=t+1 minutes
```

[这些年，为了进阿里背过的面试题]()



#### ZooKeeper 的作用

目前，Kafka 使用 ZooKeeper 存放集群元数据、成员管理、Controller 选举，以及其他一些管理类任务。之后，等 KIP-500 提案完成后，Kafka 将完全不再依赖于 ZooKeeper。

- **存放元数据**是指主题分区的所有数据都保存在 ZooKeeper 中，其他“人”都要与它保持对齐。
- **成员管理**是指 Broker 节点的注册、注销以及属性变更等 。
- **Controller 选举**是指选举集群 Controller，包括但不限于主题删除、参数配置等。

一言以蔽之:**KIP-500 ，是使用社区自研的基于 Raft 的共识算法，实现 Controller 自选举**。

同样是存储元数据，这几年**基于Raft算法的etcd**认可度越来越高

​	越来越多的系统开始用它保存关键数据。比如，**秒杀系统经常用它保存各节点信息**，以便控制消费 MQ 的服务数量。还有些**业务系统的配置数据**，也会通过 etcd 实时**同步给业务系统的各节点**，比如，秒杀管理后台会使用 etcd 将**秒杀活动的配置数据实时同步给秒杀 API 服务各节点**。





#### Replica副本的作用

**Kafka 只有 Leader 副本才能 对外提供读写服务，响应 Clients 端的请求。Follower 副本只是采用拉(PULL)的方 式，被动地同步 Leader 副本中的数据，并且在 Leader 副本所在的 Broker 宕机后，随时准备应聘 Leader 副本。**

- **自 Kafka 2.4 版本开始**，社区可以通过配置参数，允许 Follower 副本有限度地提供读服务。
- 之前确保一致性的主要手段是高水位机制， 但高水位值无法保证 Leader 连续变更场景下的数据一致性，因此，社区引入了 **Leader Epoch** 机制，来修复高水位值的弊端。



#### 为什么不支持读写分离?

- **自 Kafka 2.4 之后**，Kafka 提供了有限度的读写分离。

- **场景不适用**。读写分离适用于那种读负载很大，而写操作相对不频繁的场景。
- **同步机制**。Kafka 采用 PULL 方式实现 Follower 的同步，同时复制延迟较大。





#### 如何防止重复消费

- 代码层面每次消费需提交offset
- 通过Mysql的**唯一键约束**，结合Redis查看**id是否被消费**，存Redis可以直接使用set方法
- 量大且允许误判的情况下，使用布隆过滤器也可以




#### **如何保证数据不会丢失**

- **生产者**生产消息可以通过comfirm配置**ack=all**解决
- **Broker**同步过程中leader宕机可以通过配置**ISR副本+重试**解决
- **消费者**丢失可以**关闭自动提交**offset功能，系统处理完成时提交offset



#### **如何保证顺序消费**

- 单 topic，单partition，单 consumer，单线程消费，吞吐量低，不推荐
- **如只需保证单key有序**，为每个key申请单独内存 queue，每个线程分别消费一个内存 queue 即可，这样就能保证单key（例如用户id、活动id）顺序性。



#### 【线上】如何解决积压消费

- **修复consumer**，使其具备消费能力，并且扩容N台
- 写一个**分发的程序**，将Topic均匀分发到临时Topic中
- 同时**起N台consumer**，消费不同的**临时Topic**



#### 如何避免消息积压

- 提高消费并行度
- 批量消费
- 减少组件IO的交互次数
- 优先级消费

```java
if (maxOffset - curOffset > 100000) {
  // TODO 消息堆积情况的优先处理逻辑
  // 未处理的消息可以选择丢弃或者打日志
  return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
}
// TODO 正常消费过程
return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
```



#### 如何设计消息队列

需要支持快速水平扩容，broker+partition，partition放不同的机器上，增加机器时将数据根据topic做迁移，分布式需要考虑一致性、可用性、分区容错性

- **一致性：**生产者的消息确认、消费者的幂等性、Broker的数据同步
- **可用性：**数据如何保证不丢不重、数据如何持久化、持久化时如何读写
- **分区容错：**采用何种选举机制、如何进行多副本同步
- **海量数据：**如何解决消息积压、海量Topic性能下降

性能上，可以借鉴**时间轮、零拷贝、IO多路复用、顺序读写、压缩批处理**





# 	七、Spring篇 

### 设计思想&Beans

#### **1、IOC 控制反转**

​		IoC（Inverse of Control:控制反转）是⼀种设计思想，就是将原本在程序中⼿动创建对象的控制权，交由Spring框架来管理。 IoC 在其他语⾔中也有应⽤，并⾮ Spring 特有。 

​		IoC 容器是 Spring⽤来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注⼊。这样可以很⼤程度上简化应⽤的开发，把应⽤从复杂的依赖关系中解放出来。 IoC 容器就像是⼀个⼯⼚⼀样，当我们需要创建⼀个对象的时候，只需要配置好配置⽂件/注解即可，完全不⽤考虑对象是如何被创建出来的。



**DI 依赖注入**

​	DI:（Dependancy Injection：依赖注入)站在容器的角度，将对象创建依赖的其他对象注入到对象中。



#### **2、AOP 动态代理**

​		AOP(Aspect-Oriented Programming:⾯向切⾯编程)能够将那些与业务⽆关，却为业务模块所共同调⽤的逻辑或责任（例如事务处理、⽇志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。

​		Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接⼝，那么Spring AOP会使⽤JDKProxy，去创建代理对象，⽽对于没有实现接⼝的对象，就⽆法使⽤ JDK Proxy 去进⾏代理了，这时候Spring AOP会使⽤基于asm框架字节流的Cglib动态代理 ，这时候Spring AOP会使⽤ Cglib ⽣成⼀个被代理对象的⼦类来作为代理。



#### **3、Bean生命周期** 

**单例对象：** singleton                    

总结：单例对象的生命周期和容器相同        

**多例对象：** prototype           

出生：使用对象时spring框架为我们创建            

活着：对象只要是在使用过程中就一直活着            

死亡：当对象长时间不用且没有其它对象引用时，由java的垃圾回收机制回收

<img src="https://s0.lgstatic.com/i/image3/M01/89/0C/Cgq2xl6WvHqAdmt4AABGAn2eSiI631.png" alt="img" style="zoom:67%;" />

IOC容器初始化加载Bean流程：

```java
@Override
public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) {
  // 第一步:刷新前的预处理 
  prepareRefresh();
  //第二步: 获取BeanFactory并注册到 BeanDefitionRegistry
  ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();
  // 第三步:加载BeanFactory的预准备工作(BeanFactory进行一些设置，比如context的类加载器等)
  prepareBeanFactory(beanFactory);
  try {
    // 第四步:完成BeanFactory准备工作后的前置处理工作 
    postProcessBeanFactory(beanFactory);
    // 第五步:实例化BeanFactoryPostProcessor接口的Bean 
    invokeBeanFactoryPostProcessors(beanFactory);
    // 第六步:注册BeanPostProcessor后置处理器，在创建bean的后执行 
    registerBeanPostProcessors(beanFactory);
    // 第七步:初始化MessageSource组件(做国际化功能;消息绑定，消息解析); 
    initMessageSource();
    // 第八步:注册初始化事件派发器 
    initApplicationEventMulticaster();
    // 第九步:子类重写这个方法，在容器刷新的时候可以自定义逻辑 
    onRefresh();
    // 第十步:注册应用的监听器。就是注册实现了ApplicationListener接口的监听器
    registerListeners();
    //第十一步:初始化所有剩下的非懒加载的单例bean 初始化创建非懒加载方式的单例Bean实例(未设置属性)
    finishBeanFactoryInitialization(beanFactory);
    //第十二步: 完成context的刷新。主要是调用LifecycleProcessor的onRefresh()方法，完成创建
    finishRefresh();
	}
  ……
} 
```

总结：

**四个阶段**

- 实例化 Instantiation
- 属性赋值 Populate
- 初始化 Initialization
- 销毁 Destruction

**多个扩展点**

- 影响多个Bean
  - BeanPostProcessor
  - InstantiationAwareBeanPostProcessor
- 影响单个Bean
  - Aware

**完整流程**  

1. 实例化一个Bean－－也就是我们常说的**new**；
2. 按照Spring上下文对实例化的Bean进行配置－－**也就是IOC注入**；
3. 如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String)方法，也就是根据就是Spring配置文件中**Bean的id和name进行传递**
4. 如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现setBeanFactory(BeanFactory)也就是Spring配置文件配置的**Spring工厂自身进行传递**；
5.  如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，和4传递的信息一样但是因为ApplicationContext是BeanFactory的子接口，所以**更加灵活**
6.  如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization()方法，BeanPostProcessor经常被用作是Bean内容的更改，由于这个是在Bean初始化结束时调用那个的方法，也可以被应用于**内存或缓存技**术
7.  如果Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法。
8.   如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessAfterInitialization()，**打印日志或者三级缓存技术里面的bean升级**
9.   以上工作完成以后就可以应用这个Bean了，那这个Bean是一个Singleton的，所以一般情况下我们调用同一个id的Bean会是在内容地址相同的实例，当然在Spring配置文件中也可以配置非Singleton，这里我们不做赘述。
10.   当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，或者根据spring配置的destroy-method属性，调用实现的destroy()方法





#### **4**、Bean作用域

| 名称           | 作用域                                                       |
| -------------- | ------------------------------------------------------------ |
| **singleton**  | **单例对象，默认值的作用域**                                 |
| **prototype**  | **每次获取都会创建⼀个新的 bean 实例**                       |
| request        | 每⼀次HTTP请求都会产⽣⼀个新的bean，该bean仅在当前HTTP request内有效。 |
| session        | 在一次 HTTP session 中，容器将返回同一个实例                 |
| global-session | 将对象存入到web项目集群的session域中,若不存在集群,则global session相当于session |

默认作用域是singleton，多个线程访问同一个bean时会存在线程不安全问题

**保障线程安全方法：**

1. 在Bean对象中尽量避免定义可变的成员变量（不太现实）。

2. 在类中定义⼀个ThreadLocal成员变量，将需要的可变成员变量保存在 ThreadLocal 中

 **ThreadLocal**：

  ​		每个线程中都有一个自己的ThreadLocalMap类对象，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。

  ​		将一个共用的ThreadLocal静态实例作为key，将不同对象的引用保存到不同线程的ThreadLocalMap中，然后**在线程执行的各处通过这个静态ThreadLocal实例的get()方法取得自己线程保存的那个对象**，避免了将这个对象作为参数传递的麻烦。



#### 5、循环依赖

​	循环依赖其实就是循环引用，也就是两个或者两个以上的 Bean 互相持有对方，最终形成闭环。比如A 依赖于B，B又依赖于A

Spring中循环依赖场景有: 

- prototype 原型 bean循环依赖

- 构造器的循环依赖（构造器注入）

- Field 属性的循环依赖（set注入）

  其中，构造器的循环依赖问题无法解决，在解决属性循环依赖时，可以使用懒加载，spring采用的是提前暴露对象的方法。

**懒加载@Lazy解决循环依赖问题**

​	Spring 启动的时候会把所有bean信息(包括XML和注解)解析转化成Spring能够识别的BeanDefinition并存到Hashmap里供下面的初始化时用，然后对每个 BeanDefinition 进行处理。普通 Bean 的初始化是在容器启动初始化阶段执行的，而被lazy-init=true修饰的 bean 则是在从容器里第一次进行**context.getBean() 时进行触发**。

​     

**三级缓存解决循环依赖问题**

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glv7ivru2lj31980qcn13.jpg" alt="循环依赖问题" style="zoom: 33%;" />

1. Spring容器初始化ClassA通过构造器初始化对象后提前暴露到Spring容器中的singletonFactorys（三级缓存中）。

2. ClassA调用setClassB方法，Spring首先尝试从容器中获取ClassB，此时ClassB不存在Spring 容器中。

3. Spring容器初始化ClassB，ClasssB首先将自己暴露在三级缓存中，然后从Spring容器一级、二级、三级缓存中一次中获取ClassA 。

4. 获取到ClassA后将自己实例化放入单例池中，实例 ClassA通过Spring容器获取到ClassB，完成了自己对象初始化操作。

5. 这样ClassA和ClassB都完成了对象初始化操作，从而解决了循环依赖问题。

   

### Spring注解

#### 1、@SpringBoot 

​	**声明bean的注解**

​	**@Component** 通⽤的注解，可标注任意类为  Spring 组件

​	**@Service** 在业务逻辑层使用（service层）

​	**@Repository** 在数据访问层使用（dao层）

​	**@Controller** 在展现层使用，控制器的声明（controller层）

​	**注入bean的注解**

​	**@Autowired**：默认按照类型来装配注入，**@Qualifier**：可以改成名称

​	**@Resource**：默认按照名称来装配注入，JDK的注解，新版本已经弃用



**@Autowired注解原理** 

​		 @Autowired的使用简化了我们的开发，

​				实现 AutowiredAnnotationBeanPostProcessor 类，该类实现了 Spring 框架的一些扩展接口。
​				实现 BeanFactoryAware 接口使其内部持有了 BeanFactory（可轻松的获取需要依赖的的 Bean）。
​				实现 MergedBeanDefinitionPostProcessor 接口，实例化Bean 前获取到 里面的 @Autowired 信息并缓存下来；
​				实现 postProcessPropertyValues 接口， 实例化Bean 后从缓存取出注解信息，通过反射将依赖对象设置到 Bean 属性里面。



**@SpringBootApplication**

```java
@SpringBootApplication
public class JpaApplication {
    public static void main(String[] args) {
        SpringApplication.run(JpaApplication.class, args);
    }
}
```

**@SpringBootApplication**注解等同于下面三个注解：

- **@SpringBootConfiguration：** 底层是**Configuration**注解，说白了就是支持**JavaConfig**的方式来进行配置
- **@EnableAutoConfiguration：**开启**自动配置**功能
- **@ComponentScan：**就是**扫描**注解，默认是扫描**当前类下**的package

其中`@EnableAutoConfiguration`是关键(启用自动配置)，内部实际上就去加载`META-INF/spring.factories`文件的信息，然后筛选出以`EnableAutoConfiguration`为key的数据，加载到IOC容器中，实现自动配置功能！

它主要加载了@SpringBootApplication注解主配置类，这个@SpringBootApplication注解主配置类里边最主要的功能就是SpringBoot开启了一个@EnableAutoConfiguration注解的自动配置功能。

 **@EnableAutoConfiguration作用：**

它主要利用了一个

EnableAutoConfigurationImportSelector选择器给Spring容器中来导入一些组件。

```java
@Import(EnableAutoConfigurationImportSelector.class)
public @interface EnableAutoConfiguration 
```





#### **2、@SpringMVC**

```java
@Controller 声明该类为SpringMVC中的Controller
@RequestMapping 用于映射Web请求
@ResponseBody 支持将返回值放在response内，而不是一个页面，通常用户返回json数据
@RequestBody 允许request的参数在request体中，而不是在直接连接在地址后面。
@PathVariable 用于接收路径参数
@RequestMapping("/hello/{name}")申明的路径，将注解放在参数中前，即可获取该值，通常作为Restful的接口实现方法。
```

**SpringMVC原理** 

<img src="https://img-blog.csdn.net/20181022224058617?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2F3YWtlX2xxaA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" style="zoom: 50%;" />

1. 客户端（浏览器）发送请求，直接请求到  DispatcherServlet 。
2. DispatcherServlet 根据请求信息调⽤  HandlerMapping ，解析请求对应的  Handler 。
3. 解析到对应的  Handler （也就是  Controller 控制器）后，开始由HandlerAdapter 适配器处理。
4. HandlerAdapter 会根据  Handler 来调⽤真正的处理器开处理请求，并处理相应的业务逻辑。
5. 处理器处理完业务后，会返回⼀个  ModelAndView 对象， Model 是返回的数据对象
6. ViewResolver 会根据逻辑  View 查找实际的  View 。
7. DispaterServlet 把返回的  Model 传给  View （视图渲染）。
8. 把  View 返回给请求者（浏览器）



#### 3、@SpringMybatis

```java
@Insert ： 插入sql ,和xml insert sql语法完全一样
@Select ： 查询sql, 和xml select sql语法完全一样
@Update ： 更新sql, 和xml update sql语法完全一样
@Delete ： 删除sql, 和xml delete sql语法完全一样
@Param ： 入参
@Results ： 设置结果集合@Result ： 结果
@ResultMap ： 引用结果集合
@SelectKey ： 获取最新插入id 
```

**mybatis如何防止sql注入？**

​	简单的说就是#{}是经过预编译的，是安全的，**$**{}是未经过预编译的，仅仅是取变量的值，是非安全的，存在SQL注入。在编写mybatis的映射语句时，尽量采用**“#{xxx}”**这样的格式。如果需要实现动态传入表名、列名，还需要做如下修改：添加属性**statementType="STATEMENT"**，同时sql里的属有变量取值都改成**${xxxx}**



**Mybatis和Hibernate的区别** 

**Hibernate 框架：** 

​    **Hibernate**是一个开放源代码的对象关系映射框架,它对JDBC进行了非常轻量级的对象封装,建立对象与数据库表的映射。是一个全自动的、完全面向对象的持久层框架。

**Mybatis框架：**

​    **Mybatis**是一个开源对象关系映射框架，原名：ibatis,2010年由谷歌接管以后更名。是一个半自动化的持久层框架。

**区别：**

  **开发方面**

​    在项目开发过程当中，就速度而言：

​      hibernate开发中，sql语句已经被封装，直接可以使用，加快系统开发；

​      Mybatis 属于半自动化，sql需要手工完成，稍微繁琐；

​    但是，凡事都不是绝对的，如果对于庞大复杂的系统项目来说，复杂语句较多，hibernate 就不是好方案。

  **sql优化方面**

​    Hibernate 自动生成sql,有些语句较为繁琐，会多消耗一些性能；

​    Mybatis 手动编写sql，可以避免不需要的查询，提高系统性能；

  **对象管理比对**

​    Hibernate 是完整的对象-关系映射的框架，开发工程中，无需过多关注底层实现，只要去管理对象即可；

​    Mybatis 需要自行管理映射关系；



#### 4、@Transactional

```java
@EnableTransactionManagement 
@Transactional
```

注意事项：

​	①事务函数中不要处理耗时任务，会导致长期占有数据库连接。

​	②事务函数中不要处理无关业务，防止产生异常导致事务回滚。

**事务传播属性**

**1) REQUIRED（默认属性）** 如果存在一个事务，则支持当前事务。如果没有事务则开启一个新的事务。 

2) MANDATORY  支持当前事务，如果当前没有事务，就抛出异常。 

3) NEVER  以非事务方式执行，如果当前存在事务，则抛出异常。 

4) NOT_SUPPORTED  以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 

5) REQUIRES_NEW  新建事务，如果当前存在事务，把当前事务挂起。 

6) SUPPORTS  支持当前事务，如果当前没有事务，就以非事务方式执行。 

**7) NESTED** （**局部回滚**） 支持当前事务，新增Savepoint点，与当前事务同步提交或回滚。 **嵌套事务一个非常重要的概念就是内层事务依赖于外层事务。外层事务失败时，会回滚内层事务所做的动作。而内层事务操作失败并不会引起外层事务的回滚。**



### Spring源码阅读

#### **1、Spring中的设计模式** 

参考：[spring中的设计模式](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485303&idx=1&sn=9e4626a1e3f001f9b0d84a6fa0cff04a&chksm=cea248bcf9d5c1aaf48b67cc52bac74eb29d6037848d6cf213b0e5466f2d1fda970db700ba41&token=255050878&lang=zh_CN%23rd)

**单例设计模式 :** Spring 中的 Bean 默认都是单例的。

**⼯⼚设计模式 :** Spring使⽤⼯⼚模式通过  BeanFactory 、 ApplicationContext 创建bean 对象。

**代理设计模式 :** Spring AOP 功能的实现。

**观察者模式：** Spring 事件驱动模型就是观察者模式很经典的⼀个应⽤。

**适配器模式：**Spring AOP 的增强或通知(Advice)使⽤到了适配器模式、spring MVC 中也是⽤到了适配器模式适配 Controller 。











# 八、SpringCloud篇

#### Why SpringCloud

> ​	Spring cloud 是一系列框架的有序集合。它利用 spring boot 的开发便利性巧妙地简化了分布式系统基础设施的开发，如**服务发现注册**、**配置中心**、**消息总线**、**负载均衡**、**断路器**、**数据监控**等，都可以用 spring boot 的开发风格做到一键启动和部署。

| SpringCloud（微服务解决方案）    | Dubbo（分布式服务治理框架） |
| -------------------------------- | --------------------------- |
| Rest API （轻量、灵活、swagger） | RPC远程调用（高效、耦合）   |
| Eureka、Nacos                    | Zookeeper                   |
| 使用方便                         | 性能好                      |
| 即将推出SpringCloud2.0           | 断档5年后17年重启           |

​	SpringBoot是Spring推出用于解决传统框架配置文件冗余,装配组件繁杂的基于Maven的解决方案,**旨在快速搭建单个微服务**，SpringCloud是依赖于SpringBoot的,而SpringBoot并不是依赖与SpringCloud,甚至还可以和Dubbo进行优秀的整合开发

​	MartinFlower 提出的微服务之间是通过RestFulApi进行通信，具体实现

- RestTemplate：基于HTTP协议
- Feign：封装了ribbon和Hystrix 、RestTemplate 简化了客户端开发工作量
- RPC：基于TCP协议，序列化和传输效率提升明显
- MQ：异步解耦微服务之间的调用

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmawejgpgwj30ht0bnt9d.jpg" alt="img" style="zoom:67%;" />

#### Spring Boot

> Spring Boot 通过**简单的步骤**就可以创建一个 Spring 应用。
>
> Spring Boot 为 Spring 整合第三方框架提供了**开箱即用功能**。
>
> Spring Boot 的核心思想是**约定大于配置**。

**Spring Boot 解决的问题**

- 搭建后端框架时需要手动添加 Maven 配置，涉及很多 XML 配置文件，增加了搭建难度和时间成本。

- 将项目编译成 war 包，部署到 Tomcat 中，项目部署依赖 Tomcat，这样非常不方便。

- 应用监控做的比较简单，通常都是通过一个没有任何逻辑的接口来判断应用的存活状态。

**Spring Boot 优点**

**自动装配：**Spring Boot 会根据某些规则对所有配置的 Bean 进行初始化。可以减少了很多重复性的工作。

​	比如使用 MongoDB 时，只需加入 MongoDB 的 Starter 包，然后配置  的连接信息，就可以直接使用 MongoTemplate 自动装配来操作数据库了。简化了 Maven Jar 包的依赖，降低了烦琐配置的出错几率。

**内嵌容器：**Spring Boot 应用程序可以不用部署到外部容器中，比如 Tomcat。

​	应用程序可以直接通过 Maven 命令编译成可执行的 jar 包，通过 java-jar 命令启动即可，非常方便。

**应用监控：**Spring Boot 中自带监控功能 Actuator，可以实现对程序内部运行情况进行监控，

​	比如 Bean 加载情况、环境变量、日志信息、线程信息等。当然也可以自定义跟业务相关的监控，通过Actuator 的端点信息进行暴露。

```java
spring-boot-starter-web          //用于快速构建基于 Spring MVC 的 Web 项目。
spring-boot-starter-data-redis   //用于快速整合并操作 Redis。
spring-boot-starter-data-mongodb //用于对 MongoDB 的集成。
spring-boot-starter-data-jpa     //用于操作 MySQL。
```

**自定义一个Starter**

1. 创建 Starter 项目，定义 Starter 需要的配置（Properties）类，比如数据库的连接信息；

2. 编写自动配置类，自动配置类就是获取配置，根据配置来自动装配 Bean；

3. 编写 spring.factories 文件加载自动配置类，Spring 启动的时候会扫描 spring.factories 文件，；

4. 编写配置提示文件 spring-configuration-metadata.json（不是必须的），在添加配置的时候，我们想要知道具体的配置项是什么作用，可以通过编写提示文件来提示；

5. 在项目中引入自定义 Starter 的 Maven 依赖，增加配置值后即可使用。

**Spring Boot Admin**（将 actuator 提供的数据进行可视化）

- 显示应用程序的监控状态、查看 JVM 和线程信息

- 应用程序上下线监控  

- 可视化的查看日志、动态切换日志级别

- HTTP 请求信息跟踪等实用功能



#### GateWay / Zuul

> GateWay⽬标是取代Netflflix Zuul，它基于Spring5.0+SpringBoot2.0+WebFlux等技术开发，提供**统⼀的路由**⽅式（反向代理）并且基于 **Filter**(定义过滤器对请求过滤，完成⼀些功能) 链的⽅式提供了⽹关基本的功能，例如：鉴权、流量控制、熔断、路径重写、⽇志监控。

**组成：**

- **路由route：** ⽹关最基础的⼯作单元。路由由⼀个ID、⼀个⽬标URL、⼀系列的断⾔（匹配条件判断）和Filter过滤器组成。如果断⾔为true，则匹配该路由。

- **断⾔predicates：**参考了Java8中的断⾔Predicate，匹配Http请求中的所有内容（类似于nginx中的location匹配⼀样），如果断⾔与请求相匹配则路由。

- **过滤器filter：**标准的Spring webFilter，使⽤过滤器在请求之前或者之后执⾏业务逻辑。

  请求前`pre`类型过滤器：做**参数校验**、**权限校验**、**流量监控**、**⽇志输出**、**协议转换**等，

  请求前`post`类型的过滤器：做**响应内容**、**响应头**的修改、**⽇志的输出**、**流量监控**等。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmc49l9babj31do0n7n13.jpg" alt="image-20210105001419761" style="zoom: 50%;" />

**GateWayFilter** 应⽤到单个路由路由上 、**GlobalFilter** 应⽤到所有的路由上











#### Eureka / Zookeeper

> 服务注册中⼼本质上是为了解耦服务提供者和服务消费者，为了⽀持弹性扩缩容特性，⼀个微服务的提供者的数量和分布往往是动态变化的。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmawwm3k7bj30o80ecq3u.jpg" alt="image-20210103231405882" style="zoom: 50%;" />

| 区别   | Zookeeper        | Eureka                       | Nacos              |
| ------ | ---------------- | ---------------------------- | ------------------ |
| CAP    | CP               | AP                           | CP/AP切换          |
| 可用性 | 选举期间不可用   | 自我保护机制，数据不是最新的 |                    |
| 组成   | Leader和Follower | 节点平等                     |                    |
| 优势   | 分布式协调       | 注册与发现                   | 注册中心和配置中心 |
| 底层   | 进程             | 服务                         | Jar包              |

**Eureka**通过**⼼跳检测**、**健康检查**和**客户端缓存**等机制，提⾼系统的灵活性、可伸缩性和可⽤性。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxc493qyj30ji0a6mxx.jpg" alt="image-20210103232900353" style="zoom:67%;" />

1. us-east-1c、us-east-1d，us-east-1e代表不同的机房，**每⼀个Eureka Server都是⼀个集群**。
2. Service作为服务提供者向Eureka中注册服务，Eureka接受到注册事件会在**集群和分区中进⾏数据同步**，Client作为消费端（服务消费者）可以从Eureka中获取到服务注册信息，进⾏服务调⽤。
3. 微服务启动后，会周期性地向Eureka**发送⼼跳**（默认周期为30秒）以续约⾃⼰的信息
4. Eureka在⼀定时间内**（默认90秒）没有接收**到某个微服务节点的⼼跳，Eureka将会注销该微服务节点
5. Eureka Client**会缓存Eureka Server中的信息**。即使所有的Eureka Server节点都宕掉，服务消费者依然可以使⽤缓存中的信息找到服务提供者



**Eureka缓存**

> 新服务上线后，服务消费者**不能立即访问**到刚上线的新服务，需要过⼀段时间后才能访问？或是将服务下线后，服务还是会被调⽤到，⼀段时候后**才彻底停⽌服务**，访问前期会导致频繁报错！

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxmk97q0j30vw0j6gmu.jpg" alt="image-20210103233902439" style="zoom:50%;" />

​	服务注册到注册中⼼后，服务实例信息是**存储在Registry表**中的，也就是内存中。但Eureka为了提⾼响应速度，在内部做了优化，加⼊了两层的缓存结构，将Client需要的实例信息，直接缓存起来，获取的时候直接从缓存中拿数据然后响应给 Client。 

- 第⼀层缓存是**readOnlyCacheMap**，采⽤**ConcurrentHashMap**来存储数据的，主要负责定时与readWriteCacheMap进⾏数据同步，默认同步时间为 **30** 秒⼀次。

- 第⼆层缓存是**readWriteCacheMap**，采⽤**Guava**来实现缓存。缓存过期时间默认为**180**秒，当服务**下线、过期、注册、状态变更**等操作都会清除此缓存中的数据。

- 如果两级缓存都无法查询，会**触发缓存的加载**，从存储层拉取数据到缓存中，然后再返回给 Client。

  Eureka之所以设计⼆级缓存机制，也是为了**提⾼ Eureka Server 的响应速度**，缺点是缓存会导致 Client**获取不到最新的服务实例信息**，然后导致⽆法快速发现新的服务和已下线的服务。

**解决方案**

- 我们可以**缩短读缓存的更新时间**让服务发现变得更加及时，或者**直接将只读缓存关闭**，同时可以缩短客户端如ribbon服务的定时刷新间隔，多级缓存也导致C层⾯（数据⼀致性）很薄弱。
- Eureka Server 中会有**定时任务去检测失效**的服务，将服务实例信息从注册表中移除，也可以将这个失效检测的**时间缩短**，这样服务下线后就能够及时从注册表中清除。

**自我保护机制开启条件**

- 期望最小每分钟能够续租的次数（实例* 频率 * 比例==10* 2 *0.85）
- 期望的服务实例数量（10）

**健康检查**

- Eureka Client 会定时发送心跳给 Eureka Server 来证明自己处于健康的状态

- 集成SBA以后可以把所有健康状态信息一并返回给eureka

  

#### Feign / Ribbon

- Feign 可以与 Eureka 和 Ribbon 组合使用以支持负载均衡，
- Feign 可以与 Hystrix 组合使用，支持熔断回退
- Feign 可以与ProtoBuf实现快速的RPC调用

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmbxsh2rfnj30uo0fgmxz.jpg" alt="img" style="zoom:80%;" />

- **InvocationHandlerFactory 代理**

  采用 JDK 的动态代理方式生成代理对象，当我们调用这个接口，实际上是要去调用远程的 HTTP API

- **Contract 契约组件**

  比如请求类型是 GET 还是 POST，请求的 URI 是什么

- **Encoder 编码组件 \ Decoder 解码组件**

  通过该组件我们可以将请求信息采用指定的编码方式进行编解码后传输

- **Logger 日志记录**

  负责 Feign 中记录日志的，可以指定 Logger 的级别以及自定义日志的输出

- **Client 请求执行组件**

  负责 HTTP 请求执行的组件，Feign 中默认的 Client 是通过 JDK 的 HttpURLConnection 来发起请求的，在每次发送请求的时候，都会创建新的 HttpURLConnection 链接，Feign 的性能会很差，可以通过扩展该接口，使用 Apache HttpClient 等基于连接池的高性能 HTTP 客户端。

- **Retryer 重试组件**

  负责重试的组件，Feign 内置了重试器，当 HTTP 请求出现 IO 异常时，Feign 会限定一个最大重试次数来进行重试操作。

- **RequestInterceptor 请求拦截器**

  可以为 Feign 添加多个拦截器，在请求执行前设置一些扩展的参数信息。

**Feign最佳使用技巧**

- 继承特性

- 拦截器

  比如添加指定的请求头信息，这个可以用在服务间传递某些信息的时候。

- GET 请求多参数传递

- 日志配置

  FULL 会输出全部完整的请求信息。

- 异常解码器

  异常解码器中可以获取异常信息，而不是简单的一个code，然后转换成对应的异常对象返回。

- 源码查看是如何继承Hystrix

  HystrixFeign.builder 中可以看到继承了 Feign 的 Builder，增加了 Hystrix的SetterFactory， build 方法里，对 invocationHandlerFactory 进行了重写， create 的时候**返回HystrixInvocationHandler**， 在 invoke 的时候**会将请求包装成 HystrixCommand** 去执行，这里就自然的集成了 Hystrix



**Ribbon**

<img src="http://s0.lgstatic.com/i/image2/M01/93/96/CgotOV2Nux-AO2PcAAEcl4M1Zi4629.png" alt="img" style="zoom: 50%;" />



**使用方式**

- **原生 API**，Ribbon 是 Netflix 开源的，没有使用 Spring Cloud，需要使用 Ribbon 的原生 API。

- **Ribbon + RestTemplate**，整合Spring Cloud 后，可以基于 RestTemplate 提供负载均衡的服务

- **Ribbon + Feign**

  <img src="http://s0.lgstatic.com/i/image2/M01/93/76/CgoB5l2NuyCALoefAAAdV1DlSHY088.png" alt="img" style="zoom: 67%;" />

**负载均衡算法**

- RoundRobinRule 是**轮询的算法**，A和B轮流选择。

- RandomRule 是**随机算法**，这个就比较简单了，在服务列表中随机选取。

- BestAvailableRule 选择一个最**小的并发请求 server**

**自定义负载均衡算法**

- 实现 Irule 接口
- 继承 AbstractLoadBalancerRule 类

**自定义负载均衡使用场景**（核心）

- **灰度发布**

  灰度发布是能够平滑过渡的一种发布方式，在发布过程中，先发布一部分应用，让指定的用户使用刚发布的应用，等到测试没有问题后，再将其他的全部应用发布。如果新发布的有问题，只需要将这部分恢复即可，不用恢复所有的应用。

- **多版本隔离**

  多版本隔离跟灰度发布类似，为了兼容或者过度，某些应用会有多个版本，这个时候如何保证 1.0 版本的客户端不会调用到 1.1 版本的服务，就是我们需要考虑的问题。

- **故障隔离**

  当线上某个实例发生故障后，为了不影响用户，我们一般都会先留存证据，比如：线程信息、JVM 信息等，然后将这个实例重启或直接停止。然后线下根据一些信息分析故障原因，如果我能做到故障隔离，就可以直接将出问题的实例隔离，不让正常的用户请求访问到这个出问题的实例，只让指定的用户访问，这样就可以单独用特定的用户来对这个出问题的实例进行测试、故障分析等。



#### Hystrix / Sentinel

**服务雪崩场景**

自己即是服务消费者，同时也是服务提供者，同步调用等待结果导致资源耗尽

**解决方案**

服务方：扩容、限流，排查代码问题，增加硬件监控

消费方：使用Hystrix资源隔离，熔断降级，快速失败

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmby7y9ykzj30wr0ehac5.jpg" alt="img" style="zoom:150%;" />

**Hystrix断路保护器的作用**

- **封装请求**会将用户的操作进行统一封装，统一封装的目的在于进行统一控制。
- **资源隔离限流**会将对应的资源按照指定的类型进行隔离，比如**线程池**和**信号量**。
  - 计数器限流，例如5秒内技术1000请求，超数后限流，未超数重新计数
  - 滑动窗口限流，解决计数器不够精确的问题，把一个窗口拆分多滚动窗口
  - 令牌桶限流，类似景区售票，售票的速度是固定的，拿到令牌才能去处理请求
  - 漏桶限流，生产者消费者模型，实现了恒定速度处理请求，能够绝对防止突发流量
- **失败回退**其实是一个备用的方案，就是说当请求失败后，有没有备用方案来满足这个请求的需求。
- **断路器**这个是**最核心**的，，如果断路器处于打开的状态，那么所有请求都将失败，执行回退逻辑。如果断路器处于关闭状态，那么请求将会被正常执行。有些场景我们需要手动**打开断路器强制降级**。
- **指标监控**会对请求的生**命周期进行监控**，请求成功、失败、超时、拒绝等状态，都会被监控起来。

**Hystrix使用上遇到的坑**

- 配置可以对接**配置中心**进行动态调整

  Hystrix 的配置项非常多，如果不对接配置中心，所有的配置只能在代码里修改，在集群部署的难以应对紧急情况，我们项目只设置一个 CommandKey，其他的都在配置中心进行指定，紧急情况如需隔离部分请求时，只需在配置中心进行修改以后，强制更新即可。

- 回退逻辑中可以**手动埋点**或者通过**输出日志**进行告警

  当请求失败或者超时，会执行回退逻辑，如果有大量的回退，则证明某些服务出问题了，这个时候我们可以在回退的逻辑中进行埋点操作，上报数据给监控系统，也可以输出回退的日志，统一由日志收集的程序去进行处理，这些方式都可以将问题暴露出去，然后通过实时数据分析进行告警操作

- 用 **ThreadLocal**配合**线程池隔离**模式需当心

  当我们用了线程池隔离模式的时候，被隔离的方法会包装成一个 Command 丢入到独立的线程池中进行执行，这个时候就是从 A 线程切换到了 B 线程，ThreadLocal 的数据就会丢失

- **Gateway中**多用信号量隔离

  网关是所有请求的入口，路由的服务数量会很多，几十个到上百个都有可能，如果用线程池隔离，那么需要创建上百个独立的线程池，开销太大，用信号量隔离开销就小很多，还能起到限流的作用。
  
  

[^常见问题]: Hystrix的超时时间要⼤于Ribbon的超时时间，因为Hystrix将请求包装了起来，特别需要注意的是，如果Ribbon开启了重试机制，⽐如重试3 次，Ribbon 的超时为 1 秒，那么Hystrix 的超时时间应该⼤于 3 秒，否则就会出现 Ribbon 还在重试中，⽽ Hystrix 已经超时的现象。



**Sentinel** 

> Sentinel是⼀个⾯向云原⽣微服务的流量控制、熔断降级组件。
>
> 替代Hystrix，针对问题：服务雪崩、服务降级、服务熔断、服务限流

Hystrix区别：

- 独⽴可部署Dashboard（基于 Spring Boot 开发）控制台组件
- 不依赖任何框架/库，减少代码开发，通过UI界⾯配置即可完成细粒度控制

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmbza4zixbj30kl09sq4p.jpg" alt="image-20210104212151598" style="zoom:80%;" />

**丰富的应⽤场景**：Sentinel 承接了阿⾥巴巴近 10 年的双⼗⼀⼤促流量的核⼼场景，例如秒杀、消息削峰填⾕、集群流量控制、实时熔断下游不可⽤应⽤等。

**完备的实时监控**：可以看到500 台以下规模的集群的汇总也可以看到单机的秒级数据。

**⼴泛的开源⽣态：**与 SpringCloud、Dubbo的整合。您只需要引⼊相应的依赖并进⾏简单的配置即可快速地接⼊ Sentinel。

**区别：**

- Sentinel不会像Hystrix那样放过⼀个请求尝试⾃我修复，就是明明确确按照时间窗⼝来，熔断触发后，时间窗⼝内拒绝请求，时间窗⼝后就恢复。
- Sentinel Dashboard中添加的规则数据存储在内存，微服务停掉规则数据就消失，在⽣产环境下不合适。可以将Sentinel规则数据持久化到Nacos配置中⼼，让微服务从Nacos获取。

| #              | Sentinel                                       | Hystrix                       |
| -------------- | ---------------------------------------------- | ----------------------------- |
| 隔离策略       | 信号量隔离                                     | 线程池隔离/信号量隔离         |
| 熔断降级策略   | 基于响应时间或失败比率                         | 基于失败比率                  |
| 实时指标实现   | 滑动窗口                                       | 滑动窗口（基于 RxJava）       |
| 扩展性         | 多个扩展点                                     | 插件的形式                    |
| 限流           | 基于 QPS，支持基于调用关系的限流               | 不支持                        |
| 流量整形       | 支持慢启动、匀速器模式                         | 不支持                        |
| 系统负载保护   | 支持                                           | 不支持                        |
| 控制台         | 开箱即用，可配置规则、查看秒级监控、机器发现等 | 不完善                        |
| 常见框架的适配 | Servlet、Spring Cloud、Dubbo、gRPC             | Servlet、Spring Cloud Netflix |





#### Config / Nacos

> Nacos是阿⾥巴巴开源的⼀个针对微服务架构中**服务发现**、**配置管理**和**服务管理平台**。
>
> Nacos就是**注册中⼼+配置中⼼**的组合（Nacos=Eureka+Confifig+Bus）

**Nacos**功能特性

- 服务发现与健康检查
- 动态配置管理
- 动态DNS服务
- 服务和元数据管理

**保护阈值：**

当服务A健康实例数/总实例数 < 保护阈值 的时候，说明健康实例真的不多了，这个时候保护阈值会被触发（状态true），nacos将会把该服务所有的实例信息（健康的+不健康的）全部提供给消费者，消费者可能访问到不健康的实例，请求失败，但这样也⽐造成雪崩要好，牺牲了⼀些请求，保证了整个系统的⼀个可⽤。

**Nacos** 数据模型（领域模型）

- **Namespace** 代表不同的环境，如开发dev、测试test、⽣产环境prod
- **Group** 代表某项⽬，⽐如爪哇云项⽬
- **Service** 某个项⽬中具体xxx服务
- **DataId** 某个项⽬中具体的xxx配置⽂件

可以通过 Spring Cloud 原⽣注解 `@RefreshScope` 实现配置⾃动更新



#### Bus / Stream

> Spring Cloud Stream 消息驱动组件帮助我们更快速，更⽅便的去构建**消息驱动**微服务的
>
> 本质：屏蔽掉了底层不同**MQ**消息中间件之间的差异，统⼀了**MQ**的编程模型，降低了学习、开发、维护**MQ**的成本，⽬前⽀持Rabbit、Kafka两种消息



#### **Sleuth / Zipkin**

**全链路追踪**

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmc3avezqrj30xb0lw76z.jpg" alt="image-20210104234058218" style="zoom:67%;" />

**Trace ID**：当请求发送到分布式系统的⼊⼝端点时，Sleuth为该请求创建⼀个唯⼀的跟踪标识Trace ID，在分布式系统内部流转的时候，框架始终保持该唯⼀标识，直到返回给请求⽅

**Span ID**：为了统计各处理单元的时间延迟，当请求到达各个服务组件时，也是通过⼀个唯⼀标识SpanID来标记它的开始，具体过程以及结束。

Spring Cloud Sleuth （追踪服务框架）可以追踪服务之间的调⽤，Sleuth可以记录⼀个服务请求经过哪些服务、服务处理时⻓等，根据这些，我们能够理清各微服务间的调⽤关系及进⾏问题追踪分析。

**耗时分析**：通过 Sleuth 了解采样请求的耗时，分析服务性能问题（哪些服务调⽤⽐较耗时）

**链路优化**：发现频繁调⽤的服务，针对性优化等

**聚合展示**：数据信息发送给 Zipkin 进⾏聚合，利⽤ Zipkin 存储并展示数据。



### **安全认证**

- Session

  认证中最常用的一种方式，也是最简单的。存在**多节点session丢失**的情况，可通过**nginx粘性Cookie**和Redis集中式Session存储解决

- HTTP Basic Authentication 

  服务端针对请求头中base64加密的Authorization 和用户名和密码进行**校验**。

- Token

  Session 只是一个 key，**会话信息存储在后端**。而 Token 中会存储用户的信息，然后通过加密算法进行加密，只有服务端才能解密，**服务端拿到 Token 后进行解密获取用户信息**。

- JWT认证

> JWT（JSON Web Token）用户提供用户名和密码给认证服务器，服务器验证用户提交信息的合法性；如果验证成功，会产生并返回一个 Token，用户可以使用这个 Token 访问服务器上受保护的资源。

<img src="http://s0.lgstatic.com/i/image2/M01/AB/87/CgotOV3WUG2ARl98AAD_xcd-ElM857.png" alt="img" style="zoom:70%;" />

1. 认证服务提供认证的 API，校验用户信息，返回认证结果
2. 通过JWTUtils中的RSA算法，生成JWT token，token里封装用户id和有效期
3. 服务间参数通过请求头进行传递，服务内部通过 ThreadLocal 进行上下文传递。
4. Hystrix导致ThreadLocal失效的问题可以通过，重写 Hystrix 的 Callable 方法，传递需要的数据。

**Token最佳实践**

- 设置**较短（合理）的过期时间**。

- 注销的 Token **及时清除**（放入 Redis 中做一层过滤）。

  虽然不能修改 Token 的信息，但是能在验证层面做一层过滤来进行处理。

- 监控 Token 的**使用频率**。

  为了防止数据被别人爬取，最常见的就是监控使用频率，程序写出来的爬虫程序访问频率是有迹可循的 

- 核心功能敏感操作可以使用**动态验证**（验证码）。

  比如提现的功能，要求在提现时再次进行验证码的验证，防止不是本人操作。

- **网络环境、浏览器**信息等识别。

  银行 APP 对环境有很高的要求，使用时如果断网，APP 会自动退出，重新登录，因为网络环境跟之前使用的不一样了，还有一些浏览器的信息之类的判断，这些都是可以用来保证后端 API 的安全。

- **加密密钥**支持动态修改。

  如果 Token 的加密密钥泄露了，也就意味着别人可以伪造你的 Token，可以将密钥存储在配置中心，以支持动态修改刷新，需要注意的是建议在流量低峰的时候去做更换的操作，否则 Token 全部失效，所有在线的请求都会重新申请 Token，并发量会比较大。



### 灰度发布

**痛点：**

- 服务数量多，业务变动频繁，一周一发布

- 灰度发布能降低发布失败风险，**减少影响范围**

  通过灰度发布，先让一部分用户体验新的服务，或者只让测试人员进行测试，等功能正常后再全部发布，这样能降低发布失败带来的影响范围。 

- 当发布出现故障时，可以**快速回滚**，不影响用户

  灰度后如果发现这个节点有问题，那么只需回滚这个节点即可，当然不回滚也没关系，通过灰度策略隔离，也不会影响正常用户

可以通过Ribbon的负载均衡策略进行灰度发布，可以使用更可靠的Discovery

**Discovery**

> 基于Discovery 服务注册发现、Ribbon 负载均衡、Feign 和 RestTemplate 调用等组件的企业级微服务开源解决方案，包括灰度发布、灰度路由、服务隔离等功能

<img src="https://s0.lgstatic.com/i/image3/M01/54/41/CgpOIF3nXSaAB9bRAAE8rktrUyY037.png" alt="img" style="zoom:50%;" />

1. 首先将需要发布的服务从转发过程中移除，等流量剔除之后再发布。

2. 部分机器中的版本进行升级，用户默认还是请求老的服务，通过版本来支持测试请求。

3. 测试完成之后，让新的版本接收正常流量，然后部署下一个节点，以此类推。

```java
grayVersions = {"discovery-article-service":["1.01"]}
```



### 多版本隔离



<img src="https://s0.lgstatic.com/i/image3/M01/54/41/Cgq2xl3nXSeAZMTOAAE2sCaIhPE668.png" alt="img" style="zoom:50%;" />



**本地复用测试服务**-Eureka Zone亮点

​	**region** 地理上的分区，比如北京、上海等

​	**zone** 可以简单理解为 region 内的具体机房

​	在调用的过程中会优先选择相同的 zone 发起调用，当找不到相同名称的 zone 时会选择其他的 zone 进行调用，我们可以利用这个特性来解决本地需要启动多个服务的问题。

[^]: 当你访问修改的服务 A 时，这个服务依赖了 B、C 两个服务，B 和 C 本地没有启动，B 和 C 找不到相同的 zone 就会选择其他的 zone 进行调用，也就是会调用到测试环境部署的 B 和 C 服务，这样一来就解决了本地部署多个服务的问题。



#### **各组件调优**

当你对网关进行压测时，会发现并发量一直上不去，错误率也很高。因为你用的是默认配置，这个时候我们就需要去调整配置以达到最优的效果。

首先我们可以对容器进行调优，最常见的就是**内置的 Tomcat** 容器了，

```java
server.tomcat.accept-count //请求队列排队数
server.tomcat.max-threads //最大线程数
server.tomcat.max-connections //最大连接数
```

**Hystrix** 的信号量（semaphore）隔离模式，并发量上不去很大的原因都在这里，信号量默认值是 100，也就是最大并发只有 100，超过 100 就得等待。

```java
//信号量
zuul.semaphore.max-semaphores //信号量：最大并发数
//线程池
hystrix.threadpool.default.coreSize //最大线程数
hystrix.threadpool.default.maximumSize //队列的大
hystrix.threadpool.default.maxQueueSize //等参数
```

配置**Gateway**并发信息，

```java
gateway.host.max-per-route-connections //每个路由的连接数 
gateway.host.max-total-connections //总连接数
```

调整**Ribbon** 的并发配置，

```java
ribbon.MaxConnectionsPerHost //单服务并发数
ribbon.MaxTotalConnections   //总并发数
```

修改**Feign**默认的HttpURLConnection 替换成 httpclient 来提高性能

```java
feign.httpclient.max-connections-per-route//每个路由的连接数
feign.httpclient.max-connections //总连接数
```

Gateway+配置中心实现动态路由

Feign+配置中心实现动态日志



# **九、分布式篇**

> 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。

### **发展历程**

- 入口级负载均衡
  - 网关负载均衡
  - 客户端负载均衡
- 单应用架构
  - 应用服务和数据服务分离
  - 应用服务集群
  - 应用服务中心化SAAS

- 数据库主备读写分离
  - 全文搜索引擎加快数据统计
  - 缓存集群缓解数据库读压力
  - 分布式消息中间件缓解数据库写压力
  - 数据库水平拆分适应微服务
  - 数据库垂直拆分解决慢查询

- 划分上下文拆分微服务
  - 服务注册发现（Eureka、Nacos）
  - 配置动态更新（Config、Apollo）
  - 业务灰度发布（Gateway、Feign）
  - 统一安全认证（Gateway、Auth）
  - 服务降级限流（Hystrix、Sentinel）
  - 接口检查监控（Actuator、Prometheus）
  - 服务全链路追踪（Sleuth、Zipkin）



### CAP

- **一致性**（2PC、3PC、Paxos、Raft）
  - 强一致性：**数据库一致性**，牺牲了性能
    - **ACID**：原子性、一致性、隔离性、持久性
  - 弱一致性：**数据库和缓存**，**延迟双删、重试**
  - 单调读一致性：**缓存一致性**，ID或者IP哈希
  - 最终一致性：**边缘业务**，消息队列
- **可用性**（多级缓存、读写分离）
  - **BASE** 基本可用：限流导致响应速度慢、降级导致用户体验差
    - Basically Availabe 基本可用  
    - Soft state 软状态
    - Eventual Consistency 最终一致性
- 分区容忍性（一致性Hash解决扩缩容问题）



### 一致性

#### XA方案

**2PC**协议：两阶段提交协议，P是指**准备**阶段，C是指**提交**阶段

- 准备阶段：询问是否可以开始，写Undo、Redo日志，收到响应
- 提交阶段：执行Redo日志进行**Commit**，执行Undo日志进行**Rollback** 



**3PC**协议：将提交阶段分为**CanCommit**、**PreCommit**、**DoCommit**三个阶段

**CanCommit**：发送canCommit请求，并开始等待

**PreCommit**：收到全部Yes，写Undo、Redo日志。超时或者No，则中断

**DoCommit**：执行Redo日志进行**Commit**，执行Undo日志进行**Rollback** 

区别是第二步，参与者**自身增加了超时**，如果**失败可以及时释放资源**



#### **Paxos算法**

> 如何在一个发生异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致

​	参与者（例如Kafka）的一致性可以由协调者（例如Zookeeper）来保证，**协调者的一致性就只能由Paxos保证了**

Paxos算法中的角色：

- **Client**：客户端、例如，对分布式文件服务器中文件的写请求。
- **Proposer**：提案发起者，根据Accept返回选择最大N对应的V，发送[N+1,V]
- **Acceptor**：决策者，Accept以后会拒绝小于N的提案，并把自己的[N,V]返回给Proposer
- **Learners**：最终决策的学习者、学习者充当该协议的复制因素

```java
//算法约束
P1:一个Acceptor必须接受它收到的第一个提案。
//考虑到半数以上才作数，一个Accpter得接受多个相同v的提案
P2a:如果某个v的提案被accept，那么被Acceptor接受编号更高的提案必须也是v
P2b:如果某个v的提案被accept，那么从Proposal提出编号更高的提案必须也是v
//如何确保v的提案Accpter被选定后，Proposal都能提出编号更高的提案呢
针对任意的[Mid,Vid]，有半数以上的Accepter集合S，满足以下二选一：
  S中接受的提案都大于Mid
  S中接受的提案若小于Mid，编号最大的那个值为Vid
```

![image-20210112225118095](https://tva1.sinaimg.cn/large/008eGmZEly1gmlato63bnj319m0u0wmi.jpg)

面试题：如何保证Paxos算法活性

​	假设存在这样一种极端情况，有两个Proposer依次提出了一系列编号递增的提案，导致最终陷入死循环，没有value被选定

- **通过选取主Proposer**，规定只有主Proposer才能提出议案。只要主Proposer和过半的Acceptor能够正常网络通信，主Proposer提出一个编号更高的提案，该提案终将会被批准。
- 每个Proposer发送提交提案的时间设置为**一段时间内随机**，保证不会一直死循环



#### **ZAB算法**

#### Raft算法

> Raft 是一种为了管理复制日志的一致性算法

Raft使用**心跳机制**来触发选举。当server启动时，初始状态都是**follower**。每一个server都有一个定时器，超时时间为election timeout（**一般为150-300ms**），如果某server**没有超时的情况下收到**来自领导者或者候选者的任何消息，**定时器重启**，如果超时，它就**开始一次选举**。

**Leader异常**：异常期间Follower会超时选举，完成后Leader比较彼此步长

**Follower异常：**恢复后直接同步至Leader当前状态

**多个Candidate：**选举时失败，失败后超时继续选举



#### 数据库和Redis的一致性

**全量缓存保证高效读取**

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gx2u2g0qkvj31500la0uf.jpg" alt="image-20211205121457205" style="zoom: 50%;" />

所有数据都存储在缓存里，读服务在查询时不会再降级到数据库里，所有的请求都完全依赖缓存。此时，因降级到数据库导致的毛刺问题就解决了。但全量缓存并**没有解决更新时的分布式事务**问题，反而把问题放大了。因为全量缓存**对数据更新要求更加严格**，要求所有数据库**已有数据和实时更新**的数据必须完全同步至缓存，不能有遗漏。对于此问题，一种有效的方案是采用**订阅数据库的 Binlog** 实现数据同步

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gx2u9jj094j31c60padiu.jpg" alt="image-20211205121611997" style="zoom:50%;" />

​	现在很多开源工具（如**阿里的 Canal**等）可以模拟主从复制的协议。通过模拟协议读取主数据库的 Binlog 文件，从而获取主库的所有变更。对于这些变更，它们开放了各种接口供业务服务获取数据。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gx2ugzh9ohj31hc0g00ud.jpg" alt="image-20211205121730145" style="zoom:50%;" />

​	将 Binlog 的中间件挂载至目标数据库上，就可以**实时获取该数据库的所有变更数据**。对这些变更数据解析后，便可**直接写入缓存里**。优点还有：

- 大幅提升了读取的速度，降低了延迟

- Binlog 的主从复制是基于 **ACK** 机制， 解决了分布式事务的问题

  如果同步缓存失败了，被消费的 Binlog 不会被确认，下一次会重复消费，数据最终会写入缓存中

**缺点**不可避免：1、增加复杂度 2、消耗缓存资源 3、需要筛选和压缩数据 4、极端情况数据丢失

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gx2u6ltwa4j314s0hs765.jpg" alt="image-20211205121850418" style="zoom:50%;" />

可以通过异步校准方案进行补齐，但是会损耗数据库性能。但是此方案会隐藏中间件使用错误的细节，线上环境前期更重要的是记录日志排查在做后续优化，不能本末倒置。



### 可用性

#### **心跳检测**

> 以**固定的频率**向其他节点汇报当前节点状态的方式。收到心跳，说明网络和节点的状态是健康的。心跳汇报时，一般会携带一些附加的**状态、元数据，以便管理**

**周期检测心跳机制**：超时未返回

**累计失效检测机制**：重试超次数



#### **多机房实时热备**

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gx2ufbmlkmj31fw0mutc1.jpg" alt="image-20211205122631299" style="zoom:50%;" />

两套缓存集群可以分别部署到不同城市的机房。读服务也相应地部署到不同城市或不同分区。在承接请求时，不同机房或分区的读服务只依赖同样属性的缓存集群。此方案有两个好处。

1. **提升了性能。**读服务不要分层，读服务要尽可能地和缓存数据源靠近。
2. **增加了可用。**当单机房出现故障时，可以秒级将所有流量都切换至存活的机房或分区

此方案虽然带来了性能和可用性的提升，但代价是资源成本的上升。











### 分区容错性

> 分布式系统对于错误包容的能力

通过限流、降级、兜底、重试、负载均衡等方式增强系统的健壮性

#### 日志复制

![image-20210114154435003](https://i.loli.net/2021/01/14/fmYEJy9N7Zjp2Xd.png)

1. **Leader**把指令添加到日志中，发起 RPC 给其他的服务器，让他们复制这条信息
2. **Leader**会不断的重试，直到所有的 Follower响应了ACK并复制了所有的日志条目
3. 通知所有的**Follower**提交，同时Leader该表这条日志的状态，并返回给客户端



#### **主备（Master-Slave）**

​	主机宕机时，备机接管主机的一切工作，主机恢复正常后，以自动（**热备**）或手动（**冷备**）方式将服务切换到主机上运行，**Mysql**和**Redis**中常用。

​	MySQL之间数据复制的基础是**二进制日志文件**（binary log fifile）。它的数据库中所有操作都会以**“事件”**的方式记录在二进制日志中，其他数据库作为slave通过一个**I/O线程与主服务器保持通信**，并**监控**master的二进制日志文件的变化，如果发现master二进制日志文件**发生变化**，则会把变化复制到自己的**中继日志**中，然后slave的一个SQL线程会把相关的“事件”**执行**到自己的数据库中，以此实现从数据库和主数据库的**一致性**，也就实现了**主从复制**



#### **互备（Active-Active）**

​	指两台主机**同时运行**各自的服务工作且**相互监测**情况。在数据库高可用部分，常见的互备是**MM**模式。MM模式即**Multi-Master**模式，指一个系统存在多个master，每个master都具有**read-write**能力，会根据**时间戳**或**业务逻辑**合并版本。



#### **集群（Cluster）模式**

​	是指有多个节点在运行，同时可以通过主控节点**分担服务**请求。如Zookeeper。集群模式需要解决主控节点**本身的高可用**问题，一般采用主备模式。



### 分布式事务

#### XA方案 

**两阶段提交** | **三阶段提交**

- 准备阶段的资源锁定，存在性能问题，严重时会造成死锁问题
- 提交事务请求后，出现网络异常，部分数据收到并执行，会造成一致性问



#### TCC方案 

**Try Confirm Cancel / 短事务**

- **Try** 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行**锁定或者预留**

- **Confirm** 阶段：这个阶段说的是在各个服务中**执行实际的操作**

- **Cancel** 阶段：如果任何一个服务的业务方法执行出错，那么就需要**进行补偿**/回滚

  

#### **Saga方案** 

事务性补偿 / 长事务

- 流程**长**、流程**多**、调用第三方业务

  

#### **本地消息表（eBay）**

#### **MQ最终一致性**	

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gmr1k3dfbxj31h00pkjy8.jpg" alt="image-20210117220405706" style="zoom:50%;" />

比如阿里的 RocketMQ 就支持消息事务（核心：**双端确认，重试幂等**）

1. A**(订单)** 系统先发送一个 **prepared** 消息到 mq，prepared 消息发送失败则取消操作不执行了
2. 发送成功后，那么执行本地事务，执行成功和和失败发送**确认和回滚**消息到mq
3. 如果发送了确认消息，那么此时 B**(仓储)** 系统会接收到确认消息，然后执行本地的事务
4. mq 会自动**定时轮询**所有 prepared 消息回调的接口，确认事务执行状态
5.  B 的事务失败后自动**不断重试**直到成功，达到一定次数后发送报警由人工来**手工回滚**和**补偿**



#### 最大努力通知方案（订单 -> 积分）

1. 系统 A 本地事务执行完之后，发送个消息到 MQ；
2. 这里会有个专门消费 MQ 的**最大努力通知服务**，接着调用系统 B 的接口；
3. 要是系统 B 执行失败了，就定时尝试重新调用系统 B，**反复 N 次**，最后还是不行就**放弃**



你找一个严格**资金**要求绝对不能错的场景，你可以说你是用的 **TCC 方案**；

如果是一般的分布式事务场景，例如**积分**数据，可以用可靠消息**最终一致性方案**

如果分布式场景**允许不一致**，可以使用最大努力通知方案



### 面试题

#### 分布式Session实现方案

- 基于JWT的Token，数据从cache或者数据库中获取
- 基于Tomcat的Redis，简单配置conf文件
- 基于Spring的Redis，支持SpringCloud和Springb