# 一、自我介绍

你好，我叫刘旺杰，19年毕业后我就来杭州这边工作了，最近两年就职于谊品到家科技有限公司，在商品团队做Java业务开发。

我负责的两个微服务是商品基础数据系统和商品异步任务系统。

在电商系统中，电商商品系统负责了商品的生命周期管理，合理的电商商品系统设计有助于更好地帮助用户获取信息，利于平台进行商品管理。

# 二、职业技能

## JDK-集合

## JDK-ThreadLocal

## JDK-线程池

## JDK-AQS

## MySQL-索引

## MySQL-锁

## MySQL-MVCC

幻读

## MySQL-SQL调优

## Redis-底层数据结构

## Redis-持久化机制



## 开源框架-SpringBoot

## 开源框架-gRPC

## 开源框架-Pulsar

## 开源框架-Mybatis

## 开源框架-Netty

## [开源框架-Elasticsearch](../ElasticSearch/ElasticSearch-Interview.md)

## JVM-Java内存模型（JMM）

## JVM-类加载机制

## JVM-GC算法和垃圾回收器

## JVM-线上 JVM 的调优

在异步任务系统中会生成很多的临时文件，





# 三、项目介绍

## 3.1 谊品自研ERP系统

谊品自研ERP系统，摆脱对富基、康铭泰克等第三方ERP系统的依赖。搭建数据中台和业务中台，建立数据服务层并开放数据服务能力，提供运营分析和决策支撑，以达到数据反哺业务的目标。商品中心作为业务中台之一，负责提供管理商品数据，同时要够快速支撑供应链货品和多渠道销售商品的创建和管理，及时响应业务诉求，实现业务快速扩张。

**涉及技术：**gRPC、springBoot、myBatis、pulsar、mysql、redis、elasticSearch、apollo

**岗位职责**：负责相关技术方案设计与落地、系统编码实现

### 快捷开营业部

监听商户组织变更消息，设计新开营业部后商品的同步功能，极大的缩减运营人力成本。

### 门店品类箱子

为线下门店提供品类运营能力和销售管控能力，支持清洗负毛利商品，指导清退门店非畅销品。

### 问题的定位

1. 确认刚才是否有过代码变更和部署，因为有比较高的概率是刚才变更的代码又搞坏了；（是否是由发版导致）
2. 追踪链路日志看链路是否有异常报出；（看下链路中有没有报错）
3. 通过RPC的控制台调用看接口输入输出是否符合预期；（查最终db的数据对不对）
4. 追踪关键方法的入参和出参，看是否有问题；（如果db数据对，但接口返回数据不对，那么就要看是哪个方法出问题了）
5. 定位到方法细节后，推理逻辑是否有问题；（找到对方法看逻辑是否又问题）
6. 如果无法通过推理，那就最后一招，回放异常流量debug， 这样肯定能够找到原因；（实在不行debug 一步一步看）

### 查询性能优化

某个链路耗时比较长，基于链路追踪进行性能优化步骤：

1. 通过实际流量制造一个耗时较高的trace;
2. 通过traceId进行链路耗时分析．看清楚耗时最多的原因；
3. 针对对原因找解决方案，可能的方案有
   1. 减少数据访问次数或者计算量，常见手段是增加cache;
     - 机器本地缓存
     - 分布式缓存redis
     - 页面缓存
   2. 增强处理速度，比如多线程加速；
   3. 减少循环调用次数。比如请求合并后再分发；
   4. 减少数据处理范围，比如减少查询内容，异步加载分页；
   5. 逻辑简化，比如逻辑进行优化，或者非核心逻辑异步化等；
4. 改掉以后，回放同样的case，看性能消耗是否满足预期，不满足预期继续优化；

## 3.2 谊品供应链商品系统

随着业务的迅速发展，商品系统中的一些弊端也逐渐暴露出来，例如 数据扩散问题导致地点纬度数据量过大，影响DB查询性能；商品模型定义不够明确，接口透出字段大而全，接口性能无法保证；商品中台和门店端都存在导入导出场景，并且导入的细分场景复杂繁多，需要进行功能的收口 等。

因为供应链测和销售测所关注的商品属性存在很大差异，之前这些字段都耦合在一张表里，所以要进行表的垂直拆分。同时商品系统也进行了更细粒度的微服务拆分，分离出商品基础数据、销售商品、供应链货品三个微服务系统，它们提供核心的原子服务，由上层系统进行组合编排然后对外输出业务能力。

**涉及技术**： gRPC、springBoot、pulsar、myBatisPlus、powerJob、elasticSearch、shardingjdbc、redisson、nacos

**岗位职责**：商品基础数据负责人 负责核心的领域设计与编码、CodeReview、项目发布计划以及线上运维，以及推进外域进行接口切换

对需求理解，技术选型，代码的质量把控，线上问题处理，线上的相关指标，后期的迭代和重构。

## 主导项目1-商品基础数据服务DDD实践

### 项目背景和简介

最开始的时候商品对外只有一个模型，叫做商品模型，但是后来在业务演进过程中，供应链侧（库存、仓店、采购、调拨），销售测（营销、导购、交易、订单）所关注的商品属性有很大差异。比如：货品（采购生命周期、物流模式、供应商编码，采购单位、税率），销售商品（上下架状态、销售单位、换算比例）

但是我们在模型层面没有做很好的区分，导致接口透出的数据大而全，这样导致接口的性能在压测的时候得不到保障，外域在接口的使用上也存也安全隐患（后续做了应用层面的接口粒度鉴权）。

因此为了更好的区分和使用供应链货品模型和销售商品模型，所以对老的商品系统进行了更细粒度的微服务拆分。首先是最底层的数据表结构根据销售商品和供应链货品做了垂直拆分，拆成了商家纬度的商家商品表和商家货品表，地点纬度的地点商品表和地点货品表，同时由于地点纬度的数据量较大，所以进行了分库分表。我们的查询场景更多是在某个门店下查找符合条件的商品，所以我们将地点id作为分库分表健，一共分了 4库32表。

上层的的系统也跟着拆分成了销售商品系统和供应链货品系统，商品和货品的基础属性作为商品的主档，将商品主档、品牌、类目、属性标签 这几部分划分到商品基础数据服务系统中，这样就构成了三个最底层的系统，提供核心的原子服务。

其他还有 聚合ES查询的系统、监听binlog同步ES的系统、 处理商品异步任务的系统 和 对接业务中台的系统。

### 模型的好处 

模型的作用就是简化认知的东西，可以简化人对事物的认识过程。常见的模型不止领域模型，还有 JMM内存模型、ISO的网络模型。

刚入职的时候，如何能够快速的了解业务。通过模型就是一个比较好的切入点。它里面包含了领域的实体，以及实体之间的关联关系，一下子就知道了整个业务是怎么流转的。如果一开始就去看代码，这样很容易陷入一些代码细节中，很难看到业务的全貌。但如果通过模型对业务有了一个整体的认识之后，心中大致有了一个框架，这时候再去看代码就更像是一个查漏补缺的过程，看一些开源框架的源码也是类似，也去了解主干流程，之后再选择性的了解细节。

### 落地方案

**我们采用的是CQRS模式进行DDD落地。**

**在应用层面做了更细粒度的接口鉴权和限流**

### CQRS

CQRS表示的是“命令和查询责任分离”，它用途是在于将领域模型与查询功能进行分离，让一些复杂的查询摆脱领域模型的限制，以更为简单的 DTO 形式展现查询结果。个人理解核心是为了分离数据存储结构，让开发者按照查询的功能与需要自由的选择数据方式。

这个和MySQL读写分离很相似。但是MySQL的读写分离强调的是物理数据库的分离，而CQRS更关注的是模型层面的分离。

MySQL进行了读写分离后，主从库的数据结构还是一样的，是为了分摊了数据库的压力，提高数据库层面的性能和可用性。

但是CQRS强调的是 command与query访问的数据模型不同，根据command与query的不同特性设计对应的数据模型。比如command更强调模型的范式化、完整性约束等；而query的模型更强调性能，可以不过多地受范式的约束、可以更多的数据冗余。

例如 command 用关系数据库，query用NoSQL数据库。当然command与query使用同一个物理数据库，query使用View也是可以的，这是与读写分离的区别。

> 也可以使用不同的物理数据库，如果数据表结构一样的话就类似于MySQL的读写分离。
>
> 表结构不同的话，就需要对应的handler 来处理 command事件，来更新query的的查询结果，这种方式是event sourcing。
>
> commond 和 query 可以部署在不同的应用上，也可以在同一个应用内的不同接口上体现，核心的目标是模型的分离。

**为什么采用CQRS？**

首先最终目的一定是性能的提升。CQRS允许读取和写入的负载独立缩放，这样可以减少锁的竞争。同时它可以分离读写数据的存储结构，读和写使用不同的存储方式。比如command用db，query用es，这样可以针对性的进行优化。

虽然我们现在只在模型上分离，没有在存储上做分离，但是如果以后要分离存储的话，模型分离也是第一步。还有一点就是领域模型对象在构建时会有诸多检查和多次对象转换，使用query模型可以尽可能的避免这些点。

### DDD-准备设计阶段

**为什么采用DDD？**

- DDD可以很好的指导微服务拆分和演进。

  通过DDD 的方法来建立领域模型，划分领域边界，再根据这些领域边界从业务视角来划分微服务边界。可以很好地实现微服务的“高内聚、低耦合”。这样就解决了拆分的问题；其次微服务内部的实体都是以聚合为单位的（传统可能是按照类型比如service，pojo），聚合内实现了高内聚的业务逻辑。如果该微服务后边需要更细粒度的拆分，就可以按照聚合进行拆分，这样可以更好的进行服务演进。

  比如老商品拆分后分为商品基础数据服务、销售商品服务、供应链货品服务，而商品基础数据服务中的主档、类目、品牌、属性标签都是按聚合划分的，后期演进时可以拆成独立的服务。

- 实体采用充血模型对核心业务进行沉淀。

  这点是DDD和三层架构的一个比较大的区别，传统的services层中承接全部的业务逻辑，其中包含很多重复但比较核心的业务逻辑

  实体采用充血模型，在实体类内部实现实体相关的所有业务逻辑，实现的形式是实体类中的方法。实体是微服务的原子业务逻辑单元。在设计时我们主要考虑实体自身的属性和业务行为，实现领域模型的核心基础能力。不必过多考虑外部操作和业务流程，这样才能保证领域模型的稳定性。

  DDD 提倡富领域模型，尽量将业务逻辑归属到实体对象上，实在无法归属的部分则设计成领域服务。领域服务会对多个实体或实体方法进行组装和编排，实现跨多个实体的复杂核心业务逻辑。对于严格分层架构，如果单个实体的方法需要对应用层暴露，则需要通过领域服务封装后才能暴露给应用服务。

- 使用CQRS来

- 分层

- 建立通用语言减少沟通成本。

  确保业务术语没有二义性，可以帮助开发、测试、产品、运营、业务更好沟通。比如商品域中的主档、商品、货品 这些名词都对应领域实体，像 上架、下架、在售、停售 这些动词都代表实体的行为。这个在商品域里边可能体现的不够明显，但是在金融、保险这类专业词汇较多的行业内有很好的效果。

DDD还有很多优点，我们可以结合自身业务情况进行取舍。



为了解决代码散乱，无法体现出业务与领域模型的这种情况。我们采用DDD，因为DDD可以直接在模型层了解到业务逻辑与领域行为。

 然后就是DDD的模式可以解耦。 领域层和仓储层解耦，领域层无须关注具体存储哪张表。 让开发者对业务有更深层次的了解。



我说一下我们的重写方案吧。首先是与产品和测试对齐重写方针。
在准备阶段我们与产品首先确定商户的业务模型。比如确定销售商模型、营业部模型、公司模型等等。依照公司现有的业务逻辑。
依旧是准备阶段，开发会去收集各域依赖的接口。 我们当时的操作是直接拿到外域的代码权限。拉下来查看依赖的接口以及出入参。 然后形成表格之后与外域对接口。明确接口用途。这一步非常重要，它将直接影响你新接口的设计。

**接口梳理**

**消息梳理**

### DDD-落地实践阶段

首先我们在工程中采用CQRS架构，将中台管理服务与对外原子查询服务分开，使用两个工程来实现。分别是phoenix和owl工程。两个工程都可以访问数据库。我们这么设计的目的是希望它们发布不会影响。 然后区别于管理服务调用原子服务，它没有分布式事务问题。

接口定义上，我们针对不同的业务实体定义了不同的接口。首先定义了一个基础属性的接口。该接口只包含基本信息，然后就是根据不同类型分别设计详细信息接口。然后就是关联关系接口。
并行开发。
然后就是外域切换。
系统下线。
数据模型变更。

**代码落地**

在工程中，我们使用CQRS架构，充血模型来落地。首先明确聚合。例如门店、仓库、公司等等是聚合根。 聚合根：具有全局标识 实体：只在聚合内有唯一标识。
在分层架构中，接口层调用应用服务层，应用服务层仅负责业务逻辑，比如说调用领域对象的行为。 例如现在有一个保存门店的方法。 在内存中构建出来一个门店对象，然后调用门店对象的行为来对内存中的领域对象进行修改。 同时行为会产生事件。 事件会存储在对象内部。当调用仓储层store方法提交事务之后，会调用事件发布将这些事件发布出去。 后续会有监听来实现 缓存清理和消息发送。

**数据同步**



### DDD-遇到问题

第一个就是消息一致性问题。因为消息是由事件来驱动。 而消息又是业务中比较重要的一环。万一事务提交了但是消息因为各种原因发送失败了，那么对于下游业务就会存在问题。存在分布式一致性问题。

**解决：**
对于业务消息，我们更改了方案，采用本地消息表，在事务中写表。然后异步线程会去发送这条消息。并将其置为已发送。 当时也考虑要引入其他团队开发的 分布式流程任务引擎来解决一致性问题，考虑到成本比较高，且我们编辑的情况不多，就采用了本地消息表。





针对接口大而全的问题，梳理业务与外域诉求，细分接口职能，定义核心原子服务能力。

毋庸置疑「领域」在 DDD 中占据了核心的地位，DDD 通过领域对象之间的交互实现业务逻辑与流程，并通过分层的方式将业务逻辑剥离出来，单独进行维护，从而控制业务本身的复杂度



所以我们才需要用DDD的分层思想去重构一下以上的代码，通过不同的代码分层和规范，拆分出逻辑清晰，职责明确的分层和模块，也便于一些通用能力的沉淀。







## 主导项目2-商品异步任务系统

**使用TTL传递ThreadLocal**

**借鉴tomcat线程池，提高性能**

模版方法模式设计顶层的抽象类来控制流程，具体的业务实现

设计商品异步任务系统，承接业务对商品的大批量变更和数据导出操作。

Excel的导入导出在商品域中里有大量的应用场景，尤其是 导入操作，极大提高了业务人员操作系统的便捷性。 

在原有商品模型里，Excel导入导出处理部分是由PMS工程来完成的，在使用过程中 遇到了一些问题，如：

- 要处理的Excel数据量过大时，会占用过多系统内存，影响机器性能，甚至造成 服务不可用的情况。
- 在处理过程中，遇到机器重启或其他导致处理中断时，没有重试机制，无法保 证任务的执行结果。
- 无法对处理流程进行很好的扩展。
- 用户在操作后，无法感知任务处理的进度。

在本次的商品重构中，针对以上问题进行了一些架构方面的调整。

PMS不再负责Excel导入导出数据的解析校验，这一部分操作由独立的Task工程 来承载，

PMS只提供查询和持久化明细数据的服务。

 Task工程通过接入流程引擎Engine，来提供失败重试机制，以保证任务执行的 结果。通过流程引擎的流程编排服务还可以更方便地对流程编排控制，方便进 一步的扩展。

 后续还会提供对任务的监控，以及执行进度的感知。



## 缓存方案

https://www.51cto.com/article/709819.html

旁路缓存的模式

ES->redis-> db

为解决db的读扩散问题，

es天然分片，而且内部利用倒排索引的形式来加速数据查询。

而且将mysql接入es也非常简单，我们可以通过开源工具 canal 监听mysql的binlog日志变更，再将数据解析后写入es，这样es就能提供近实时的查询能力。

![image-20221010162518053](img/0_面试宝典/image-20221010162518053.png)

## 通用注解缓存查询组件

基于注解设计通用缓存查询组件，不侵入代码，提高接口查询性能。

https://blog.csdn.net/youbl/article/details/113052502

1. 基于注解，不侵入代码，使用Spring AOP实现，轻量级，相比Spring 提供的注解缓存，可扩展性更高一些。

2. kernelCache缓存的删除是基于binlog的，基础架构会将bin log解析发送至消息队列，我们监听消息去处理删缓存，这里也可以做一些延时双删的操作，来保证db缓存的一致性。

3. 还有一些都是默认的Redis缓存实现不支持，需要去实现相关扩展接口的。

   - 默认的RedisCacheManager是使用JDK的序列化，性能差；我们自己的缓存组件也就是kernelCache，是可配置序列化方式，默认gson。

   - 默认的是无法配置不同的缓存过期时间；kernelcache可以根据缓存枚举中的过期时间时间来差异化配置。

   - 默认是同步写缓存的，而kernelCache是异步线程池写缓存。

## 流程引擎重试保证最终一致性

**客户端**

- 业务方引入springboot-starter包，通过SPI加载启动配置类
- 向注册中心注册引擎服务，用来和服务端进行通讯，如流程点火、接受服务端推送任务、回调通知服务端、获取服务端的配置信息等
- 扫描当前实例标识的任务步骤，完成初始化，如多种线程池（接受任务线程池、通知服务端执行结果的线程池、具体的工作线程池）

**服务端**

- 启动向服务注册中心注册引擎点火的服务

- 提供业务配置相关的流程及，向客户端请求步骤信息

  

通过注解的方式将需要进行rpc调用的步骤包装起来，可以由外部系统进行步骤的编排和远程调用。外部系统中可以控制调用的时间以及获取调用结果，还支持步骤失败的重试，以及报警。



流程引擎接入

- 调用方引入依赖包，依赖包里会使用SPI注册一个引擎的服务

- 将需要保证最终一致性的rpc调用包装成执行步骤（processor）。每个processor都有自己的唯一标识

- 将原来多次rpc调用的地方，进行引擎点火，点火需要传入序列化后的入参和指定唯一的流程id（一个流程相当于原来的本地事务）

- 去引擎的管理后台对流程进行步骤配置，步骤之间可以串行也可以并行。

  

流程引擎引擎接入：

配置相关：

1. 配置项目模块，engine 服务端 提供的中台页面配置接入引擎项目对应的项目模块
2. 配置任务流程，在中台页面的对应项目模块中，创建对应的任务流程，每个流程中细分为多个步骤。引擎系统用来 **尽可能** 保证每个任务流程的最终一致性，也就是保证流程里的每个步骤都会执行成功，如果执行失败的话支持任务的重试，如果该任务业务校验上就失败，无论重试多少次都不成功，那么这个任务会告警，需要人工介入处理。
3. 编排任务步骤，任务流程进行任务步骤的编排，可以串行，也可以并行，也可以先并行后串行，这点在系统切换中经常可以用到，比如新系统中数据变更需要发新老的消息，如果切换完成，不再需要发送老的消息，只需要将发送老消息的步骤移除即可，不需要系统发布。

代码开发：

1. 引入 engine-springboot-starter

2. 继承 `AbstractEngineStepHandler` 类实现任务步骤，在 `process()`中定义业务逻辑，并将该类用 `@EngineTask` 注解标注。

   ```java
   //根据updateKernelProduct在任务流程中编排任务
   @EngineTask(value = "updateKernelProduct", desc = "更新主档") 
   ```

3. 在需要保证分布式事务最终一致性的地方，通过 EngineOperateService 对 任务流程 进行点火。任务流程的编码也需要在业务方来指定，并在engine中台进行配置。

## 表结构优化

- **为什么分表？**

> 首先第一点是主表字段过多了。 有很多垃圾字段。
> 另外我们希望推进外域根据业务场景来辨别实际使用的业务实体类型。根据不同的实体用不同的表。还有就是需要去掉一些类型。

- **分表方案**

> 首先是写的问题。原先依赖主键Id。现在分表了，为了兼容性，不同实体Id全局唯一。因此我们采用了分段发号策略。在主表中找到目前已有的最大的Id，加一位，然后根据不同的实体采用不同的号段。基于数据量不大的情况下，不需要引入分布式发号服务。降低复杂度。
> 同时也可以根据号段来区分新老Id与类型。

不要做过早的优化，没事别上来就分100个表，很多时候真用不上。

## 线上机器CPU100%

排查线上机器 CPU 负载 100%与服务不可用问题。

7⽉5号晚上 19点40分 接到电话反馈商品服务不可⽤，销售端相关依赖商品服务的⻚⾯展示空⽩⻚，后台ERP系统商品选品，类⽬接⼝⻚⾯提示系统异常。

以下是问题分析定位的过程： 

**第⼀步 workspace 查看监控，以及杨航反馈的截图，有⼀台商品的Pod的 CPU使⽤率和内存使⽤率飙⾼，单台POD服务不可⽤，并定位这台异常的pod的 容器Ip为 192.168.98.4**

但从反馈的频率来看，商品有26个pod实例，线上每次刷新都是空⽩⻚；从 分析上来看GRPC的负载策略是随机 +轮训，不可能每次都是空⽩⻚，⻚⾯的报错⼏乎是每次刷新必现。带着这个疑点去对⽇志进⾏了分析，从19点40 开 始，按分钟的粒度逐步查询异常的⽇志，排查的过程中发现出现了如下的⽇志：

<img src="img/0_面试常问/image-20220429163601106.png" alt="image-20220429163601106" style="zoom:50%;" />

 获取不到Jedis链接⼀般来说有两种可能： 

**1、Redis的负载很⾼，响应很慢或者没有链接可以分配，会导致客户端的连接池获取不到链接**

**2、应⽤的负载很⾼，CPU没有资源来调度应⽤线程，这个时候也会造成链接Jedis的请求⻓时间没有响应，导致新来的请求把池中的链接资源分配光但不能及时释放链接的场景，也会导致这样的错误。**

从时间上来分析，19点40的节点，应⽤的CPU使⽤已经 250%，cpuLoad全 被占⽤，内存使⽤率也满了，很容易可以分析出这个时间的应⽤线程是没有资源被CPU调度到的，都是挂起的状态，⾃然Jedis的链接是⻓时间没有响应的。推断属于第⼆种场景，于是⼜去看了Redis和MySQL的负载情况（MySQL是正常的 这⾥就不截图了） 可以很明显的看到，Redis除了链接数激增，响应时间和负载都是正常的， ⽽QPS骤降和连接数的上升，说明了在19点40这个点 请求⼀下变少了，但是 连接数增加；验证了我们上述推断，应⽤疯狂的向连接池申请创建链接，但却很⻓时间未释放链接（因为被应⽤Hold住了）

回到Jedis异常的⽇志点，以上可以分析得出Jedis并不是问题的引起⽅，这 是应⽤负载⾼了以后的⼀种报警形式，继续按⼀分钟的粒度往前推移，发现在3 分钟内存在⼤量的如下⽇志：

<img src="img/0_面试常问/image-20220429165357364.png" alt="image-20220429165357364" style="zoom:50%;" />

**这时⼜切换了别的Pod所在的容器的IP，并⽆类似的⽇志，于是猜测有没有 可能是因为服务发现中⼼在获取服务可⽤列表的时候只发现了⼀台实例，导 致所有的流量在某⼀个时间段都路由到了同⼀个服务实例上？如果是这样的话也 就能解释为什么26个Pod只有⼀个pod出问题导致整个商品服务都是不可⽤的 了。带着这个问题⼜重新去看了应⽤的负载和监控：**

可以看到出问题的pod和其他的Pod，CPU使⽤率，内存，⽹络IO流量，磁 盘在出问题的时间点趋势完全是呈反⽐的，看上去就像所有的请求 突然从别的实 例上都转移到了这个pod上。貌似也复核了我们之前的推断，有没有可能是服务 发现的负载出了问题，可⽤服务实例列表的缺失造成的负载不均；从⽽导致单台 应⽤⽆法承载整个集群的请求量？

这个时候带着疑问，⼜去查看了在出问题的点附近 是否存在⼤量的接⼝调 ⽤，或者导⼊导出等耗资源的操作 经过排查我们排除了导⼊导出操作造成的影响，同时可以较为明显的看到19点到 问题发⽣的点并没有接⼝调⽤量的激增。

⼤体分析下来可能就是 服务发现的负载问题，导致了POD压⼒瞬间过载 从 ⽽影响整个服务。

## JVM内存泄漏



## 商品模型

商品中心为各域提供商品基础数据，其重要性毋庸置疑。

从使用层面上来讲，商品中心分为前台和后台，

前台商品，我们可以称之为销售商品，为前台业务，包括到家商品列表，商品 详情，购物车，订单，营销活动等提供基础数据支撑。

后台商品，我们可以称之为供应链货品，为采购、wms、库存等提供基础数据 支撑。

从管理层面上来讲，商品中心分为，商品基础资料（主档）、商家商品、地点 商品，

商品基础资料（主档），控制了商品编码的唯一性，作为商品的一个基础数据 模板；

商家层面，主要为中台商品，提供管理能力，商家商品的变更，影响到下发的 地点商品，商家商品也分为商家货品、商家销售商品。 地点层面，才是真正销售或采购的内容，地点商品也分为地点货品，地点销 售商品。基于商户组织的划分，商家商品维度，绑定的是商家id；地点商品维度对 于谊品生鲜（线下）绑定的是门店id、配送中心id，对于谊品到家（线上）绑定的 是营业部id、配送中心id。

本次系统改造只对接到家销售渠道， 对于平台销售商品测， 下发到地点是 通过运营组控制一批营业部进行下发的。 所以本次地点商品， 主要是到运营组 维度的商品ES查询列表。

 

## 系统拆分

ypsx-kernel-product：商品基础数据服务，提供商品主档，类目，品牌，属性等管 理和查询能力。

ypsx-kernel-item：销售商品服务，提供商家商品，销售范围，地点商品的创建、修 改等管理能力，以及支撑外域对销售商品属性查询的能力。

ypsx-kernel-goods：供应链货品服务，提供商家货品，供货范围，地点货品的创 建、修改等管理能力，以及支撑外域对货品属性查询的能力。

ypsx-titan：DB同步到ES的数据通道，解析数据库binlog，聚合商品ES的数据结 果，存储到ES。内部直接连接商品的数据库，监听到binlog消息，反向查数据库组 装数据，binlog的解析采用了Maxwell。

ypsx-matrix：ES读取服务，将DB的查询服务与ES的查询服务解耦，提供统一索引 查询入口。针对索引维度的通用抽象查询方法。

ypsx-kernel-flow：商品流程服务，用于聚合整体的商品发布流程，里面通过流程引 擎保证分布式事务中数据的最终一致性。例如：商家商品提报发布时，最终需要先 下发营业部商品，然后修改商家商品的状态为可见同时创建商家商品和运营组的关系。这个流程里有两步，可以通过flow来编排好流程，提供完整的方法给管理后台 调用。

 ypsx-kernel-pms：商品管理后台的api服务，提供和前端交互的api接口，内部聚合 商品、价格、库存的底层服务，有自己的防腐层和内部针对管理后台的领域模型， 以商品为骨架，聚合价格和库存，支持后续其他系统的扩展。会包含较多的校验逻辑



## 数据指标

### 数据量大小

商品主档有10w左右，常用的商品大概在3000，剩下很多是业务早期

商家纬度有 

地点纬度有 目前总共2亿多 3000家门店  6个渠道 每家店有1.5w商品，地点商品是到具体的sku纬度，像快消品存在多规格的情况，称重品是存在多pluCode的情况，例如一颗白菜，早中晚的新鲜程度不一样，所以价格也不一样，就是通过plucode来做的区分，这样的话一颗白菜也算3个商品。而且1.5w是下发下去的商品，门店不一定会真正售卖。

### 机器指标

4核8G内存，26个pod

线上QPS平均是2000左右  高峰期是能上万

RT时间 是99%100ms之内  90%50ms

## 出技术方案

1. 看清楚之前的需求，把这个需求所在的场景和链路大致阅读一遍

2. 找到需求的变化点；

3. 分析变更的方案，涉及的内容可能会有：

   1. 数据结构会不会变，如何变；

   2. 交互协议会不会变，如何变，

      交互协议分为：

      1. 端和组件要不要变；
      2. 和下游接口要不要变；

   3. 执行逻辑会不会变，如何变，执行逻辑变更的细化考虑点：

      1. 是否变更域服务；
      2. 是否变更流程编排；
      3. 是否变更主干逻辑；
      4. 是否变更扩展点；
      5. 是否变更扩展点的内部逻辑，变更内部逻辑的时候，又可以进一步拆解：
         1. 重构原有的方法，覆盖之前的逻辑，那就需要进行回归；
         2. 通过逻辑路由到新的方法，这里需要增加路由逻辑；

   4. 稳定性方案；

   5. 发布方案；

# 四、提问环节

##  你的职业规划？为此你做过什么努力？

要点： 讲下你要成为怎么样的人

找准自身的定位和想要达成的目标，一年规划，三年规划

## 讲一下你的优缺点？

1. 我也不知道这个算不算缺点，就是我有的时候喜欢追求一些细节，比如在学习新内容的时候，会过分的陷入某些部分，从而拖慢了整体的学习进度。

   在工作也会出现类似的问题，比如在需求开发的过程中，会追求代码的质量（比如加注释、方法重构、变量名调整），影响开发的效率。

   以上的问题自己也有注意到，所以平时会更注重提升时间管理能力，改变工作方式，先完成大致框架，最后再留出时间改善细节。

## 技术面试官问题

1. 技术栈相关
2. 服务运行在云上吗，云平台 云原生
3. 业务体量，服务器的一些指标 QPS  RT 
4. 此次面试的评价

## TL问题

1. 团队在做的项目，未来规划，业务体量
2. 有过有幸入职前，我需要准备或者学习的内容
3. 您觉得我还有哪些不足的地方

## HR问题

1. 加班方面的问题

2. 绩效评估和考核

3. 薪资结构

4. 这份工作的关注点：行业发展前景、公司岗位的稳定性

5. 你有什么缺点？

   base 21 12薪 年终 0～3 公积金2w 按12%的比例  一年未涨薪







