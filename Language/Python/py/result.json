{"traceEvents": [{"ph": "M", "pid": 62900, "tid": 62900, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 62900, "tid": 2624575, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 62900, "tid": 2624575, "ts": 269286234056.02, "ph": "X", "cat": "fee", "dur": 2.98, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269286234066.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:49)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234074.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "str.rpartition"}, {"pid": 62900, "tid": 2624575, "ts": 269286234074.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "parent (<frozen importlib._bootstrap>:405)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234075.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234076.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269286234075.04, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_handle_fromlist (<frozen importlib._bootstrap>:1207)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234082.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:663)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234080.0, "ph": "X", "cat": "fee", "dur": 4.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:1438)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234084.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_thread.lock.__exit__"}, {"pid": 62900, "tid": 2624575, "ts": 269286234073.0, "ph": "X", "cat": "fee", "dur": 11.06, "name": "_init_event_loop_policy (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:750)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234072.0, "ph": "X", "cat": "fee", "dur": 12.08, "name": "get_event_loop_policy (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:758)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234093.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "time.get_clock_info"}, {"pid": 62900, "tid": 2624575, "ts": 269286234099.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234099.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "str.encode"}, {"pid": 62900, "tid": 2624575, "ts": 269286234099.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "encode (<frozen os>:756)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234098.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "__getitem__ (<frozen os>:674)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234097.0, "ph": "X", "cat": "fee", "dur": 4.02, "name": "get (<frozen _collections_abc>:771)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234096.0, "ph": "X", "cat": "fee", "dur": 5.04, "name": "_is_debug_mode (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:11)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234102.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234102.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "set_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1943)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234105.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:37)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234090.0, "ph": "X", "cat": "fee", "dur": 17.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:389)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234110.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:63)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234109.0, "ph": "X", "cat": "fee", "dur": 2.02, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:209)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234108.0, "ph": "X", "cat": "fee", "dur": 6.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:509)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234118.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_thread.RLock.acquire"}, {"pid": 62900, "tid": 2624575, "ts": 269286234118.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "_acquireLock (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:228)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234120.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "disable (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1319)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234121.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "getEffectiveLevel (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1720)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234122.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_thread.RLock.release"}, {"pid": 62900, "tid": 2624575, "ts": 269286234122.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_releaseLock (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:237)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234117.0, "ph": "X", "cat": "fee", "dur": 5.08, "name": "isEnabledFor (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1734)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234115.0, "ph": "X", "cat": "fee", "dur": 7.1, "name": "debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1467)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234126.0, "ph": "X", "cat": "fee", "dur": 6.0, "name": "_socket.socketpair"}, {"pid": 62900, "tid": 2624575, "ts": 269286234133.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_socket.socket.detach"}, {"pid": 62900, "tid": 2624575, "ts": 269286234135.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:220)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234137.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_socket.socket.detach"}, {"pid": 62900, "tid": 2624575, "ts": 269286234137.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:220)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234125.0, "ph": "X", "cat": "fee", "dur": 13.02, "name": "socketpair (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:595)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234139.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "socket.setblocking"}, {"pid": 62900, "tid": 2624575, "ts": 269286234140.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "socket.setblocking"}, {"pid": 62900, "tid": 2624575, "ts": 269286234141.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "socket.fileno"}, {"pid": 62900, "tid": 2624575, "ts": 269286234144.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234145.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_contextvars.copy_context"}, {"pid": 62900, "tid": 2624575, "ts": 269286234147.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234145.0, "ph": "X", "cat": "fee", "dur": 2.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234149.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_map (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:272)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234152.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234152.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_fileobj_to_fd (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:21)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234150.02, "ph": "X", "cat": "fee", "dur": 2.06, "name": "_fileobj_lookup (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:215)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234153.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "str.format"}, {"pid": 62900, "tid": 2624575, "ts": 269286234150.0, "ph": "X", "cat": "fee", "dur": 3.04, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:69)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234154.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "str.format"}, {"pid": 62900, "tid": 2624575, "ts": 269286234148.0, "ph": "X", "cat": "fee", "dur": 6.04, "name": "get_key (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:180)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234158.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234158.02, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_fileobj_to_fd (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:21)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234158.0, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_fileobj_lookup (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:215)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234159.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "type.__new__"}, {"pid": 62900, "tid": 2624575, "ts": 269286234159.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "<lambda> (<string>:1)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234157.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "register (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:234)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234162.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269286234156.0, "ph": "X", "cat": "fee", "dur": 9.02, "name": "register (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:516)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234143.0, "ph": "X", "cat": "fee", "dur": 23.0, "name": "_add_reader (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:261)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234124.0, "ph": "X", "cat": "fee", "dur": 42.02, "name": "_make_self_pipe (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:105)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234169.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269286234170.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "dict.items"}, {"pid": 62900, "tid": 2624575, "ts": 269286234170.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "dict.items"}, {"pid": 62900, "tid": 2624575, "ts": 269286234169.0, "ph": "X", "cat": "fee", "dur": 1.08, "name": "update (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py:289)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234167.0, "ph": "X", "cat": "fee", "dur": 3.1, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py:104)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234088.0, "ph": "X", "cat": "fee", "dur": 82.12, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:49)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234087.0, "ph": "X", "cat": "fee", "dur": 84.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:63)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234084.1, "ph": "X", "cat": "fee", "dur": 86.92, "name": "new_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:689)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234071.0, "ph": "X", "cat": "fee", "dur": 100.04, "name": "new_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:804)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234172.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_event_loop_policy (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:758)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234175.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234174.0, "ph": "X", "cat": "fee", "dur": 1.04, "name": "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:682)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234173.0, "ph": "X", "cat": "fee", "dur": 2.06, "name": "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:1449)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234172.0, "ph": "X", "cat": "fee", "dur": 3.08, "name": "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:799)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234176.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_contextvars.copy_context"}, {"pid": 62900, "tid": 2624575, "ts": 269286234069.0, "ph": "X", "cat": "fee", "dur": 107.04, "name": "_lazy_init (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:131)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234068.0, "ph": "X", "cat": "fee", "dur": 108.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:58)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234181.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234181.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269286234181.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.add"}, {"pid": 62900, "tid": 2624575, "ts": 269286234180.0, "ph": "X", "cat": "fee", "dur": 1.12, "name": "iscoroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234182.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269286234182.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_lazy_init (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:131)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234184.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234186.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234188.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234190.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234189.02, "ph": "X", "cat": "fee", "dur": 1.02, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234190.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269286234189.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234188.0, "ph": "X", "cat": "fee", "dur": 3.02, "name": "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234192.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.add"}, {"pid": 62900, "tid": 2624575, "ts": 269286234192.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "add (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:85)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234184.0, "ph": "X", "cat": "fee", "dur": 9.0, "name": "create_task (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:429)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234194.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_thread.get_ident"}, {"pid": 62900, "tid": 2624575, "ts": 269286234194.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "current_thread (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1446)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234194.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "main_thread (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1590)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234195.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_signal.getsignal"}, {"pid": 62900, "tid": 2624575, "ts": 269286234198.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_missing_ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1180)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234198.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234198.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.issubclass"}, {"pid": 62900, "tid": 2624575, "ts": 269286234196.04, "ph": "X", "cat": "fee", "dur": 3.96, "name": "__new__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1091)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234196.02, "ph": "X", "cat": "fee", "dur": 4.0, "name": "__call__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:686)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234196.0, "ph": "X", "cat": "fee", "dur": 4.04, "name": "_int_to_enum (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:24)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234195.0, "ph": "X", "cat": "fee", "dur": 5.06, "name": "getsignal (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:60)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234201.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_enum_to_int (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234201.06, "ph": "X", "cat": "fee", "dur": 1.94, "name": "_enum_to_int (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234203.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_signal.signal"}, {"pid": 62900, "tid": 2624575, "ts": 269286234204.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_missing_ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1180)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234205.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269286234205.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.issubclass"}, {"pid": 62900, "tid": 2624575, "ts": 269286234204.04, "ph": "X", "cat": "fee", "dur": 1.04, "name": "__new__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1091)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234204.02, "ph": "X", "cat": "fee", "dur": 1.08, "name": "__call__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:686)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234204.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "_int_to_enum (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:24)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234201.0, "ph": "X", "cat": "fee", "dur": 5.02, "name": "signal (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:54)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234208.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234209.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234209.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269286234209.0, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:586)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234210.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269286234210.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "isfuture (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:14)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234214.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269286234214.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "isfuture (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:14)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234215.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.get_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269286234215.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_get_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:299)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234213.0, "ph": "X", "cat": "fee", "dur": 2.08, "name": "_ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:652)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234212.0, "ph": "X", "cat": "fee", "dur": 3.1, "name": "ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:644)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234216.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.add_done_callback"}, {"pid": 62900, "tid": 2624575, "ts": 269286234218.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234218.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234218.12, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269286234218.06, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:586)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234219.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_set_coroutine_origin_tracking (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1925)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234220.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "sys.get_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269286234220.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_thread.get_ident"}, {"pid": 62900, "tid": 2624575, "ts": 269286234221.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "sys.set_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269286234222.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._set_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269286234227.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269286234229.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269286234229.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269286234229.1, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269286234229.14, "ph": "X", "cat": "fee", "dur": 12.86, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269286234229.0, "ph": "X", "cat": "fee", "dur": 14.0, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234244.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234245.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269286234244.04, "ph": "X", "cat": "fee", "dur": 1.0, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234245.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269286234246.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269286234251.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269286234252.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234252.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "create_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:425)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234255.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269286234255.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234256.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234258.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_contextvars.copy_context"}, {"pid": 62900, "tid": 2624575, "ts": 269286234259.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234258.02, "ph": "X", "cat": "fee", "dur": 1.02, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234258.0, "ph": "X", "cat": "fee", "dur": 1.06, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:103)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234260.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_heapq.heappush"}, {"pid": 62900, "tid": 2624575, "ts": 269286234256.0, "ph": "X", "cat": "fee", "dur": 4.04, "name": "call_at (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234254.0, "ph": "X", "cat": "fee", "dur": 7.0, "name": "call_later (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:709)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234251.0, "ph": "X", "cat": "fee", "dur": 10.02, "name": "sleep (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:627)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234249.0, "ph": "X", "cat": "fee", "dur": 12.04, "name": "main (/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py:4)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234248.02, "ph": "X", "cat": "fee", "dur": 13.04, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269286234248.0, "ph": "X", "cat": "fee", "dur": 13.08, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234226.0, "ph": "X", "cat": "fee", "dur": 36.0, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234262.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269286234263.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269286234263.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234263.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269286234263.12, "ph": "X", "cat": "fee", "dur": 0.88, "name": "builtins.min"}, {"pid": 62900, "tid": 2624575, "ts": 269286234264.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269286234264.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269286234264.12, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269286234264.16, "ph": "X", "cat": "fee", "dur": 1001039.84, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269286234264.02, "ph": "X", "cat": "fee", "dur": 1001046.98, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235320.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235327.0, "ph": "X", "cat": "fee", "dur": 4.0, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269287235324.0, "ph": "X", "cat": "fee", "dur": 7.02, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235340.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_heapq.heappop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235344.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235346.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235363.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269287235375.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_asyncio.Future.cancelled"}, {"pid": 62900, "tid": 2624575, "ts": 269287235382.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235391.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235387.0, "ph": "X", "cat": "fee", "dur": 4.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235392.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235383.02, "ph": "X", "cat": "fee", "dur": 9.02, "name": "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235381.0, "ph": "X", "cat": "fee", "dur": 11.06, "name": "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235376.02, "ph": "X", "cat": "fee", "dur": 16.98, "name": "_asyncio.Future.set_result"}, {"pid": 62900, "tid": 2624575, "ts": 269287235373.0, "ph": "X", "cat": "fee", "dur": 20.02, "name": "_set_result_unless_cancelled (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:311)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235370.0, "ph": "X", "cat": "fee", "dur": 23.04, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269287235367.0, "ph": "X", "cat": "fee", "dur": 27.0, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234262.02, "ph": "X", "cat": "fee", "dur": 1001132.0, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235396.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235399.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235402.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235403.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235404.0, "ph": "X", "cat": "fee", "dur": 20.0, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269287235398.0, "ph": "X", "cat": "fee", "dur": 27.0, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235425.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235426.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269287235426.02, "ph": "X", "cat": "fee", "dur": 0.06, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235427.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235428.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269287235442.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_timer_handle_cancelled (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1840)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235448.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235447.0, "ph": "X", "cat": "fee", "dur": 1.04, "name": "cancel (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:64)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235440.0, "ph": "X", "cat": "fee", "dur": 9.0, "name": "cancel (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:147)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235437.0, "ph": "X", "cat": "fee", "dur": 12.02, "name": "sleep (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:627)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235455.0, "ph": "X", "cat": "fee", "dur": 42.0, "name": "builtins.print"}, {"pid": 62900, "tid": 2624575, "ts": 269287235435.0, "ph": "X", "cat": "fee", "dur": 63.0, "name": "main (/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py:4)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235499.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235502.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235501.02, "ph": "X", "cat": "fee", "dur": 2.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235503.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235501.0, "ph": "X", "cat": "fee", "dur": 2.08, "name": "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235499.0, "ph": "X", "cat": "fee", "dur": 5.0, "name": "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235429.02, "ph": "X", "cat": "fee", "dur": 75.0, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269287235429.0, "ph": "X", "cat": "fee", "dur": 76.0, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235396.0, "ph": "X", "cat": "fee", "dur": 110.0, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235507.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235508.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235508.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235509.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235509.06, "ph": "X", "cat": "fee", "dur": 24.94, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269287235508.0, "ph": "X", "cat": "fee", "dur": 29.0, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235539.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235542.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269287235541.0, "ph": "X", "cat": "fee", "dur": 1.04, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235544.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235547.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269287235556.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.cancelled"}, {"pid": 62900, "tid": 2624575, "ts": 269287235557.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_asyncio.Task.exception"}, {"pid": 62900, "tid": 2624575, "ts": 269287235558.02, "ph": "X", "cat": "fee", "dur": 1.98, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269287235562.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.get_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235562.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_get_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:299)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235572.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "stop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:655)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235553.0, "ph": "X", "cat": "fee", "dur": 20.02, "name": "_run_until_complete_cb (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:180)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235550.0, "ph": "X", "cat": "fee", "dur": 23.04, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269287235548.0, "ph": "X", "cat": "fee", "dur": 26.0, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235506.02, "ph": "X", "cat": "fee", "dur": 74.98, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235583.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "_asyncio._set_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235587.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_set_coroutine_origin_tracking (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1925)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235590.0, "ph": "X", "cat": "fee", "dur": 5.0, "name": "sys.set_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269286234218.0, "ph": "X", "cat": "fee", "dur": 1001378.0, "name": "run_forever (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:593)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235597.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 62900, "tid": 2624575, "ts": 269287235598.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.done"}, {"pid": 62900, "tid": 2624575, "ts": 269287235598.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.result"}, {"pid": 62900, "tid": 2624575, "ts": 269286234208.0, "ph": "X", "cat": "fee", "dur": 1001390.08, "name": "run_until_complete (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:617)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235603.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_signal.getsignal"}, {"pid": 62900, "tid": 2624575, "ts": 269287235618.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_missing_ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1180)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235618.04, "ph": "X", "cat": "fee", "dur": 0.96, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269287235620.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "builtins.issubclass"}, {"pid": 62900, "tid": 2624575, "ts": 269287235628.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "builtins.id"}, {"pid": 62900, "tid": 2624575, "ts": 269287235630.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_thread.get_ident"}, {"pid": 62900, "tid": 2624575, "ts": 269287235632.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "set.add"}, {"pid": 62900, "tid": 2624575, "ts": 269287235646.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "str.lower"}, {"pid": 62900, "tid": 2624575, "ts": 269287235654.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235658.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "builtins.repr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235659.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235658.0, "ph": "X", "cat": "fee", "dur": 1.06, "name": "repr_instance (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:143)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235652.0, "ph": "X", "cat": "fee", "dur": 8.0, "name": "repr1 (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:55)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235649.0, "ph": "X", "cat": "fee", "dur": 11.02, "name": "repr (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:52)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235661.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235644.0, "ph": "X", "cat": "fee", "dur": 18.0, "name": "_future_repr_info (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:45)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235662.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.cancelling"}, {"pid": 62900, "tid": 2624575, "ts": 269287235663.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_asyncio.Task.get_name"}, {"pid": 62900, "tid": 2624575, "ts": 269287235665.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "list.insert"}, {"pid": 62900, "tid": 2624575, "ts": 269287235672.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "iscoroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235673.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235678.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235678.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "get_name (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:53)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235680.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235680.04, "ph": "X", "cat": "fee", "dur": 1.96, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235671.0, "ph": "X", "cat": "fee", "dur": 14.0, "name": "_format_coroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:50)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235686.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.insert"}, {"pid": 62900, "tid": 2624575, "ts": 269287235639.0, "ph": "X", "cat": "fee", "dur": 47.04, "name": "_task_repr_info (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py:9)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235687.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "str.join"}, {"pid": 62900, "tid": 2624575, "ts": 269287235636.0, "ph": "X", "cat": "fee", "dur": 53.0, "name": "_task_repr (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py:26)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235689.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "set.discard"}, {"pid": 62900, "tid": 2624575, "ts": 269287235627.0, "ph": "X", "cat": "fee", "dur": 63.02, "name": "wrapper (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:15)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235611.0, "ph": "X", "cat": "fee", "dur": 88.0, "name": "__new__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1091)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235608.0, "ph": "X", "cat": "fee", "dur": 91.02, "name": "__call__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:686)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235605.0, "ph": "X", "cat": "fee", "dur": 96.0, "name": "_int_to_enum (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:24)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235601.0, "ph": "X", "cat": "fee", "dur": 100.02, "name": "getsignal (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:60)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235702.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_enum_to_int (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235704.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "_enum_to_int (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235707.02, "ph": "X", "cat": "fee", "dur": 1.98, "name": "_signal.signal"}, {"pid": 62900, "tid": 2624575, "ts": 269287235713.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_missing_ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1180)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235713.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269287235714.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.issubclass"}, {"pid": 62900, "tid": 2624575, "ts": 269287235715.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "builtins.id"}, {"pid": 62900, "tid": 2624575, "ts": 269287235716.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_thread.get_ident"}, {"pid": 62900, "tid": 2624575, "ts": 269287235716.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "set.add"}, {"pid": 62900, "tid": 2624575, "ts": 269287235718.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "str.lower"}, {"pid": 62900, "tid": 2624575, "ts": 269287235720.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235721.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.repr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235721.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235721.0, "ph": "X", "cat": "fee", "dur": 0.1, "name": "repr_instance (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:143)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235719.02, "ph": "X", "cat": "fee", "dur": 2.98, "name": "repr1 (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:55)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235719.0, "ph": "X", "cat": "fee", "dur": 3.02, "name": "repr (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:52)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235722.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235718.0, "ph": "X", "cat": "fee", "dur": 5.0, "name": "_future_repr_info (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:45)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235723.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.cancelling"}, {"pid": 62900, "tid": 2624575, "ts": 269287235723.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.get_name"}, {"pid": 62900, "tid": 2624575, "ts": 269287235724.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.insert"}, {"pid": 62900, "tid": 2624575, "ts": 269287235725.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "iscoroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235726.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235727.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235727.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "get_name (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:53)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235728.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235728.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235725.0, "ph": "X", "cat": "fee", "dur": 5.0, "name": "_format_coroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:50)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235730.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.insert"}, {"pid": 62900, "tid": 2624575, "ts": 269287235717.04, "ph": "X", "cat": "fee", "dur": 13.96, "name": "_task_repr_info (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py:9)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235731.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "str.join"}, {"pid": 62900, "tid": 2624575, "ts": 269287235717.02, "ph": "X", "cat": "fee", "dur": 14.98, "name": "_task_repr (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py:26)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235732.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.discard"}, {"pid": 62900, "tid": 2624575, "ts": 269287235715.0, "ph": "X", "cat": "fee", "dur": 17.06, "name": "wrapper (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:15)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235711.02, "ph": "X", "cat": "fee", "dur": 23.98, "name": "__new__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1091)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235711.0, "ph": "X", "cat": "fee", "dur": 24.02, "name": "__call__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:686)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235710.0, "ph": "X", "cat": "fee", "dur": 26.0, "name": "_int_to_enum (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:24)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235702.0, "ph": "X", "cat": "fee", "dur": 34.02, "name": "signal (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:54)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234179.0, "ph": "X", "cat": "fee", "dur": 1001557.04, "name": "run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:86)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235740.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "set.discard"}, {"pid": 62900, "tid": 2624575, "ts": 269287235739.0, "ph": "X", "cat": "fee", "dur": 2.02, "name": "_remove (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:39)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235758.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235758.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235757.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "__len__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:72)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235764.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:17)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235767.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.add"}, {"pid": 62900, "tid": 2624575, "ts": 269287235767.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "__enter__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:21)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235771.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "set.remove"}, {"pid": 62900, "tid": 2624575, "ts": 269287235774.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "list.pop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235774.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "_commit_removals (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:53)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235770.0, "ph": "X", "cat": "fee", "dur": 6.02, "name": "__exit__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:27)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235761.0, "ph": "X", "cat": "fee", "dur": 15.04, "name": "__iter__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:63)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235778.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "<setcomp> (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:61)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235754.0, "ph": "X", "cat": "fee", "dur": 25.02, "name": "all_tasks (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:42)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235751.0, "ph": "X", "cat": "fee", "dur": 29.0, "name": "_cancel_all_tasks (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:193)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235782.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235783.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235784.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235783.0, "ph": "X", "cat": "fee", "dur": 1.06, "name": "_check_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:586)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235786.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235785.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "isfuture (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:14)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235791.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235791.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "isfuture (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:14)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235792.04, "ph": "X", "cat": "fee", "dur": 0.96, "name": "iscoroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235794.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235797.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235801.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235803.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235802.0, "ph": "X", "cat": "fee", "dur": 2.02, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235805.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235801.06, "ph": "X", "cat": "fee", "dur": 3.98, "name": "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235801.0, "ph": "X", "cat": "fee", "dur": 4.06, "name": "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235807.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.add"}, {"pid": 62900, "tid": 2624575, "ts": 269287235806.0, "ph": "X", "cat": "fee", "dur": 1.04, "name": "add (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:85)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235794.0, "ph": "X", "cat": "fee", "dur": 14.0, "name": "create_task (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:429)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235790.0, "ph": "X", "cat": "fee", "dur": 18.02, "name": "_ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:652)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235789.0, "ph": "X", "cat": "fee", "dur": 19.04, "name": "ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:644)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235809.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.add_done_callback"}, {"pid": 62900, "tid": 2624575, "ts": 269287235810.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235810.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235811.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235810.06, "ph": "X", "cat": "fee", "dur": 3.94, "name": "_check_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:586)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235815.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_set_coroutine_origin_tracking (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1925)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235816.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "sys.get_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269287235817.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_thread.get_ident"}, {"pid": 62900, "tid": 2624575, "ts": 269287235818.02, "ph": "X", "cat": "fee", "dur": 1.98, "name": "sys.set_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269287235820.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_asyncio._set_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235822.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235823.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235825.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235825.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235826.0, "ph": "X", "cat": "fee", "dur": 16.0, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269287235823.0, "ph": "X", "cat": "fee", "dur": 19.02, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235843.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235844.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269287235844.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235845.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235846.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269287235853.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235853.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235852.02, "ph": "X", "cat": "fee", "dur": 1.06, "name": "__len__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:72)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235852.0, "ph": "X", "cat": "fee", "dur": 1.1, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235851.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "shutdown_asyncgens (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:539)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235855.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235857.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235856.0, "ph": "X", "cat": "fee", "dur": 1.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235858.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235855.06, "ph": "X", "cat": "fee", "dur": 2.98, "name": "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235855.0, "ph": "X", "cat": "fee", "dur": 3.06, "name": "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235848.0, "ph": "X", "cat": "fee", "dur": 11.0, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269287235847.0, "ph": "X", "cat": "fee", "dur": 12.02, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235821.02, "ph": "X", "cat": "fee", "dur": 38.98, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235861.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235861.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235862.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235862.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235863.0, "ph": "X", "cat": "fee", "dur": 15.0, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269287235861.04, "ph": "X", "cat": "fee", "dur": 17.96, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235879.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235880.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269287235880.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235881.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235881.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269287235883.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.cancelled"}, {"pid": 62900, "tid": 2624575, "ts": 269287235883.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.exception"}, {"pid": 62900, "tid": 2624575, "ts": 269287235884.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269287235885.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.get_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235885.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_get_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:299)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235886.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "stop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:655)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235883.0, "ph": "X", "cat": "fee", "dur": 3.04, "name": "_run_until_complete_cb (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:180)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235882.02, "ph": "X", "cat": "fee", "dur": 4.04, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269287235882.0, "ph": "X", "cat": "fee", "dur": 5.0, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235860.02, "ph": "X", "cat": "fee", "dur": 27.0, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235888.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._set_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235888.04, "ph": "X", "cat": "fee", "dur": 0.96, "name": "_set_coroutine_origin_tracking (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1925)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235889.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "sys.set_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269287235810.0, "ph": "X", "cat": "fee", "dur": 80.02, "name": "run_forever (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:593)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235891.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 62900, "tid": 2624575, "ts": 269287235891.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.done"}, {"pid": 62900, "tid": 2624575, "ts": 269287235892.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.result"}, {"pid": 62900, "tid": 2624575, "ts": 269287235782.0, "ph": "X", "cat": "fee", "dur": 110.04, "name": "run_until_complete (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:617)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235893.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.discard"}, {"pid": 62900, "tid": 2624575, "ts": 269287235893.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_remove (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:39)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235895.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235896.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235896.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235895.06, "ph": "X", "cat": "fee", "dur": 1.02, "name": "_check_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:586)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235897.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235897.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "isfuture (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:14)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235899.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.hasattr"}, {"pid": 62900, "tid": 2624575, "ts": 269287235899.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "isfuture (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:14)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235900.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "iscoroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:34)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235901.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235902.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235904.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235907.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235905.0, "ph": "X", "cat": "fee", "dur": 2.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235907.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235904.06, "ph": "X", "cat": "fee", "dur": 3.96, "name": "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235904.0, "ph": "X", "cat": "fee", "dur": 4.04, "name": "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235909.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.add"}, {"pid": 62900, "tid": 2624575, "ts": 269287235908.06, "ph": "X", "cat": "fee", "dur": 0.98, "name": "add (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:85)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235901.0, "ph": "X", "cat": "fee", "dur": 8.06, "name": "create_task (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:429)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235899.0, "ph": "X", "cat": "fee", "dur": 10.08, "name": "_ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:652)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235898.04, "ph": "X", "cat": "fee", "dur": 11.96, "name": "ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:644)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235910.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.add_done_callback"}, {"pid": 62900, "tid": 2624575, "ts": 269287235911.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235911.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235912.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235911.04, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_check_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:586)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235912.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_set_coroutine_origin_tracking (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1925)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235913.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "sys.get_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269287235913.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_thread.get_ident"}, {"pid": 62900, "tid": 2624575, "ts": 269287235914.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "sys.set_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269287235915.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._set_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235916.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235917.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235917.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235917.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235918.0, "ph": "X", "cat": "fee", "dur": 14.0, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269287235916.04, "ph": "X", "cat": "fee", "dur": 15.98, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235933.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235933.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269287235933.04, "ph": "X", "cat": "fee", "dur": 0.98, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235934.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235935.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269287235938.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "shutdown_default_executor (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:564)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235939.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235941.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235940.0, "ph": "X", "cat": "fee", "dur": 1.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235941.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 62900, "tid": 2624575, "ts": 269287235939.06, "ph": "X", "cat": "fee", "dur": 2.04, "name": "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235939.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235936.0, "ph": "X", "cat": "fee", "dur": 6.02, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269287235935.04, "ph": "X", "cat": "fee", "dur": 7.0, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235915.06, "ph": "X", "cat": "fee", "dur": 27.0, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235945.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235947.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235948.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235948.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.max"}, {"pid": 62900, "tid": 2624575, "ts": 269287235948.1, "ph": "X", "cat": "fee", "dur": 14.9, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269287235945.06, "ph": "X", "cat": "fee", "dur": 17.96, "name": "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235963.04, "ph": "X", "cat": "fee", "dur": 0.96, "name": "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235965.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 62900, "tid": 2624575, "ts": 269287235965.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235966.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 62900, "tid": 2624575, "ts": 269287235966.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 62900, "tid": 2624575, "ts": 269287235969.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.cancelled"}, {"pid": 62900, "tid": 2624575, "ts": 269287235969.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.exception"}, {"pid": 62900, "tid": 2624575, "ts": 269287235969.1, "ph": "X", "cat": "fee", "dur": 0.9, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269287235970.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.get_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235970.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "_get_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:299)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235971.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "stop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:655)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235969.0, "ph": "X", "cat": "fee", "dur": 2.06, "name": "_run_until_complete_cb (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:180)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235968.0, "ph": "X", "cat": "fee", "dur": 3.08, "name": "_contextvars.Context.run"}, {"pid": 62900, "tid": 2624575, "ts": 269287235967.0, "ph": "X", "cat": "fee", "dur": 5.0, "name": "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235945.0, "ph": "X", "cat": "fee", "dur": 27.02, "name": "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235972.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._set_running_loop"}, {"pid": 62900, "tid": 2624575, "ts": 269287235973.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_set_coroutine_origin_tracking (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1925)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235973.04, "ph": "X", "cat": "fee", "dur": 0.96, "name": "sys.set_asyncgen_hooks"}, {"pid": 62900, "tid": 2624575, "ts": 269287235910.06, "ph": "X", "cat": "fee", "dur": 63.96, "name": "run_forever (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:593)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235975.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 62900, "tid": 2624575, "ts": 269287235975.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.done"}, {"pid": 62900, "tid": 2624575, "ts": 269287235975.08, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.result"}, {"pid": 62900, "tid": 2624575, "ts": 269287235895.0, "ph": "X", "cat": "fee", "dur": 80.12, "name": "run_until_complete (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:617)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235976.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "set.discard"}, {"pid": 62900, "tid": 2624575, "ts": 269287235976.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_remove (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:39)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235978.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_event_loop_policy (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:758)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235981.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:682)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235979.0, "ph": "X", "cat": "fee", "dur": 4.02, "name": "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:1449)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235978.0, "ph": "X", "cat": "fee", "dur": 5.04, "name": "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:799)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235990.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235991.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:686)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235994.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "socket.fileno"}, {"pid": 62900, "tid": 2624575, "ts": 269287236000.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:686)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236002.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_map (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:272)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236005.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269287236004.02, "ph": "X", "cat": "fee", "dur": 1.02, "name": "_fileobj_to_fd (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:21)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236004.0, "ph": "X", "cat": "fee", "dur": 1.06, "name": "_fileobj_lookup (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:215)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236003.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:69)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236001.0, "ph": "X", "cat": "fee", "dur": 5.02, "name": "get_key (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:180)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236016.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 62900, "tid": 2624575, "ts": 269287236016.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "_fileobj_to_fd (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:21)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236015.02, "ph": "X", "cat": "fee", "dur": 1.06, "name": "_fileobj_lookup (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:215)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236016.1, "ph": "X", "cat": "fee", "dur": 0.9, "name": "dict.pop"}, {"pid": 62900, "tid": 2624575, "ts": 269287236015.0, "ph": "X", "cat": "fee", "dur": 2.02, "name": "unregister (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:247)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236020.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "select.kqueue.control"}, {"pid": 62900, "tid": 2624575, "ts": 269287236013.0, "ph": "X", "cat": "fee", "dur": 11.0, "name": "unregister (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:532)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236026.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236025.0, "ph": "X", "cat": "fee", "dur": 1.04, "name": "cancel (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:64)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236000.0, "ph": "X", "cat": "fee", "dur": 26.06, "name": "_remove_reader (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:277)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236031.0, "ph": "X", "cat": "fee", "dur": 8.0, "name": "socket.close"}, {"pid": 62900, "tid": 2624575, "ts": 269287236030.0, "ph": "X", "cat": "fee", "dur": 9.02, "name": "_real_close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:495)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236028.0, "ph": "X", "cat": "fee", "dur": 11.04, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:499)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236041.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "socket.close"}, {"pid": 62900, "tid": 2624575, "ts": 269287236040.02, "ph": "X", "cat": "fee", "dur": 2.0, "name": "_real_close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:495)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236040.0, "ph": "X", "cat": "fee", "dur": 3.0, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:499)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235993.0, "ph": "X", "cat": "fee", "dur": 50.02, "name": "_close_self_pipe (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:97)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236046.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236048.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.clear"}, {"pid": 62900, "tid": 2624575, "ts": 269287236048.04, "ph": "X", "cat": "fee", "dur": 0.96, "name": "list.clear"}, {"pid": 62900, "tid": 2624575, "ts": 269287236046.0, "ph": "X", "cat": "fee", "dur": 3.02, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:663)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236051.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "select.kqueue.close"}, {"pid": 62900, "tid": 2624575, "ts": 269287236055.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "dict.clear"}, {"pid": 62900, "tid": 2624575, "ts": 269287236054.0, "ph": "X", "cat": "fee", "dur": 2.0, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:268)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236050.0, "ph": "X", "cat": "fee", "dur": 7.0, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:578)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235989.0, "ph": "X", "cat": "fee", "dur": 68.02, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:86)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236060.0, "ph": "X", "cat": "fee", "dur": 1.0, "name": "sys.is_finalizing"}, {"pid": 62900, "tid": 2624575, "ts": 269287235987.0, "ph": "X", "cat": "fee", "dur": 76.0, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:67)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235747.0, "ph": "X", "cat": "fee", "dur": 316.02, "name": "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:65)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236066.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:686)"}, {"pid": 62900, "tid": 2624575, "ts": 269287236066.0, "ph": "X", "cat": "fee", "dur": 0.06, "name": "__del__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:690)"}, {"pid": 62900, "tid": 2624575, "ts": 269287235742.0, "ph": "X", "cat": "fee", "dur": 328.0, "name": "__exit__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:62)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234056.0, "ph": "X", "cat": "fee", "dur": 1002014.02, "name": "run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:160)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234051.0, "ph": "X", "cat": "fee", "dur": 1002019.04, "name": "<module> (/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py:1)"}, {"pid": 62900, "tid": 2624575, "ts": 269286234050.0, "ph": "X", "cat": "fee", "dur": 1002024.0, "name": "builtins.exec"}], "viztracer_metadata": {"overflow": false, "version": "0.16.1"}, "file_info": {"files": {"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py": ["__all__ = ('Runner', 'run')\n\nimport contextvars\nimport enum\nimport functools\nimport threading\nimport signal\nimport sys\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import tasks\n\n\nclass _State(enum.Enum):\n    CREATED = \"created\"\n    INITIALIZED = \"initialized\"\n    CLOSED = \"closed\"\n\n\nclass Runner:\n    \"\"\"A context manager that controls event loop life cycle.\n\n    The context manager always creates a new event loop,\n    allows to run async functions inside it,\n    and properly finalizes the loop at the context manager exit.\n\n    If debug is True, the event loop will be run in debug mode.\n    If loop_factory is passed, it is used for new event loop creation.\n\n    asyncio.run(main(), debug=True)\n\n    is a shortcut for\n\n    with asyncio.Runner(debug=True) as runner:\n        runner.run(main())\n\n    The run() method can be called multiple times within the runner's context.\n\n    This can be useful for interactive console (e.g. IPython),\n    unittest runners, console tools, -- everywhere when async code\n    is called from existing sync framework and where the preferred single\n    asyncio.run() call doesn't work.\n\n    \"\"\"\n\n    # Note: the class is final, it is not intended for inheritance.\n\n    def __init__(self, *, debug=None, loop_factory=None):\n        self._state = _State.CREATED\n        self._debug = debug\n        self._loop_factory = loop_factory\n        self._loop = None\n        self._context = None\n        self._interrupt_count = 0\n        self._set_event_loop = False\n\n    def __enter__(self):\n        self._lazy_init()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        \"\"\"Shutdown and close event loop.\"\"\"\n        if self._state is not _State.INITIALIZED:\n            return\n        try:\n            loop = self._loop\n            _cancel_all_tasks(loop)\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            loop.run_until_complete(loop.shutdown_default_executor())\n        finally:\n            if self._set_event_loop:\n                events.set_event_loop(None)\n            loop.close()\n            self._loop = None\n            self._state = _State.CLOSED\n\n    def get_loop(self):\n        \"\"\"Return embedded event loop.\"\"\"\n        self._lazy_init()\n        return self._loop\n\n    def run(self, coro, *, context=None):\n        \"\"\"Run a coroutine inside the embedded event loop.\"\"\"\n        if not coroutines.iscoroutine(coro):\n            raise ValueError(\"a coroutine was expected, got {!r}\".format(coro))\n\n        if events._get_running_loop() is not None:\n            # fail fast with short traceback\n            raise RuntimeError(\n                \"Runner.run() cannot be called from a running event loop\")\n\n        self._lazy_init()\n\n        if context is None:\n            context = self._context\n        task = self._loop.create_task(coro, context=context)\n\n        if (threading.current_thread() is threading.main_thread()\n            and signal.getsignal(signal.SIGINT) is signal.default_int_handler\n        ):\n            sigint_handler = functools.partial(self._on_sigint, main_task=task)\n            try:\n                signal.signal(signal.SIGINT, sigint_handler)\n            except ValueError:\n                # `signal.signal` may throw if `threading.main_thread` does\n                # not support signals (e.g. embedded interpreter with signals\n                # not registered - see gh-91880)\n                sigint_handler = None\n        else:\n            sigint_handler = None\n\n        self._interrupt_count = 0\n        try:\n            return self._loop.run_until_complete(task)\n        except exceptions.CancelledError:\n            if self._interrupt_count > 0:\n                uncancel = getattr(task, \"uncancel\", None)\n                if uncancel is not None and uncancel() == 0:\n                    raise KeyboardInterrupt()\n            raise  # CancelledError\n        finally:\n            if (sigint_handler is not None\n                and signal.getsignal(signal.SIGINT) is sigint_handler\n            ):\n                signal.signal(signal.SIGINT, signal.default_int_handler)\n\n    def _lazy_init(self):\n        if self._state is _State.CLOSED:\n            raise RuntimeError(\"Runner is closed\")\n        if self._state is _State.INITIALIZED:\n            return\n        if self._loop_factory is None:\n            self._loop = events.new_event_loop()\n            if not self._set_event_loop:\n                # Call set_event_loop only once to avoid calling\n                # attach_loop multiple times on child watchers\n                events.set_event_loop(self._loop)\n                self._set_event_loop = True\n        else:\n            self._loop = self._loop_factory()\n        if self._debug is not None:\n            self._loop.set_debug(self._debug)\n        self._context = contextvars.copy_context()\n        self._state = _State.INITIALIZED\n\n    def _on_sigint(self, signum, frame, main_task):\n        self._interrupt_count += 1\n        if self._interrupt_count == 1 and not main_task.done():\n            main_task.cancel()\n            # wakeup loop if it is blocked by select() with long timeout\n            self._loop.call_soon_threadsafe(lambda: None)\n            return\n        raise KeyboardInterrupt()\n\n\ndef run(main, *, debug=None):\n    \"\"\"Execute the coroutine and return the result.\n\n    This function runs the passed coroutine, taking care of\n    managing the asyncio event loop and finalizing asynchronous\n    generators.\n\n    This function cannot be called when another asyncio event loop is\n    running in the same thread.\n\n    If debug is True, the event loop will be run in debug mode.\n\n    This function always creates a new event loop and closes it at the end.\n    It should be used as a main entry point for asyncio programs, and should\n    ideally only be called once.\n\n    Example:\n\n        async def main():\n            await asyncio.sleep(1)\n            print('hello')\n\n        asyncio.run(main())\n    \"\"\"\n    if events._get_running_loop() is not None:\n        # fail fast with short traceback\n        raise RuntimeError(\n            \"asyncio.run() cannot be called from a running event loop\")\n\n    with Runner(debug=debug) as runner:\n        return runner.run(main)\n\n\ndef _cancel_all_tasks(loop):\n    to_cancel = tasks.all_tasks(loop)\n    if not to_cancel:\n        return\n\n    for task in to_cancel:\n        task.cancel()\n\n    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\n\n    for task in to_cancel:\n        if task.cancelled():\n            continue\n        if task.exception() is not None:\n            loop.call_exception_handler({\n                'message': 'unhandled exception during asyncio.run() shutdown',\n                'exception': task.exception(),\n                'task': task,\n            })\n", 211], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py": ["\"\"\"Event loop and event loop policy.\"\"\"\n\n__all__ = (\n    'AbstractEventLoopPolicy',\n    'AbstractEventLoop', 'AbstractServer',\n    'Handle', 'TimerHandle',\n    'get_event_loop_policy', 'set_event_loop_policy',\n    'get_event_loop', 'set_event_loop', 'new_event_loop',\n    'get_child_watcher', 'set_child_watcher',\n    '_set_running_loop', 'get_running_loop',\n    '_get_running_loop',\n)\n\nimport contextvars\nimport os\nimport socket\nimport subprocess\nimport sys\nimport threading\n\nfrom . import format_helpers\n\n\nclass Handle:\n    \"\"\"Object returned by callback registration methods.\"\"\"\n\n    __slots__ = ('_callback', '_args', '_cancelled', '_loop',\n                 '_source_traceback', '_repr', '__weakref__',\n                 '_context')\n\n    def __init__(self, callback, args, loop, context=None):\n        if context is None:\n            context = contextvars.copy_context()\n        self._context = context\n        self._loop = loop\n        self._callback = callback\n        self._args = args\n        self._cancelled = False\n        self._repr = None\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n        else:\n            self._source_traceback = None\n\n    def _repr_info(self):\n        info = [self.__class__.__name__]\n        if self._cancelled:\n            info.append('cancelled')\n        if self._callback is not None:\n            info.append(format_helpers._format_callback_source(\n                self._callback, self._args))\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            info.append(f'created at {frame[0]}:{frame[1]}')\n        return info\n\n    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n        info = self._repr_info()\n        return '<{}>'.format(' '.join(info))\n\n    def cancel(self):\n        if not self._cancelled:\n            self._cancelled = True\n            if self._loop.get_debug():\n                # Keep a representation in debug mode to keep callback and\n                # parameters. For example, to log the warning\n                # \"Executing <Handle...> took 2.5 second\"\n                self._repr = repr(self)\n            self._callback = None\n            self._args = None\n\n    def cancelled(self):\n        return self._cancelled\n\n    def _run(self):\n        try:\n            self._context.run(self._callback, *self._args)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            cb = format_helpers._format_callback_source(\n                self._callback, self._args)\n            msg = f'Exception in callback {cb}'\n            context = {\n                'message': msg,\n                'exception': exc,\n                'handle': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self = None  # Needed to break cycles when an exception occurs.\n\n\nclass TimerHandle(Handle):\n    \"\"\"Object returned by timed callback registration methods.\"\"\"\n\n    __slots__ = ['_scheduled', '_when']\n\n    def __init__(self, when, callback, args, loop, context=None):\n        super().__init__(callback, args, loop, context)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._when = when\n        self._scheduled = False\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        pos = 2 if self._cancelled else 1\n        info.insert(pos, f'when={self._when}')\n        return info\n\n    def __hash__(self):\n        return hash(self._when)\n\n    def __lt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, TimerHandle):\n            return (self._when == other._when and\n                    self._callback == other._callback and\n                    self._args == other._args and\n                    self._cancelled == other._cancelled)\n        return NotImplemented\n\n    def cancel(self):\n        if not self._cancelled:\n            self._loop._timer_handle_cancelled(self)\n        super().cancel()\n\n    def when(self):\n        \"\"\"Return a scheduled callback time.\n\n        The time is an absolute timestamp, using the same time\n        reference as loop.time().\n        \"\"\"\n        return self._when\n\n\nclass AbstractServer:\n    \"\"\"Abstract server returned by create_server().\"\"\"\n\n    def close(self):\n        \"\"\"Stop serving.  This leaves existing connections open.\"\"\"\n        raise NotImplementedError\n\n    def get_loop(self):\n        \"\"\"Get the event loop the Server object is attached to.\"\"\"\n        raise NotImplementedError\n\n    def is_serving(self):\n        \"\"\"Return True if the server is accepting connections.\"\"\"\n        raise NotImplementedError\n\n    async def start_serving(self):\n        \"\"\"Start accepting connections.\n\n        This method is idempotent, so it can be called when\n        the server is already being serving.\n        \"\"\"\n        raise NotImplementedError\n\n    async def serve_forever(self):\n        \"\"\"Start accepting connections until the coroutine is cancelled.\n\n        The server is closed when the coroutine is cancelled.\n        \"\"\"\n        raise NotImplementedError\n\n    async def wait_closed(self):\n        \"\"\"Coroutine to wait until service is closed.\"\"\"\n        raise NotImplementedError\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        self.close()\n        await self.wait_closed()\n\n\nclass AbstractEventLoop:\n    \"\"\"Abstract event loop.\"\"\"\n\n    # Running and stopping the event loop.\n\n    def run_forever(self):\n        \"\"\"Run the event loop until stop() is called.\"\"\"\n        raise NotImplementedError\n\n    def run_until_complete(self, future):\n        \"\"\"Run the event loop until a Future is done.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        raise NotImplementedError\n\n    def stop(self):\n        \"\"\"Stop the event loop as soon as reasonable.\n\n        Exactly how soon that is may depend on the implementation, but\n        no more I/O callbacks should be scheduled.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_running(self):\n        \"\"\"Return whether the event loop is currently running.\"\"\"\n        raise NotImplementedError\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the loop.\n\n        The loop should not be running.\n\n        This is idempotent and irreversible.\n\n        No other methods should be called after this one.\n        \"\"\"\n        raise NotImplementedError\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        raise NotImplementedError\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        raise NotImplementedError\n\n    # Methods scheduling callbacks.  All these return Handles.\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        raise NotImplementedError\n\n    def call_soon(self, callback, *args, context=None):\n        return self.call_later(0, callback, *args, context=context)\n\n    def call_later(self, delay, callback, *args, context=None):\n        raise NotImplementedError\n\n    def call_at(self, when, callback, *args, context=None):\n        raise NotImplementedError\n\n    def time(self):\n        raise NotImplementedError\n\n    def create_future(self):\n        raise NotImplementedError\n\n    # Method scheduling a coroutine object: create a task.\n\n    def create_task(self, coro, *, name=None, context=None):\n        raise NotImplementedError\n\n    # Methods for interacting with threads.\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        raise NotImplementedError\n\n    def run_in_executor(self, executor, func, *args):\n        raise NotImplementedError\n\n    def set_default_executor(self, executor):\n        raise NotImplementedError\n\n    # Network I/O methods returning Futures.\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        raise NotImplementedError\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        raise NotImplementedError\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0, proto=0,\n            flags=0, sock=None, local_addr=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        raise NotImplementedError\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *, family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE, sock=None, backlog=100,\n            ssl=None, reuse_address=None, reuse_port=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a TCP server bound to host and port.\n\n        The return value is a Server object which can be used to stop\n        the service.\n\n        If host is an empty string or None all interfaces are assumed\n        and a list of multiple sockets will be returned (most likely\n        one for IPv4 and another one for IPv6). The host parameter can also be\n        a sequence (e.g. list) of hosts to bind to.\n\n        family can be set to either AF_INET or AF_INET6 to force the\n        socket to use IPv4 or IPv6. If not set it will be determined\n        from host (defaults to AF_UNSPEC).\n\n        flags is a bitmask for getaddrinfo().\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL handshake before aborting the\n        connection. Default is 60s.\n\n        ssl_shutdown_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL shutdown procedure\n        before aborting the connection. Default is 30s.\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file through a transport.\n\n        Return an amount of sent bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None,\n                        ssl_shutdown_timeout=None):\n        \"\"\"Upgrade a transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        raise NotImplementedError\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a UNIX Domain Socket server.\n\n        The return value is a Server object, which can be used to stop\n        the service.\n\n        path is a str, representing a file system path to bind the\n        server socket to.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for the SSL handshake to complete (defaults to 60s).\n\n        ssl_shutdown_timeout is the time in seconds that an SSL server\n        will wait for the SSL shutdown to finish (defaults to 30s).\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        \"\"\"Handle an accepted connection.\n\n        This is used by servers that accept connections outside of\n        asyncio, but use asyncio to handle connections.\n\n        This method is a coroutine.  When completed, the coroutine\n        returns a (transport, protocol) pair.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=None, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"A coroutine which creates a datagram endpoint.\n\n        This method will try to establish the endpoint in the background.\n        When successful, the coroutine returns a (transport, protocol) pair.\n\n        protocol_factory must be a callable returning a protocol instance.\n\n        socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on\n        host (or family if specified), socket type SOCK_DGRAM.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified it will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows and some UNIX's. If the\n        :py:data:`~socket.SO_REUSEPORT` constant is not defined then this\n        capability is unsupported.\n\n        allow_broadcast tells the kernel to allow this endpoint to send\n        messages to the broadcast address.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n        \"\"\"\n        raise NotImplementedError\n\n    # Pipes and subprocesses.\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        \"\"\"Register read pipe in event loop. Set the pipe to non-blocking mode.\n\n        protocol_factory should instantiate object with Protocol interface.\n        pipe is a file-like object.\n        Return pair (transport, protocol), where transport supports the\n        ReadTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        \"\"\"Register write pipe in event loop.\n\n        protocol_factory should instantiate object with BaseProtocol interface.\n        Pipe is file-like object already switched to nonblocking.\n        Return pair (transport, protocol), where transport support\n        WriteTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs):\n        raise NotImplementedError\n\n    async def subprocess_exec(self, protocol_factory, *args,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE,\n                              **kwargs):\n        raise NotImplementedError\n\n    # Ready-based callback registration methods.\n    # The add_*() methods return None.\n    # The remove_*() methods return True if something was removed,\n    # False if there was nothing to delete.\n\n    def add_reader(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_reader(self, fd):\n        raise NotImplementedError\n\n    def add_writer(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_writer(self, fd):\n        raise NotImplementedError\n\n    # Completion based I/O methods returning Futures.\n\n    async def sock_recv(self, sock, nbytes):\n        raise NotImplementedError\n\n    async def sock_recv_into(self, sock, buf):\n        raise NotImplementedError\n\n    async def sock_recvfrom(self, sock, bufsize):\n        raise NotImplementedError\n\n    async def sock_recvfrom_into(self, sock, buf, nbytes=0):\n        raise NotImplementedError\n\n    async def sock_sendall(self, sock, data):\n        raise NotImplementedError\n\n    async def sock_sendto(self, sock, data, address):\n        raise NotImplementedError\n\n    async def sock_connect(self, sock, address):\n        raise NotImplementedError\n\n    async def sock_accept(self, sock):\n        raise NotImplementedError\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=None):\n        raise NotImplementedError\n\n    # Signal handling.\n\n    def add_signal_handler(self, sig, callback, *args):\n        raise NotImplementedError\n\n    def remove_signal_handler(self, sig):\n        raise NotImplementedError\n\n    # Task factory.\n\n    def set_task_factory(self, factory):\n        raise NotImplementedError\n\n    def get_task_factory(self):\n        raise NotImplementedError\n\n    # Error handlers.\n\n    def get_exception_handler(self):\n        raise NotImplementedError\n\n    def set_exception_handler(self, handler):\n        raise NotImplementedError\n\n    def default_exception_handler(self, context):\n        raise NotImplementedError\n\n    def call_exception_handler(self, context):\n        raise NotImplementedError\n\n    # Debug flag management.\n\n    def get_debug(self):\n        raise NotImplementedError\n\n    def set_debug(self, enabled):\n        raise NotImplementedError\n\n\nclass AbstractEventLoopPolicy:\n    \"\"\"Abstract policy for accessing the event loop.\"\"\"\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an event loop object implementing the AbstractEventLoop interface,\n        or raises an exception in case no event loop has been set for the\n        current context and the current policy does not specify to create one.\n\n        It should never return None.\"\"\"\n        raise NotImplementedError\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop for the current context to loop.\"\"\"\n        raise NotImplementedError\n\n    def new_event_loop(self):\n        \"\"\"Create and return a new event loop object according to this\n        policy's rules. If there's need to set this loop as the event loop for\n        the current context, set_event_loop must be called explicitly.\"\"\"\n        raise NotImplementedError\n\n    # Child processes handling (Unix only).\n\n    def get_child_watcher(self):\n        \"Get the watcher for child processes.\"\n        raise NotImplementedError\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n        raise NotImplementedError\n\n\nclass BaseDefaultEventLoopPolicy(AbstractEventLoopPolicy):\n    \"\"\"Default policy implementation for accessing the event loop.\n\n    In this policy, each thread has its own event loop.  However, we\n    only automatically create an event loop by default for the main\n    thread; other threads by default have no event loop.\n\n    Other policies may have different rules (e.g. a single global\n    event loop, or automatically creating an event loop per thread, or\n    using some other notion of context to which an event loop is\n    associated).\n    \"\"\"\n\n    _loop_factory = None\n\n    class _Local(threading.local):\n        _loop = None\n        _set_called = False\n\n    def __init__(self):\n        self._local = self._Local()\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an instance of EventLoop or raises an exception.\n        \"\"\"\n        if (self._local._loop is None and\n                not self._local._set_called and\n                threading.current_thread() is threading.main_thread()):\n            self.set_event_loop(self.new_event_loop())\n\n        if self._local._loop is None:\n            raise RuntimeError('There is no current event loop in thread %r.'\n                               % threading.current_thread().name)\n\n        return self._local._loop\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\"\"\"\n        self._local._set_called = True\n        if loop is not None and not isinstance(loop, AbstractEventLoop):\n            raise TypeError(f\"loop must be an instance of AbstractEventLoop or None, not '{type(loop).__name__}'\")\n        self._local._loop = loop\n\n    def new_event_loop(self):\n        \"\"\"Create a new event loop.\n\n        You must call set_event_loop() to make this the current event\n        loop.\n        \"\"\"\n        return self._loop_factory()\n\n\n# Event loop policy.  The policy itself is always global, even if the\n# policy's rules say that there is an event loop per thread (or other\n# notion of context).  The default policy is installed by the first\n# call to get_event_loop_policy().\n_event_loop_policy = None\n\n# Lock for protecting the on-the-fly creation of the event loop policy.\n_lock = threading.Lock()\n\n\n# A TLS for the running event loop, used by _get_running_loop.\nclass _RunningLoop(threading.local):\n    loop_pid = (None, None)\n\n\n_running_loop = _RunningLoop()\n\n\ndef get_running_loop():\n    \"\"\"Return the running event loop.  Raise a RuntimeError if there is none.\n\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    loop = _get_running_loop()\n    if loop is None:\n        raise RuntimeError('no running event loop')\n    return loop\n\n\ndef _get_running_loop():\n    \"\"\"Return the running event loop or None.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    running_loop, pid = _running_loop.loop_pid\n    if running_loop is not None and pid == os.getpid():\n        return running_loop\n\n\ndef _set_running_loop(loop):\n    \"\"\"Set the running event loop.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    _running_loop.loop_pid = (loop, os.getpid())\n\n\ndef _init_event_loop_policy():\n    global _event_loop_policy\n    with _lock:\n        if _event_loop_policy is None:  # pragma: no branch\n            from . import DefaultEventLoopPolicy\n            _event_loop_policy = DefaultEventLoopPolicy()\n\n\ndef get_event_loop_policy():\n    \"\"\"Get the current event loop policy.\"\"\"\n    if _event_loop_policy is None:\n        _init_event_loop_policy()\n    return _event_loop_policy\n\n\ndef set_event_loop_policy(policy):\n    \"\"\"Set the current event loop policy.\n\n    If policy is None, the default policy is restored.\"\"\"\n    global _event_loop_policy\n    if policy is not None and not isinstance(policy, AbstractEventLoopPolicy):\n        raise TypeError(f\"policy must be an instance of AbstractEventLoopPolicy or None, not '{type(policy).__name__}'\")\n    _event_loop_policy = policy\n\n\ndef get_event_loop():\n    \"\"\"Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    return _py__get_event_loop()\n\n\ndef _get_event_loop(stacklevel=3):\n    # This internal method is going away in Python 3.12, left here only for\n    # backwards compatibility with 3.10.0 - 3.10.8 and 3.11.0.\n    # Similarly, this method's C equivalent in _asyncio is going away as well.\n    # See GH-99949 for more details.\n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n\n\ndef set_event_loop(loop):\n    \"\"\"Equivalent to calling get_event_loop_policy().set_event_loop(loop).\"\"\"\n    get_event_loop_policy().set_event_loop(loop)\n\n\ndef new_event_loop():\n    \"\"\"Equivalent to calling get_event_loop_policy().new_event_loop().\"\"\"\n    return get_event_loop_policy().new_event_loop()\n\n\ndef get_child_watcher():\n    \"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\n    return get_event_loop_policy().get_child_watcher()\n\n\ndef set_child_watcher(watcher):\n    \"\"\"Equivalent to calling\n    get_event_loop_policy().set_child_watcher(watcher).\"\"\"\n    return get_event_loop_policy().set_child_watcher(watcher)\n\n\n# Alias pure-Python implementations for testing purposes.\n_py__get_running_loop = _get_running_loop\n_py__set_running_loop = _set_running_loop\n_py_get_running_loop = get_running_loop\n_py_get_event_loop = get_event_loop\n_py__get_event_loop = _get_event_loop\n\n\ntry:\n    # get_event_loop() is one of the most frequently called\n    # functions in asyncio.  Pure Python implementation is\n    # about 4 times slower than C-accelerated.\n    from _asyncio import (_get_running_loop, _set_running_loop,\n                          get_running_loop, get_event_loop, _get_event_loop)\nexcept ImportError:\n    pass\nelse:\n    # Alias C implementations for testing purposes.\n    _c__get_running_loop = _get_running_loop\n    _c__set_running_loop = _set_running_loop\n    _c_get_running_loop = get_running_loop\n    _c_get_event_loop = get_event_loop\n    _c__get_event_loop = _get_event_loop\n", 842], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py": ["\"\"\"Selector event loop for Unix with signal handling.\"\"\"\n\nimport errno\nimport io\nimport itertools\nimport os\nimport selectors\nimport signal\nimport socket\nimport stat\nimport subprocess\nimport sys\nimport threading\nimport warnings\n\nfrom . import base_events\nfrom . import base_subprocess\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import selector_events\nfrom . import tasks\nfrom . import transports\nfrom .log import logger\n\n\n__all__ = (\n    'SelectorEventLoop',\n    'AbstractChildWatcher', 'SafeChildWatcher',\n    'FastChildWatcher', 'PidfdChildWatcher',\n    'MultiLoopChildWatcher', 'ThreadedChildWatcher',\n    'DefaultEventLoopPolicy',\n)\n\n\nif sys.platform == 'win32':  # pragma: no cover\n    raise ImportError('Signals are not really supported on Windows')\n\n\ndef _sighandler_noop(signum, frame):\n    \"\"\"Dummy signal handler.\"\"\"\n    pass\n\n\ndef waitstatus_to_exitcode(status):\n    try:\n        return os.waitstatus_to_exitcode(status)\n    except ValueError:\n        # The child exited, but we don't understand its status.\n        # This shouldn't happen, but if it does, let's just\n        # return that status; perhaps that helps debug it.\n        return status\n\n\nclass _UnixSelectorEventLoop(selector_events.BaseSelectorEventLoop):\n    \"\"\"Unix event loop.\n\n    Adds signal handling and UNIX Domain Socket support to SelectorEventLoop.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__(selector)\n        self._signal_handlers = {}\n\n    def close(self):\n        super().close()\n        if not sys.is_finalizing():\n            for sig in list(self._signal_handlers):\n                self.remove_signal_handler(sig)\n        else:\n            if self._signal_handlers:\n                warnings.warn(f\"Closing the loop {self!r} \"\n                              f\"on interpreter shutdown \"\n                              f\"stage, skipping signal handlers removal\",\n                              ResourceWarning,\n                              source=self)\n                self._signal_handlers.clear()\n\n    def _process_self_data(self, data):\n        for signum in data:\n            if not signum:\n                # ignore null bytes written by _write_to_self()\n                continue\n            self._handle_signal(signum)\n\n    def add_signal_handler(self, sig, callback, *args):\n        \"\"\"Add a handler for a signal.  UNIX only.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\"coroutines cannot be used \"\n                            \"with add_signal_handler()\")\n        self._check_signal(sig)\n        self._check_closed()\n        try:\n            # set_wakeup_fd() raises ValueError if this is not the\n            # main thread.  By calling it early we ensure that an\n            # event loop running in another thread cannot add a signal\n            # handler.\n            signal.set_wakeup_fd(self._csock.fileno())\n        except (ValueError, OSError) as exc:\n            raise RuntimeError(str(exc))\n\n        handle = events.Handle(callback, args, self, None)\n        self._signal_handlers[sig] = handle\n\n        try:\n            # Register a dummy signal handler to ask Python to write the signal\n            # number in the wakeup file descriptor. _process_self_data() will\n            # read signal numbers from this file descriptor to handle signals.\n            signal.signal(sig, _sighandler_noop)\n\n            # Set SA_RESTART to limit EINTR occurrences.\n            signal.siginterrupt(sig, False)\n        except OSError as exc:\n            del self._signal_handlers[sig]\n            if not self._signal_handlers:\n                try:\n                    signal.set_wakeup_fd(-1)\n                except (ValueError, OSError) as nexc:\n                    logger.info('set_wakeup_fd(-1) failed: %s', nexc)\n\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n    def _handle_signal(self, sig):\n        \"\"\"Internal helper that is the actual signal handler.\"\"\"\n        handle = self._signal_handlers.get(sig)\n        if handle is None:\n            return  # Assume it's some race condition.\n        if handle._cancelled:\n            self.remove_signal_handler(sig)  # Remove it properly.\n        else:\n            self._add_callback_signalsafe(handle)\n\n    def remove_signal_handler(self, sig):\n        \"\"\"Remove a handler for a signal.  UNIX only.\n\n        Return True if a signal handler was removed, False if not.\n        \"\"\"\n        self._check_signal(sig)\n        try:\n            del self._signal_handlers[sig]\n        except KeyError:\n            return False\n\n        if sig == signal.SIGINT:\n            handler = signal.default_int_handler\n        else:\n            handler = signal.SIG_DFL\n\n        try:\n            signal.signal(sig, handler)\n        except OSError as exc:\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n        if not self._signal_handlers:\n            try:\n                signal.set_wakeup_fd(-1)\n            except (ValueError, OSError) as exc:\n                logger.info('set_wakeup_fd(-1) failed: %s', exc)\n\n        return True\n\n    def _check_signal(self, sig):\n        \"\"\"Internal helper to validate a signal.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if not isinstance(sig, int):\n            raise TypeError(f'sig must be an int, not {sig!r}')\n\n        if sig not in signal.valid_signals():\n            raise ValueError(f'invalid signal number {sig}')\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        return _UnixReadPipeTransport(self, pipe, protocol, waiter, extra)\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        return _UnixWritePipeTransport(self, pipe, protocol, waiter, extra)\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        with events.get_child_watcher() as watcher:\n            if not watcher.is_active():\n                # Check early.\n                # Raising exception before process creation\n                # prevents subprocess execution if the watcher\n                # is not ready to handle it.\n                raise RuntimeError(\"asyncio.get_child_watcher() is not activated, \"\n                                   \"subprocess support is not installed.\")\n            waiter = self.create_future()\n            transp = _UnixSubprocessTransport(self, protocol, args, shell,\n                                              stdin, stdout, stderr, bufsize,\n                                              waiter=waiter, extra=extra,\n                                              **kwargs)\n\n            watcher.add_child_handler(transp.get_pid(),\n                                      self._child_watcher_callback, transp)\n            try:\n                await waiter\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                transp.close()\n                await transp._wait()\n                raise\n\n        return transp\n\n    def _child_watcher_callback(self, pid, returncode, transp):\n        # Skip one iteration for callbacks to be executed\n        self.call_soon_threadsafe(self.call_soon, transp._process_exited, returncode)\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        assert server_hostname is None or isinstance(server_hostname, str)\n        if ssl:\n            if server_hostname is None:\n                raise ValueError(\n                    'you have to pass server_hostname when using ssl')\n        else:\n            if server_hostname is not None:\n                raise ValueError('server_hostname is only meaningful with ssl')\n            if ssl_handshake_timeout is not None:\n                raise ValueError(\n                    'ssl_handshake_timeout is only meaningful with ssl')\n            if ssl_shutdown_timeout is not None:\n                raise ValueError(\n                    'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM, 0)\n            try:\n                sock.setblocking(False)\n                await self.sock_connect(sock, path)\n            except:\n                sock.close()\n                raise\n\n        else:\n            if sock is None:\n                raise ValueError('no path and sock were specified')\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n            sock.setblocking(False)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        return transport, protocol\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n\n            # Check for abstract socket. `str` and `bytes` paths are supported.\n            if path[0] not in (0, '\\x00'):\n                try:\n                    if stat.S_ISSOCK(os.stat(path).st_mode):\n                        os.remove(path)\n                except FileNotFoundError:\n                    pass\n                except OSError as err:\n                    # Directory may have permissions only to create socket.\n                    logger.error('Unable to check or remove stale UNIX socket '\n                                 '%r: %r', path, err)\n\n            try:\n                sock.bind(path)\n            except OSError as exc:\n                sock.close()\n                if exc.errno == errno.EADDRINUSE:\n                    # Let's improve the error message by adding\n                    # with what exact address it occurs.\n                    msg = f'Address {path!r} is already in use'\n                    raise OSError(errno.EADDRINUSE, msg) from None\n                else:\n                    raise\n            except:\n                sock.close()\n                raise\n        else:\n            if sock is None:\n                raise ValueError(\n                    'path was not specified, and no sock specified')\n\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n\n        sock.setblocking(False)\n        server = base_events.Server(self, [sock], protocol_factory,\n                                    ssl, backlog, ssl_handshake_timeout,\n                                    ssl_shutdown_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        return server\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        try:\n            os.sendfile\n        except AttributeError:\n            raise exceptions.SendfileNotAvailableError(\n                \"os.sendfile() is not available\")\n        try:\n            fileno = file.fileno()\n        except (AttributeError, io.UnsupportedOperation) as err:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        try:\n            fsize = os.fstat(fileno).st_size\n        except OSError:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        blocksize = count if count else fsize\n        if not blocksize:\n            return 0  # empty file\n\n        fut = self.create_future()\n        self._sock_sendfile_native_impl(fut, None, sock, fileno,\n                                        offset, count, blocksize, 0)\n        return await fut\n\n    def _sock_sendfile_native_impl(self, fut, registered_fd, sock, fileno,\n                                   offset, count, blocksize, total_sent):\n        fd = sock.fileno()\n        if registered_fd is not None:\n            # Remove the callback early.  It should be rare that the\n            # selector says the fd is ready but the call still returns\n            # EAGAIN, and I am willing to take a hit in that case in\n            # order to simplify the common case.\n            self.remove_writer(registered_fd)\n        if fut.cancelled():\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            return\n        if count:\n            blocksize = count - total_sent\n            if blocksize <= 0:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n                return\n\n        try:\n            sent = os.sendfile(fd, fileno, offset, blocksize)\n        except (BlockingIOError, InterruptedError):\n            if registered_fd is None:\n                self._sock_add_cancellation_callback(fut, sock)\n            self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                            fd, sock, fileno,\n                            offset, count, blocksize, total_sent)\n        except OSError as exc:\n            if (registered_fd is not None and\n                    exc.errno == errno.ENOTCONN and\n                    type(exc) is not ConnectionError):\n                # If we have an ENOTCONN and this isn't a first call to\n                # sendfile(), i.e. the connection was closed in the middle\n                # of the operation, normalize the error to ConnectionError\n                # to make it consistent across all Posix systems.\n                new_exc = ConnectionError(\n                    \"socket is not connected\", errno.ENOTCONN)\n                new_exc.__cause__ = exc\n                exc = new_exc\n            if total_sent == 0:\n                # We can get here for different reasons, the main\n                # one being 'file' is not a regular mmap(2)-like\n                # file, in which case we'll fall back on using\n                # plain send().\n                err = exceptions.SendfileNotAvailableError(\n                    \"os.sendfile call failed\")\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(err)\n            else:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            fut.set_exception(exc)\n        else:\n            if sent == 0:\n                # EOF\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n            else:\n                offset += sent\n                total_sent += sent\n                if registered_fd is None:\n                    self._sock_add_cancellation_callback(fut, sock)\n                self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                                fd, sock, fileno,\n                                offset, count, blocksize, total_sent)\n\n    def _sock_sendfile_update_filepos(self, fileno, offset, total_sent):\n        if total_sent > 0:\n            os.lseek(fileno, offset, os.SEEK_SET)\n\n    def _sock_add_cancellation_callback(self, fut, sock):\n        def cb(fut):\n            if fut.cancelled():\n                fd = sock.fileno()\n                if fd != -1:\n                    self.remove_writer(fd)\n        fut.add_done_callback(cb)\n\n\nclass _UnixReadPipeTransport(transports.ReadTransport):\n\n    max_size = 256 * 1024  # max bytes we read in one event loop iteration\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra)\n        self._extra['pipe'] = pipe\n        self._loop = loop\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._closing = False\n        self._paused = False\n\n        mode = os.fstat(self._fileno).st_mode\n        if not (stat.S_ISFIFO(mode) or\n                stat.S_ISSOCK(mode) or\n                stat.S_ISCHR(mode)):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is for pipes/sockets only.\")\n\n        os.set_blocking(self._fileno, False)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._fileno, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def _add_reader(self, fd, callback):\n        if not self.is_reading():\n            return\n        self._loop._add_reader(fd, callback)\n\n    def is_reading(self):\n        return not self._paused and not self._closing\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_READ)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def _read_ready(self):\n        try:\n            data = os.read(self._fileno, self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._fatal_error(exc, 'Fatal read error on pipe transport')\n        else:\n            if data:\n                self._protocol.data_received(data)\n            else:\n                if self._loop.get_debug():\n                    logger.info(\"%r was closed by peer\", self)\n                self._closing = True\n                self._loop._remove_reader(self._fileno)\n                self._loop.call_soon(self._protocol.eof_received)\n                self._loop.call_soon(self._call_connection_lost, None)\n\n    def pause_reading(self):\n        if not self.is_reading():\n            return\n        self._paused = True\n        self._loop._remove_reader(self._fileno)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._loop._add_reader(self._fileno, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if not self._closing:\n            self._close(None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if (isinstance(exc, OSError) and exc.errno == errno.EIO):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc):\n        self._closing = True\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixWritePipeTransport(transports._FlowControlMixin,\n                              transports.WriteTransport):\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra, loop)\n        self._extra['pipe'] = pipe\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._buffer = bytearray()\n        self._conn_lost = 0\n        self._closing = False  # Set when close() or write_eof() called.\n\n        mode = os.fstat(self._fileno).st_mode\n        is_char = stat.S_ISCHR(mode)\n        is_fifo = stat.S_ISFIFO(mode)\n        is_socket = stat.S_ISSOCK(mode)\n        if not (is_char or is_fifo or is_socket):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is only for \"\n                             \"pipes, sockets and character devices\")\n\n        os.set_blocking(self._fileno, False)\n        self._loop.call_soon(self._protocol.connection_made, self)\n\n        # On AIX, the reader trick (to be notified when the read end of the\n        # socket is closed) only works for sockets. On other platforms it\n        # works for pipes and sockets. (Exception: OS X 10.4?  Issue #19294.)\n        if is_socket or (is_fifo and not sys.platform.startswith(\"aix\")):\n            # only start reading when connection_made() has been called\n            self._loop.call_soon(self._loop._add_reader,\n                                 self._fileno, self._read_ready)\n\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_WRITE)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'bufsize={bufsize}')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _read_ready(self):\n        # Pipe was closed by peer.\n        if self._loop.get_debug():\n            logger.info(\"%r was closed by peer\", self)\n        if self._buffer:\n            self._close(BrokenPipeError())\n        else:\n            self._close()\n\n    def write(self, data):\n        assert isinstance(data, (bytes, bytearray, memoryview)), repr(data)\n        if isinstance(data, bytearray):\n            data = memoryview(data)\n        if not data:\n            return\n\n        if self._conn_lost or self._closing:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('pipe closed by peer or '\n                               'os.write(pipe, data) raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                n = os.write(self._fileno, data)\n            except (BlockingIOError, InterruptedError):\n                n = 0\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._conn_lost += 1\n                self._fatal_error(exc, 'Fatal write error on pipe transport')\n                return\n            if n == len(data):\n                return\n            elif n > 0:\n                data = memoryview(data)[n:]\n            self._loop._add_writer(self._fileno, self._write_ready)\n\n        self._buffer += data\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        try:\n            n = os.write(self._fileno, self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._buffer.clear()\n            self._conn_lost += 1\n            # Remove writer here, _fatal_error() doesn't it\n            # because _buffer is empty.\n            self._loop._remove_writer(self._fileno)\n            self._fatal_error(exc, 'Fatal write error on pipe transport')\n        else:\n            if n == len(self._buffer):\n                self._buffer.clear()\n                self._loop._remove_writer(self._fileno)\n                self._maybe_resume_protocol()  # May append to buffer.\n                if self._closing:\n                    self._loop._remove_reader(self._fileno)\n                    self._call_connection_lost(None)\n                return\n            elif n > 0:\n                del self._buffer[:n]\n\n    def can_write_eof(self):\n        return True\n\n    def write_eof(self):\n        if self._closing:\n            return\n        assert self._pipe\n        self._closing = True\n        if not self._buffer:\n            self._loop._remove_reader(self._fileno)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._pipe is not None and not self._closing:\n            # write_eof is all what we needed to close the write pipe\n            self.write_eof()\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def abort(self):\n        self._close(None)\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc=None):\n        self._closing = True\n        if self._buffer:\n            self._loop._remove_writer(self._fileno)\n        self._buffer.clear()\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixSubprocessTransport(base_subprocess.BaseSubprocessTransport):\n\n    def _start(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs):\n        stdin_w = None\n        if stdin == subprocess.PIPE and sys.platform.startswith('aix'):\n            # Use a socket pair for stdin on AIX, since it does not\n            # support selecting read events on the write end of a\n            # socket (which we use in order to detect closing of the\n            # other end).\n            stdin, stdin_w = socket.socketpair()\n        try:\n            self._proc = subprocess.Popen(\n                args, shell=shell, stdin=stdin, stdout=stdout, stderr=stderr,\n                universal_newlines=False, bufsize=bufsize, **kwargs)\n            if stdin_w is not None:\n                stdin.close()\n                self._proc.stdin = open(stdin_w.detach(), 'wb', buffering=bufsize)\n                stdin_w = None\n        finally:\n            if stdin_w is not None:\n                stdin.close()\n                stdin_w.close()\n\n\nclass AbstractChildWatcher:\n    \"\"\"Abstract base class for monitoring child processes.\n\n    Objects derived from this class monitor a collection of subprocesses and\n    report their termination or interruption by a signal.\n\n    New callbacks are registered with .add_child_handler(). Starting a new\n    process must be done within a 'with' block to allow the watcher to suspend\n    its activity until the new process if fully registered (this is needed to\n    prevent a race condition in some implementations).\n\n    Example:\n        with watcher:\n            proc = subprocess.Popen(\"sleep 1\")\n            watcher.add_child_handler(proc.pid, callback)\n\n    Notes:\n        Implementations of this class must be thread-safe.\n\n        Since child watcher objects may catch the SIGCHLD signal and call\n        waitpid(-1), there should be only one active object per process.\n    \"\"\"\n\n    def add_child_handler(self, pid, callback, *args):\n        \"\"\"Register a new child handler.\n\n        Arrange for callback(pid, returncode, *args) to be called when\n        process 'pid' terminates. Specifying another callback for the same\n        process replaces the previous handler.\n\n        Note: callback() must be thread-safe.\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_child_handler(self, pid):\n        \"\"\"Removes the handler for process 'pid'.\n\n        The function returns True if the handler was successfully removed,\n        False if there was nothing to remove.\"\"\"\n\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        \"\"\"Attach the watcher to an event loop.\n\n        If the watcher was previously attached to an event loop, then it is\n        first detached before attaching to the new loop.\n\n        Note: loop may be None.\n        \"\"\"\n        raise NotImplementedError()\n\n    def close(self):\n        \"\"\"Close the watcher.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        raise NotImplementedError()\n\n    def is_active(self):\n        \"\"\"Return ``True`` if the watcher is active and is used by the event loop.\n\n        Return True if the watcher is installed and ready to handle process exit\n        notifications.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def __enter__(self):\n        \"\"\"Enter the watcher's context and allow starting new processes\n\n        This function must return self\"\"\"\n        raise NotImplementedError()\n\n    def __exit__(self, a, b, c):\n        \"\"\"Exit the watcher's context\"\"\"\n        raise NotImplementedError()\n\n\nclass PidfdChildWatcher(AbstractChildWatcher):\n    \"\"\"Child watcher implementation using Linux's pid file descriptors.\n\n    This child watcher polls process file descriptors (pidfds) to await child\n    process termination. In some respects, PidfdChildWatcher is a \"Goldilocks\"\n    child watcher implementation. It doesn't require signals or threads, doesn't\n    interfere with any processes launched outside the event loop, and scales\n    linearly with the number of subprocesses launched by the event loop. The\n    main disadvantage is that pidfds are specific to Linux, and only work on\n    recent (5.3+) kernels.\n    \"\"\"\n\n    def __init__(self):\n        self._loop = None\n        self._callbacks = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        pass\n\n    def is_active(self):\n        return self._loop is not None and self._loop.is_running()\n\n    def close(self):\n        self.attach_loop(None)\n\n    def attach_loop(self, loop):\n        if self._loop is not None and loop is None and self._callbacks:\n            warnings.warn(\n                'A loop is being detached '\n                'from a child watcher with pending handlers',\n                RuntimeWarning)\n        for pidfd, _, _ in self._callbacks.values():\n            self._loop._remove_reader(pidfd)\n            os.close(pidfd)\n        self._callbacks.clear()\n        self._loop = loop\n\n    def add_child_handler(self, pid, callback, *args):\n        existing = self._callbacks.get(pid)\n        if existing is not None:\n            self._callbacks[pid] = existing[0], callback, args\n        else:\n            pidfd = os.pidfd_open(pid)\n            self._loop._add_reader(pidfd, self._do_wait, pid)\n            self._callbacks[pid] = pidfd, callback, args\n\n    def _do_wait(self, pid):\n        pidfd, callback, args = self._callbacks.pop(pid)\n        self._loop._remove_reader(pidfd)\n        try:\n            _, status = os.waitpid(pid, 0)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            returncode = 255\n            logger.warning(\n                \"child process pid %d exit status already read: \"\n                \" will report returncode 255\",\n                pid)\n        else:\n            returncode = waitstatus_to_exitcode(status)\n\n        os.close(pidfd)\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        try:\n            pidfd, _, _ = self._callbacks.pop(pid)\n        except KeyError:\n            return False\n        self._loop._remove_reader(pidfd)\n        os.close(pidfd)\n        return True\n\n\nclass BaseChildWatcher(AbstractChildWatcher):\n\n    def __init__(self):\n        self._loop = None\n        self._callbacks = {}\n\n    def close(self):\n        self.attach_loop(None)\n\n    def is_active(self):\n        return self._loop is not None and self._loop.is_running()\n\n    def _do_waitpid(self, expected_pid):\n        raise NotImplementedError()\n\n    def _do_waitpid_all(self):\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        assert loop is None or isinstance(loop, events.AbstractEventLoop)\n\n        if self._loop is not None and loop is None and self._callbacks:\n            warnings.warn(\n                'A loop is being detached '\n                'from a child watcher with pending handlers',\n                RuntimeWarning)\n\n        if self._loop is not None:\n            self._loop.remove_signal_handler(signal.SIGCHLD)\n\n        self._loop = loop\n        if loop is not None:\n            loop.add_signal_handler(signal.SIGCHLD, self._sig_chld)\n\n            # Prevent a race condition in case a child terminated\n            # during the switch.\n            self._do_waitpid_all()\n\n    def _sig_chld(self):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            # self._loop should always be available here\n            # as '_sig_chld' is added as a signal handler\n            # in 'attach_loop'\n            self._loop.call_exception_handler({\n                'message': 'Unknown exception in SIGCHLD handler',\n                'exception': exc,\n            })\n\n\nclass SafeChildWatcher(BaseChildWatcher):\n    \"\"\"'Safe' child watcher implementation.\n\n    This implementation avoids disrupting other code spawning processes by\n    polling explicitly each process in the SIGCHLD handler instead of calling\n    os.waitpid(-1).\n\n    This is a safe solution but it has a significant overhead when handling a\n    big number of children (O(n) each time SIGCHLD is raised)\n    \"\"\"\n\n    def close(self):\n        self._callbacks.clear()\n        super().close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, a, b, c):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        self._callbacks[pid] = (callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = waitstatus_to_exitcode(status)\n            if self._loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        try:\n            callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            if self._loop.get_debug():\n                logger.warning(\"Child watcher got an unexpected pid: %r\",\n                               pid, exc_info=True)\n        else:\n            callback(pid, returncode, *args)\n\n\nclass FastChildWatcher(BaseChildWatcher):\n    \"\"\"'Fast' child watcher implementation.\n\n    This implementation reaps every terminated processes by calling\n    os.waitpid(-1) directly, possibly breaking other code spawning processes\n    and waiting for their termination.\n\n    There is no noticeable overhead when handling a big number of children\n    (O(1) each time a child terminates).\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = threading.Lock()\n        self._zombies = {}\n        self._forks = 0\n\n    def close(self):\n        self._callbacks.clear()\n        self._zombies.clear()\n        super().close()\n\n    def __enter__(self):\n        with self._lock:\n            self._forks += 1\n\n            return self\n\n    def __exit__(self, a, b, c):\n        with self._lock:\n            self._forks -= 1\n\n            if self._forks or not self._zombies:\n                return\n\n            collateral_victims = str(self._zombies)\n            self._zombies.clear()\n\n        logger.warning(\n            \"Caught subprocesses termination from unknown pids: %s\",\n            collateral_victims)\n\n    def add_child_handler(self, pid, callback, *args):\n        assert self._forks, \"Must use the context manager\"\n\n        with self._lock:\n            try:\n                returncode = self._zombies.pop(pid)\n            except KeyError:\n                # The child is running.\n                self._callbacks[pid] = callback, args\n                return\n\n        # The child is dead already. We can fire the callback.\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n        # Because of signal coalescing, we must keep calling waitpid() as\n        # long as we're able to reap a child.\n        while True:\n            try:\n                pid, status = os.waitpid(-1, os.WNOHANG)\n            except ChildProcessError:\n                # No more child processes exist.\n                return\n            else:\n                if pid == 0:\n                    # A child process is still alive.\n                    return\n\n                returncode = waitstatus_to_exitcode(status)\n\n            with self._lock:\n                try:\n                    callback, args = self._callbacks.pop(pid)\n                except KeyError:\n                    # unknown child\n                    if self._forks:\n                        # It may not be registered yet.\n                        self._zombies[pid] = returncode\n                        if self._loop.get_debug():\n                            logger.debug('unknown process %s exited '\n                                         'with returncode %s',\n                                         pid, returncode)\n                        continue\n                    callback = None\n                else:\n                    if self._loop.get_debug():\n                        logger.debug('process %s exited with returncode %s',\n                                     pid, returncode)\n\n            if callback is None:\n                logger.warning(\n                    \"Caught subprocess termination from unknown pid: \"\n                    \"%d -> %d\", pid, returncode)\n            else:\n                callback(pid, returncode, *args)\n\n\nclass MultiLoopChildWatcher(AbstractChildWatcher):\n    \"\"\"A watcher that doesn't require running loop in the main thread.\n\n    This implementation registers a SIGCHLD signal handler on\n    instantiation (which may conflict with other code that\n    install own handler for this signal).\n\n    The solution is safe but it has a significant overhead when\n    handling a big number of processes (*O(n)* each time a\n    SIGCHLD is received).\n    \"\"\"\n\n    # Implementation note:\n    # The class keeps compatibility with AbstractChildWatcher ABC\n    # To achieve this it has empty attach_loop() method\n    # and doesn't accept explicit loop argument\n    # for add_child_handler()/remove_child_handler()\n    # but retrieves the current loop by get_running_loop()\n\n    def __init__(self):\n        self._callbacks = {}\n        self._saved_sighandler = None\n\n    def is_active(self):\n        return self._saved_sighandler is not None\n\n    def close(self):\n        self._callbacks.clear()\n        if self._saved_sighandler is None:\n            return\n\n        handler = signal.getsignal(signal.SIGCHLD)\n        if handler != self._sig_chld:\n            logger.warning(\"SIGCHLD handler was changed by outside code\")\n        else:\n            signal.signal(signal.SIGCHLD, self._saved_sighandler)\n        self._saved_sighandler = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        self._callbacks[pid] = (loop, callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def attach_loop(self, loop):\n        # Don't save the loop but initialize itself if called first time\n        # The reason to do it here is that attach_loop() is called from\n        # unix policy only for the main thread.\n        # Main thread is required for subscription on SIGCHLD signal\n        if self._saved_sighandler is not None:\n            return\n\n        self._saved_sighandler = signal.signal(signal.SIGCHLD, self._sig_chld)\n        if self._saved_sighandler is None:\n            logger.warning(\"Previous SIGCHLD handler was set by non-Python code, \"\n                           \"restore to default handler on watcher close.\")\n            self._saved_sighandler = signal.SIG_DFL\n\n        # Set SA_RESTART to limit EINTR occurrences.\n        signal.siginterrupt(signal.SIGCHLD, False)\n\n    def _do_waitpid_all(self):\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n            debug_log = False\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = waitstatus_to_exitcode(status)\n            debug_log = True\n        try:\n            loop, callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            logger.warning(\"Child watcher got an unexpected pid: %r\",\n                           pid, exc_info=True)\n        else:\n            if loop.is_closed():\n                logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n            else:\n                if debug_log and loop.get_debug():\n                    logger.debug('process %s exited with returncode %s',\n                                 expected_pid, returncode)\n                loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n    def _sig_chld(self, signum, frame):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException:\n            logger.warning('Unknown exception in SIGCHLD handler', exc_info=True)\n\n\nclass ThreadedChildWatcher(AbstractChildWatcher):\n    \"\"\"Threaded child watcher implementation.\n\n    The watcher uses a thread per process\n    for waiting for the process finish.\n\n    It doesn't require subscription on POSIX signal\n    but a thread creation is not free.\n\n    The watcher has O(1) complexity, its performance doesn't depend\n    on amount of spawn processes.\n    \"\"\"\n\n    def __init__(self):\n        self._pid_counter = itertools.count(0)\n        self._threads = {}\n\n    def is_active(self):\n        return True\n\n    def close(self):\n        self._join_threads()\n\n    def _join_threads(self):\n        \"\"\"Internal: Join all non-daemon threads\"\"\"\n        threads = [thread for thread in list(self._threads.values())\n                   if thread.is_alive() and not thread.daemon]\n        for thread in threads:\n            thread.join()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def __del__(self, _warn=warnings.warn):\n        threads = [thread for thread in list(self._threads.values())\n                   if thread.is_alive()]\n        if threads:\n            _warn(f\"{self.__class__} has registered but not finished child processes\",\n                  ResourceWarning,\n                  source=self)\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        thread = threading.Thread(target=self._do_waitpid,\n                                  name=f\"waitpid-{next(self._pid_counter)}\",\n                                  args=(loop, pid, callback, args),\n                                  daemon=True)\n        self._threads[pid] = thread\n        thread.start()\n\n    def remove_child_handler(self, pid):\n        # asyncio never calls remove_child_handler() !!!\n        # The method is no-op but is implemented because\n        # abstract base classes require it.\n        return True\n\n    def attach_loop(self, loop):\n        pass\n\n    def _do_waitpid(self, loop, expected_pid, callback, args):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, 0)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            returncode = waitstatus_to_exitcode(status)\n            if loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        if loop.is_closed():\n            logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n        else:\n            loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n        self._threads.pop(expected_pid)\n\n\nclass _UnixDefaultEventLoopPolicy(events.BaseDefaultEventLoopPolicy):\n    \"\"\"UNIX event loop policy with a watcher for child processes.\"\"\"\n    _loop_factory = _UnixSelectorEventLoop\n\n    def __init__(self):\n        super().__init__()\n        self._watcher = None\n\n    def _init_watcher(self):\n        with events._lock:\n            if self._watcher is None:  # pragma: no branch\n                self._watcher = ThreadedChildWatcher()\n                if threading.current_thread() is threading.main_thread():\n                    self._watcher.attach_loop(self._local._loop)\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\n\n        As a side effect, if a child watcher was set before, then calling\n        .set_event_loop() from the main thread will call .attach_loop(loop) on\n        the child watcher.\n        \"\"\"\n\n        super().set_event_loop(loop)\n\n        if (self._watcher is not None and\n                threading.current_thread() is threading.main_thread()):\n            self._watcher.attach_loop(loop)\n\n    def get_child_watcher(self):\n        \"\"\"Get the watcher for child processes.\n\n        If not yet set, a ThreadedChildWatcher object is automatically created.\n        \"\"\"\n        if self._watcher is None:\n            self._init_watcher()\n\n        return self._watcher\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n\n        assert watcher is None or isinstance(watcher, AbstractChildWatcher)\n\n        if self._watcher is not None:\n            self._watcher.close()\n\n        self._watcher = watcher\n\n\nSelectorEventLoop = _UnixSelectorEventLoop\nDefaultEventLoopPolicy = _UnixDefaultEventLoopPolicy\n", 1485], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py": ["__all__ = 'iscoroutinefunction', 'iscoroutine'\n\nimport collections.abc\nimport inspect\nimport os\nimport sys\nimport traceback\nimport types\n\n\ndef _is_debug_mode():\n    # See: https://docs.python.org/3/library/asyncio-dev.html#asyncio-debug-mode.\n    return sys.flags.dev_mode or (not sys.flags.ignore_environment and\n                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))\n\n\n# A marker for iscoroutinefunction.\n_is_coroutine = object()\n\n\ndef iscoroutinefunction(func):\n    \"\"\"Return True if func is a decorated coroutine function.\"\"\"\n    return (inspect.iscoroutinefunction(func) or\n            getattr(func, '_is_coroutine', None) is _is_coroutine)\n\n\n# Prioritize native coroutine check to speed-up\n# asyncio.iscoroutine.\n_COROUTINE_TYPES = (types.CoroutineType, types.GeneratorType,\n                    collections.abc.Coroutine)\n_iscoroutine_typecache = set()\n\n\ndef iscoroutine(obj):\n    \"\"\"Return True if obj is a coroutine object.\"\"\"\n    if type(obj) in _iscoroutine_typecache:\n        return True\n\n    if isinstance(obj, _COROUTINE_TYPES):\n        # Just in case we don't want to cache more than 100\n        # positive types.  That shouldn't ever happen, unless\n        # someone stressing the system on purpose.\n        if len(_iscoroutine_typecache) < 100:\n            _iscoroutine_typecache.add(type(obj))\n        return True\n    else:\n        return False\n\n\ndef _format_coroutine(coro):\n    assert iscoroutine(coro)\n\n    def get_name(coro):\n        # Coroutines compiled with Cython sometimes don't have\n        # proper __qualname__ or __name__.  While that is a bug\n        # in Cython, asyncio shouldn't crash with an AttributeError\n        # in its __repr__ functions.\n        if hasattr(coro, '__qualname__') and coro.__qualname__:\n            coro_name = coro.__qualname__\n        elif hasattr(coro, '__name__') and coro.__name__:\n            coro_name = coro.__name__\n        else:\n            # Stop masking Cython bugs, expose them in a friendly way.\n            coro_name = f'<{type(coro).__name__} without __name__>'\n        return f'{coro_name}()'\n\n    def is_running(coro):\n        try:\n            return coro.cr_running\n        except AttributeError:\n            try:\n                return coro.gi_running\n            except AttributeError:\n                return False\n\n    coro_code = None\n    if hasattr(coro, 'cr_code') and coro.cr_code:\n        coro_code = coro.cr_code\n    elif hasattr(coro, 'gi_code') and coro.gi_code:\n        coro_code = coro.gi_code\n\n    coro_name = get_name(coro)\n\n    if not coro_code:\n        # Built-in types might not have __qualname__ or __name__.\n        if is_running(coro):\n            return f'{coro_name} running'\n        else:\n            return coro_name\n\n    coro_frame = None\n    if hasattr(coro, 'gi_frame') and coro.gi_frame:\n        coro_frame = coro.gi_frame\n    elif hasattr(coro, 'cr_frame') and coro.cr_frame:\n        coro_frame = coro.cr_frame\n\n    # If Cython's coroutine has a fake code object without proper\n    # co_filename -- expose that.\n    filename = coro_code.co_filename or '<empty co_filename>'\n\n    lineno = 0\n\n    if coro_frame is not None:\n        lineno = coro_frame.f_lineno\n        coro_repr = f'{coro_name} running at {filename}:{lineno}'\n\n    else:\n        lineno = coro_code.co_firstlineno\n        coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n\n    return coro_repr\n", 111], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py": ["\"\"\"Base implementation of event loop.\n\nThe event loop can be broken up into a multiplexer (the part\nresponsible for notifying us of I/O events) and the event loop proper,\nwhich wraps a multiplexer with functionality for scheduling callbacks,\nimmediately or at a given time in the future.\n\nWhenever a public API takes a callback, subsequent positional\narguments will be passed to the callback if/when it is called.  This\navoids the proliferation of trivial lambdas implementing closures.\nKeyword arguments for the callback are not supported; this is a\nconscious design decision, leaving the door open for keyword arguments\nto modify the meaning of the API call itself.\n\"\"\"\n\nimport collections\nimport collections.abc\nimport concurrent.futures\nimport functools\nimport heapq\nimport itertools\nimport os\nimport socket\nimport stat\nimport subprocess\nimport threading\nimport time\nimport traceback\nimport sys\nimport warnings\nimport weakref\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import staggered\nfrom . import tasks\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\n__all__ = 'BaseEventLoop','Server',\n\n\n# Minimum number of _scheduled timer handles before cleanup of\n# cancelled handles is performed.\n_MIN_SCHEDULED_TIMER_HANDLES = 100\n\n# Minimum fraction of _scheduled timer handles that are cancelled\n# before cleanup of cancelled handles is performed.\n_MIN_CANCELLED_TIMER_HANDLES_FRACTION = 0.5\n\n\n_HAS_IPv6 = hasattr(socket, 'AF_INET6')\n\n# Maximum timeout passed to select to avoid OS limitations\nMAXIMUM_SELECT_TIMEOUT = 24 * 3600\n\n\ndef _format_handle(handle):\n    cb = handle._callback\n    if isinstance(getattr(cb, '__self__', None), tasks.Task):\n        # format the task\n        return repr(cb.__self__)\n    else:\n        return str(handle)\n\n\ndef _format_pipe(fd):\n    if fd == subprocess.PIPE:\n        return '<pipe>'\n    elif fd == subprocess.STDOUT:\n        return '<stdout>'\n    else:\n        return repr(fd)\n\n\ndef _set_reuseport(sock):\n    if not hasattr(socket, 'SO_REUSEPORT'):\n        raise ValueError('reuse_port not supported by socket module')\n    else:\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        except OSError:\n            raise ValueError('reuse_port not supported by socket module, '\n                             'SO_REUSEPORT defined but not implemented.')\n\n\ndef _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):\n    # Try to skip getaddrinfo if \"host\" is already an IP. Users might have\n    # handled name resolution in their own code and pass in resolved IPs.\n    if not hasattr(socket, 'inet_pton'):\n        return\n\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or \\\n            host is None:\n        return None\n\n    if type == socket.SOCK_STREAM:\n        proto = socket.IPPROTO_TCP\n    elif type == socket.SOCK_DGRAM:\n        proto = socket.IPPROTO_UDP\n    else:\n        return None\n\n    if port is None:\n        port = 0\n    elif isinstance(port, bytes) and port == b'':\n        port = 0\n    elif isinstance(port, str) and port == '':\n        port = 0\n    else:\n        # If port's a service name like \"http\", don't skip getaddrinfo.\n        try:\n            port = int(port)\n        except (TypeError, ValueError):\n            return None\n\n    if family == socket.AF_UNSPEC:\n        afs = [socket.AF_INET]\n        if _HAS_IPv6:\n            afs.append(socket.AF_INET6)\n    else:\n        afs = [family]\n\n    if isinstance(host, bytes):\n        host = host.decode('idna')\n    if '%' in host:\n        # Linux's inet_pton doesn't accept an IPv6 zone index after host,\n        # like '::1%lo0'.\n        return None\n\n    for af in afs:\n        try:\n            socket.inet_pton(af, host)\n            # The host has already been resolved.\n            if _HAS_IPv6 and af == socket.AF_INET6:\n                return af, type, proto, '', (host, port, flowinfo, scopeid)\n            else:\n                return af, type, proto, '', (host, port)\n        except OSError:\n            pass\n\n    # \"host\" is not an IP address.\n    return None\n\n\ndef _interleave_addrinfos(addrinfos, first_address_family_count=1):\n    \"\"\"Interleave list of addrinfo tuples by family.\"\"\"\n    # Group addresses by family\n    addrinfos_by_family = collections.OrderedDict()\n    for addr in addrinfos:\n        family = addr[0]\n        if family not in addrinfos_by_family:\n            addrinfos_by_family[family] = []\n        addrinfos_by_family[family].append(addr)\n    addrinfos_lists = list(addrinfos_by_family.values())\n\n    reordered = []\n    if first_address_family_count > 1:\n        reordered.extend(addrinfos_lists[0][:first_address_family_count - 1])\n        del addrinfos_lists[0][:first_address_family_count - 1]\n    reordered.extend(\n        a for a in itertools.chain.from_iterable(\n            itertools.zip_longest(*addrinfos_lists)\n        ) if a is not None)\n    return reordered\n\n\ndef _run_until_complete_cb(fut):\n    if not fut.cancelled():\n        exc = fut.exception()\n        if isinstance(exc, (SystemExit, KeyboardInterrupt)):\n            # Issue #22429: run_forever() already finished, no need to\n            # stop it.\n            return\n    futures._get_loop(fut).stop()\n\n\nif hasattr(socket, 'TCP_NODELAY'):\n    def _set_nodelay(sock):\n        if (sock.family in {socket.AF_INET, socket.AF_INET6} and\n                sock.type == socket.SOCK_STREAM and\n                sock.proto == socket.IPPROTO_TCP):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nelse:\n    def _set_nodelay(sock):\n        pass\n\n\ndef _check_ssl_socket(sock):\n    if ssl is not None and isinstance(sock, ssl.SSLSocket):\n        raise TypeError(\"Socket cannot be of type SSLSocket\")\n\n\nclass _SendfileFallbackProtocol(protocols.Protocol):\n    def __init__(self, transp):\n        if not isinstance(transp, transports._FlowControlMixin):\n            raise TypeError(\"transport should be _FlowControlMixin instance\")\n        self._transport = transp\n        self._proto = transp.get_protocol()\n        self._should_resume_reading = transp.is_reading()\n        self._should_resume_writing = transp._protocol_paused\n        transp.pause_reading()\n        transp.set_protocol(self)\n        if self._should_resume_writing:\n            self._write_ready_fut = self._transport._loop.create_future()\n        else:\n            self._write_ready_fut = None\n\n    async def drain(self):\n        if self._transport.is_closing():\n            raise ConnectionError(\"Connection closed by peer\")\n        fut = self._write_ready_fut\n        if fut is None:\n            return\n        await fut\n\n    def connection_made(self, transport):\n        raise RuntimeError(\"Invalid state: \"\n                           \"connection should have been established already.\")\n\n    def connection_lost(self, exc):\n        if self._write_ready_fut is not None:\n            # Never happens if peer disconnects after sending the whole content\n            # Thus disconnection is always an exception from user perspective\n            if exc is None:\n                self._write_ready_fut.set_exception(\n                    ConnectionError(\"Connection is closed by peer\"))\n            else:\n                self._write_ready_fut.set_exception(exc)\n        self._proto.connection_lost(exc)\n\n    def pause_writing(self):\n        if self._write_ready_fut is not None:\n            return\n        self._write_ready_fut = self._transport._loop.create_future()\n\n    def resume_writing(self):\n        if self._write_ready_fut is None:\n            return\n        self._write_ready_fut.set_result(False)\n        self._write_ready_fut = None\n\n    def data_received(self, data):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    def eof_received(self):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    async def restore(self):\n        self._transport.set_protocol(self._proto)\n        if self._should_resume_reading:\n            self._transport.resume_reading()\n        if self._write_ready_fut is not None:\n            # Cancel the future.\n            # Basically it has no effect because protocol is switched back,\n            # no code should wait for it anymore.\n            self._write_ready_fut.cancel()\n        if self._should_resume_writing:\n            self._proto.resume_writing()\n\n\nclass Server(events.AbstractServer):\n\n    def __init__(self, loop, sockets, protocol_factory, ssl_context, backlog,\n                 ssl_handshake_timeout, ssl_shutdown_timeout=None):\n        self._loop = loop\n        self._sockets = sockets\n        self._active_count = 0\n        self._waiters = []\n        self._protocol_factory = protocol_factory\n        self._backlog = backlog\n        self._ssl_context = ssl_context\n        self._ssl_handshake_timeout = ssl_handshake_timeout\n        self._ssl_shutdown_timeout = ssl_shutdown_timeout\n        self._serving = False\n        self._serving_forever_fut = None\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__} sockets={self.sockets!r}>'\n\n    def _attach(self):\n        assert self._sockets is not None\n        self._active_count += 1\n\n    def _detach(self):\n        assert self._active_count > 0\n        self._active_count -= 1\n        if self._active_count == 0 and self._sockets is None:\n            self._wakeup()\n\n    def _wakeup(self):\n        waiters = self._waiters\n        self._waiters = None\n        for waiter in waiters:\n            if not waiter.done():\n                waiter.set_result(waiter)\n\n    def _start_serving(self):\n        if self._serving:\n            return\n        self._serving = True\n        for sock in self._sockets:\n            sock.listen(self._backlog)\n            self._loop._start_serving(\n                self._protocol_factory, sock, self._ssl_context,\n                self, self._backlog, self._ssl_handshake_timeout,\n                self._ssl_shutdown_timeout)\n\n    def get_loop(self):\n        return self._loop\n\n    def is_serving(self):\n        return self._serving\n\n    @property\n    def sockets(self):\n        if self._sockets is None:\n            return ()\n        return tuple(trsock.TransportSocket(s) for s in self._sockets)\n\n    def close(self):\n        sockets = self._sockets\n        if sockets is None:\n            return\n        self._sockets = None\n\n        for sock in sockets:\n            self._loop._stop_serving(sock)\n\n        self._serving = False\n\n        if (self._serving_forever_fut is not None and\n                not self._serving_forever_fut.done()):\n            self._serving_forever_fut.cancel()\n            self._serving_forever_fut = None\n\n        if self._active_count == 0:\n            self._wakeup()\n\n    async def start_serving(self):\n        self._start_serving()\n        # Skip one loop iteration so that all 'loop.add_reader'\n        # go through.\n        await tasks.sleep(0)\n\n    async def serve_forever(self):\n        if self._serving_forever_fut is not None:\n            raise RuntimeError(\n                f'server {self!r} is already being awaited on serve_forever()')\n        if self._sockets is None:\n            raise RuntimeError(f'server {self!r} is closed')\n\n        self._start_serving()\n        self._serving_forever_fut = self._loop.create_future()\n\n        try:\n            await self._serving_forever_fut\n        except exceptions.CancelledError:\n            try:\n                self.close()\n                await self.wait_closed()\n            finally:\n                raise\n        finally:\n            self._serving_forever_fut = None\n\n    async def wait_closed(self):\n        if self._sockets is None or self._waiters is None:\n            return\n        waiter = self._loop.create_future()\n        self._waiters.append(waiter)\n        await waiter\n\n\nclass BaseEventLoop(events.AbstractEventLoop):\n\n    def __init__(self):\n        self._timer_cancelled_count = 0\n        self._closed = False\n        self._stopping = False\n        self._ready = collections.deque()\n        self._scheduled = []\n        self._default_executor = None\n        self._internal_fds = 0\n        # Identifier of the thread running the event loop, or None if the\n        # event loop is not running\n        self._thread_id = None\n        self._clock_resolution = time.get_clock_info('monotonic').resolution\n        self._exception_handler = None\n        self.set_debug(coroutines._is_debug_mode())\n        # In debug mode, if the execution of a callback or a step of a task\n        # exceed this duration in seconds, the slow callback/task is logged.\n        self.slow_callback_duration = 0.1\n        self._current_handle = None\n        self._task_factory = None\n        self._coroutine_origin_tracking_enabled = False\n        self._coroutine_origin_tracking_saved_depth = None\n\n        # A weak set of all asynchronous generators that are\n        # being iterated by the loop.\n        self._asyncgens = weakref.WeakSet()\n        # Set to True when `loop.shutdown_asyncgens` is called.\n        self._asyncgens_shutdown_called = False\n        # Set to True when `loop.shutdown_default_executor` is called.\n        self._executor_shutdown_called = False\n\n    def __repr__(self):\n        return (\n            f'<{self.__class__.__name__} running={self.is_running()} '\n            f'closed={self.is_closed()} debug={self.get_debug()}>'\n        )\n\n    def create_future(self):\n        \"\"\"Create a Future object attached to the loop.\"\"\"\n        return futures.Future(loop=self)\n\n    def create_task(self, coro, *, name=None, context=None):\n        \"\"\"Schedule a coroutine object.\n\n        Return a task object.\n        \"\"\"\n        self._check_closed()\n        if self._task_factory is None:\n            task = tasks.Task(coro, loop=self, name=name, context=context)\n            if task._source_traceback:\n                del task._source_traceback[-1]\n        else:\n            if context is None:\n                # Use legacy API if context is not needed\n                task = self._task_factory(self, coro)\n            else:\n                task = self._task_factory(self, coro, context=context)\n\n            tasks._set_task_name(task, name)\n\n        return task\n\n    def set_task_factory(self, factory):\n        \"\"\"Set a task factory that will be used by loop.create_task().\n\n        If factory is None the default task factory will be set.\n\n        If factory is a callable, it should have a signature matching\n        '(loop, coro)', where 'loop' will be a reference to the active\n        event loop, 'coro' will be a coroutine object.  The callable\n        must return a Future.\n        \"\"\"\n        if factory is not None and not callable(factory):\n            raise TypeError('task factory must be a callable or None')\n        self._task_factory = factory\n\n    def get_task_factory(self):\n        \"\"\"Return a task factory, or None if the default one is in use.\"\"\"\n        return self._task_factory\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        \"\"\"Create socket transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            call_connection_made=True):\n        \"\"\"Create SSL transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        \"\"\"Create datagram transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        \"\"\"Create read pipe transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        \"\"\"Create write pipe transport.\"\"\"\n        raise NotImplementedError\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        \"\"\"Create subprocess transport.\"\"\"\n        raise NotImplementedError\n\n    def _write_to_self(self):\n        \"\"\"Write a byte to self-pipe, to wake up the event loop.\n\n        This may be called from a different thread.\n\n        The subclass is responsible for implementing the self-pipe.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_events(self, event_list):\n        \"\"\"Process selector events.\"\"\"\n        raise NotImplementedError\n\n    def _check_closed(self):\n        if self._closed:\n            raise RuntimeError('Event loop is closed')\n\n    def _check_default_executor(self):\n        if self._executor_shutdown_called:\n            raise RuntimeError('Executor shutdown has been called')\n\n    def _asyncgen_finalizer_hook(self, agen):\n        self._asyncgens.discard(agen)\n        if not self.is_closed():\n            self.call_soon_threadsafe(self.create_task, agen.aclose())\n\n    def _asyncgen_firstiter_hook(self, agen):\n        if self._asyncgens_shutdown_called:\n            warnings.warn(\n                f\"asynchronous generator {agen!r} was scheduled after \"\n                f\"loop.shutdown_asyncgens() call\",\n                ResourceWarning, source=self)\n\n        self._asyncgens.add(agen)\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        self._asyncgens_shutdown_called = True\n\n        if not len(self._asyncgens):\n            # If Python version is <3.6 or we don't have any asynchronous\n            # generators alive.\n            return\n\n        closing_agens = list(self._asyncgens)\n        self._asyncgens.clear()\n\n        results = await tasks.gather(\n            *[ag.aclose() for ag in closing_agens],\n            return_exceptions=True)\n\n        for result, agen in zip(results, closing_agens):\n            if isinstance(result, Exception):\n                self.call_exception_handler({\n                    'message': f'an error occurred during closing of '\n                               f'asynchronous generator {agen!r}',\n                    'exception': result,\n                    'asyncgen': agen\n                })\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        self._executor_shutdown_called = True\n        if self._default_executor is None:\n            return\n        future = self.create_future()\n        thread = threading.Thread(target=self._do_shutdown, args=(future,))\n        thread.start()\n        try:\n            await future\n        finally:\n            thread.join()\n\n    def _do_shutdown(self, future):\n        try:\n            self._default_executor.shutdown(wait=True)\n            if not self.is_closed():\n                self.call_soon_threadsafe(future.set_result, None)\n        except Exception as ex:\n            if not self.is_closed():\n                self.call_soon_threadsafe(future.set_exception, ex)\n\n    def _check_running(self):\n        if self.is_running():\n            raise RuntimeError('This event loop is already running')\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                'Cannot run the event loop while another loop is running')\n\n    def run_forever(self):\n        \"\"\"Run until stop() is called.\"\"\"\n        self._check_closed()\n        self._check_running()\n        self._set_coroutine_origin_tracking(self._debug)\n\n        old_agen_hooks = sys.get_asyncgen_hooks()\n        try:\n            self._thread_id = threading.get_ident()\n            sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n                                   finalizer=self._asyncgen_finalizer_hook)\n\n            events._set_running_loop(self)\n            while True:\n                self._run_once()\n                if self._stopping:\n                    break\n        finally:\n            self._stopping = False\n            self._thread_id = None\n            events._set_running_loop(None)\n            self._set_coroutine_origin_tracking(False)\n            sys.set_asyncgen_hooks(*old_agen_hooks)\n\n    def run_until_complete(self, future):\n        \"\"\"Run until the Future is done.\n\n        If the argument is a coroutine, it is wrapped in a Task.\n\n        WARNING: It would be disastrous to call run_until_complete()\n        with the same coroutine twice -- it would wrap it in two\n        different Tasks and that can't be good.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n\n        new_task = not futures.isfuture(future)\n        future = tasks.ensure_future(future, loop=self)\n        if new_task:\n            # An exception is raised if the future didn't complete, so there\n            # is no need to log the \"destroy pending task\" message\n            future._log_destroy_pending = False\n\n        future.add_done_callback(_run_until_complete_cb)\n        try:\n            self.run_forever()\n        except:\n            if new_task and future.done() and not future.cancelled():\n                # The coroutine raised a BaseException. Consume the exception\n                # to not log a warning, the caller doesn't have access to the\n                # local task.\n                future.exception()\n            raise\n        finally:\n            future.remove_done_callback(_run_until_complete_cb)\n        if not future.done():\n            raise RuntimeError('Event loop stopped before Future completed.')\n\n        return future.result()\n\n    def stop(self):\n        \"\"\"Stop running the event loop.\n\n        Every callback already scheduled will still run.  This simply informs\n        run_forever to stop looping after a complete iteration.\n        \"\"\"\n        self._stopping = True\n\n    def close(self):\n        \"\"\"Close the event loop.\n\n        This clears the queues and shuts down the executor,\n        but does not wait for the executor to finish.\n\n        The event loop must not be running.\n        \"\"\"\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self._closed:\n            return\n        if self._debug:\n            logger.debug(\"Close %r\", self)\n        self._closed = True\n        self._ready.clear()\n        self._scheduled.clear()\n        self._executor_shutdown_called = True\n        executor = self._default_executor\n        if executor is not None:\n            self._default_executor = None\n            executor.shutdown(wait=False)\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        return self._closed\n\n    def __del__(self, _warn=warnings.warn):\n        if not self.is_closed():\n            _warn(f\"unclosed event loop {self!r}\", ResourceWarning, source=self)\n            if not self.is_running():\n                self.close()\n\n    def is_running(self):\n        \"\"\"Returns True if the event loop is running.\"\"\"\n        return (self._thread_id is not None)\n\n    def time(self):\n        \"\"\"Return the time according to the event loop's clock.\n\n        This is a float expressed in seconds since an epoch, but the\n        epoch, precision, accuracy and drift are unspecified and may\n        differ per event loop.\n        \"\"\"\n        return time.monotonic()\n\n    def call_later(self, delay, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called at a given time.\n\n        Return a Handle: an opaque object with a cancel() method that\n        can be used to cancel the call.\n\n        The delay can be an int or float, expressed in seconds.  It is\n        always relative to the current time.\n\n        Each callback will be called exactly once.  If two callbacks\n        are scheduled for exactly the same time, it is undefined which\n        will be called first.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        if delay is None:\n            raise TypeError('delay must not be None')\n        timer = self.call_at(self.time() + delay, callback, *args,\n                             context=context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        return timer\n\n    def call_at(self, when, callback, *args, context=None):\n        \"\"\"Like call_later(), but uses an absolute time.\n\n        Absolute time corresponds to the event loop's time() method.\n        \"\"\"\n        if when is None:\n            raise TypeError(\"when cannot be None\")\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_at')\n        timer = events.TimerHandle(when, callback, args, self, context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        heapq.heappush(self._scheduled, timer)\n        timer._scheduled = True\n        return timer\n\n    def call_soon(self, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called as soon as possible.\n\n        This operates as a FIFO queue: callbacks are called in the\n        order in which they are registered.  Each callback will be\n        called exactly once.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_soon')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        return handle\n\n    def _check_callback(self, callback, method):\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\n                f\"coroutines cannot be used with {method}()\")\n        if not callable(callback):\n            raise TypeError(\n                f'a callable object was expected by {method}(), '\n                f'got {callback!r}')\n\n    def _call_soon(self, callback, args, context):\n        handle = events.Handle(callback, args, self, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._ready.append(handle)\n        return handle\n\n    def _check_thread(self):\n        \"\"\"Check that the current thread is the thread running the event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.\n        \"\"\"\n        if self._thread_id is None:\n            return\n        thread_id = threading.get_ident()\n        if thread_id != self._thread_id:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        \"\"\"Like call_soon(), but thread-safe.\"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_callback(callback, 'call_soon_threadsafe')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._write_to_self()\n        return handle\n\n    def run_in_executor(self, executor, func, *args):\n        self._check_closed()\n        if self._debug:\n            self._check_callback(func, 'run_in_executor')\n        if executor is None:\n            executor = self._default_executor\n            # Only check when the default executor is being used\n            self._check_default_executor()\n            if executor is None:\n                executor = concurrent.futures.ThreadPoolExecutor(\n                    thread_name_prefix='asyncio'\n                )\n                self._default_executor = executor\n        return futures.wrap_future(\n            executor.submit(func, *args), loop=self)\n\n    def set_default_executor(self, executor):\n        if not isinstance(executor, concurrent.futures.ThreadPoolExecutor):\n            raise TypeError('executor must be ThreadPoolExecutor instance')\n        self._default_executor = executor\n\n    def _getaddrinfo_debug(self, host, port, family, type, proto, flags):\n        msg = [f\"{host}:{port!r}\"]\n        if family:\n            msg.append(f'family={family!r}')\n        if type:\n            msg.append(f'type={type!r}')\n        if proto:\n            msg.append(f'proto={proto!r}')\n        if flags:\n            msg.append(f'flags={flags!r}')\n        msg = ', '.join(msg)\n        logger.debug('Get address info %s', msg)\n\n        t0 = self.time()\n        addrinfo = socket.getaddrinfo(host, port, family, type, proto, flags)\n        dt = self.time() - t0\n\n        msg = f'Getting address info {msg} took {dt * 1e3:.3f}ms: {addrinfo!r}'\n        if dt >= self.slow_callback_duration:\n            logger.info(msg)\n        else:\n            logger.debug(msg)\n        return addrinfo\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        if self._debug:\n            getaddr_func = self._getaddrinfo_debug\n        else:\n            getaddr_func = socket.getaddrinfo\n\n        return await self.run_in_executor(\n            None, getaddr_func, host, port, family, type, proto, flags)\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        return await self.run_in_executor(\n            None, socket.getnameinfo, sockaddr, flags)\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=True):\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        _check_ssl_socket(sock)\n        self._check_sendfile_params(sock, file, offset, count)\n        try:\n            return await self._sock_sendfile_native(sock, file,\n                                                    offset, count)\n        except exceptions.SendfileNotAvailableError as exc:\n            if not fallback:\n                raise\n        return await self._sock_sendfile_fallback(sock, file,\n                                                  offset, count)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        # NB: sendfile syscall is not supported for SSL sockets and\n        # non-mmap files even if sendfile is supported by OS\n        raise exceptions.SendfileNotAvailableError(\n            f\"syscall sendfile is not available for socket {sock!r} \"\n            f\"and file {file!r} combination\")\n\n    async def _sock_sendfile_fallback(self, sock, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = (\n            min(count, constants.SENDFILE_FALLBACK_READBUFFER_SIZE)\n            if count else constants.SENDFILE_FALLBACK_READBUFFER_SIZE\n        )\n        buf = bytearray(blocksize)\n        total_sent = 0\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    break  # EOF\n                await self.sock_sendall(sock, view[:read])\n                total_sent += read\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, sock, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not sock.type == socket.SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n        if not isinstance(offset, int):\n            raise TypeError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n        if offset < 0:\n            raise ValueError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n\n    async def _connect_sock(self, exceptions, addr_info, local_addr_infos=None):\n        \"\"\"Create, bind and connect one socket.\"\"\"\n        my_exceptions = []\n        exceptions.append(my_exceptions)\n        family, type_, proto, _, address = addr_info\n        sock = None\n        try:\n            sock = socket.socket(family=family, type=type_, proto=proto)\n            sock.setblocking(False)\n            if local_addr_infos is not None:\n                for lfamily, _, _, _, laddr in local_addr_infos:\n                    # skip local addresses of different family\n                    if lfamily != family:\n                        continue\n                    try:\n                        sock.bind(laddr)\n                        break\n                    except OSError as exc:\n                        msg = (\n                            f'error while attempting to bind on '\n                            f'address {laddr!r}: '\n                            f'{exc.strerror.lower()}'\n                        )\n                        exc = OSError(exc.errno, msg)\n                        my_exceptions.append(exc)\n                else:  # all bind attempts failed\n                    if my_exceptions:\n                        raise my_exceptions.pop()\n                    else:\n                        raise OSError(f\"no matching local address with {family=} found\")\n            await self.sock_connect(sock, address)\n            return sock\n        except OSError as exc:\n            my_exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n            raise\n        except:\n            if sock is not None:\n                sock.close()\n            raise\n        finally:\n            exceptions = my_exceptions = None\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0,\n            proto=0, flags=0, sock=None,\n            local_addr=None, server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        \"\"\"Connect to a TCP server.\n\n        Create a streaming transport connection to a given internet host and\n        port: socket family AF_INET or socket.AF_INET6 depending on host (or\n        family if specified), socket type SOCK_STREAM. protocol_factory must be\n        a callable returning a protocol instance.\n\n        This method is a coroutine which will try to establish the connection\n        in the background.  When successful, the coroutine returns a\n        (transport, protocol) pair.\n        \"\"\"\n        if server_hostname is not None and not ssl:\n            raise ValueError('server_hostname is only meaningful with ssl')\n\n        if server_hostname is None and ssl:\n            # Use host as default for server_hostname.  It is an error\n            # if host is empty or not set, e.g. when an\n            # already-connected socket was passed or when only a port\n            # is given.  To avoid this error, you can pass\n            # server_hostname='' -- this will bypass the hostname\n            # check.  (This also means that if host is a numeric\n            # IP/IPv6 address, we will attempt to verify that exact\n            # address; this will probably fail, but it is possible to\n            # create a certificate for a specific IP address, so we\n            # don't judge it here.)\n            if not host:\n                raise ValueError('You must set server_hostname '\n                                 'when using ssl without a host')\n            server_hostname = host\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if happy_eyeballs_delay is not None and interleave is None:\n            # If using happy eyeballs, default to interleave addresses by family\n            interleave = 1\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            infos = await self._ensure_resolved(\n                (host, port), family=family,\n                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)\n            if not infos:\n                raise OSError('getaddrinfo() returned empty list')\n\n            if local_addr is not None:\n                laddr_infos = await self._ensure_resolved(\n                    local_addr, family=family,\n                    type=socket.SOCK_STREAM, proto=proto,\n                    flags=flags, loop=self)\n                if not laddr_infos:\n                    raise OSError('getaddrinfo() returned empty list')\n            else:\n                laddr_infos = None\n\n            if interleave:\n                infos = _interleave_addrinfos(infos, interleave)\n\n            exceptions = []\n            if happy_eyeballs_delay is None:\n                # not using happy eyeballs\n                for addrinfo in infos:\n                    try:\n                        sock = await self._connect_sock(\n                            exceptions, addrinfo, laddr_infos)\n                        break\n                    except OSError:\n                        continue\n            else:  # using happy eyeballs\n                sock, _, _ = await staggered.staggered_race(\n                    (functools.partial(self._connect_sock,\n                                       exceptions, addrinfo, laddr_infos)\n                     for addrinfo in infos),\n                    happy_eyeballs_delay, loop=self)\n\n            if sock is None:\n                exceptions = [exc for sub in exceptions for exc in sub]\n                try:\n                    if len(exceptions) == 1:\n                        raise exceptions[0]\n                    else:\n                        # If they all have the same str(), raise one.\n                        model = str(exceptions[0])\n                        if all(str(exc) == model for exc in exceptions):\n                            raise exceptions[0]\n                        # Raise a combined exception so the user can see all\n                        # the various error messages.\n                        raise OSError('Multiple exceptions: {}'.format(\n                            ', '.join(str(exc) for exc in exceptions)))\n                finally:\n                    exceptions = None\n\n        else:\n            if sock is None:\n                raise ValueError(\n                    'host and port was not specified and no sock specified')\n            if sock.type != socket.SOCK_STREAM:\n                # We allow AF_INET, AF_INET6, AF_UNIX as long as they\n                # are SOCK_STREAM.\n                # We support passing AF_UNIX sockets even though we have\n                # a dedicated API for that: create_unix_connection.\n                # Disallowing AF_UNIX in this method, breaks backwards\n                # compatibility.\n                raise ValueError(\n                    f'A Stream Socket was expected, got {sock!r}')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r connected to %s:%r: (%r, %r)\",\n                         sock, host, port, transport, protocol)\n        return transport, protocol\n\n    async def _create_connection_transport(\n            self, sock, protocol_factory, ssl,\n            server_hostname, server_side=False,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n\n        sock.setblocking(False)\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        if ssl:\n            sslcontext = None if isinstance(ssl, bool) else ssl\n            transport = self._make_ssl_transport(\n                sock, protocol, sslcontext, waiter,\n                server_side=server_side, server_hostname=server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout,\n                ssl_shutdown_timeout=ssl_shutdown_timeout)\n        else:\n            transport = self._make_socket_transport(sock, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file to transport.\n\n        Return the total number of bytes which were sent.\n\n        The method uses high-performance os.sendfile if available.\n\n        file must be a regular file object opened in binary mode.\n\n        offset tells from where to start reading the file. If specified,\n        count is the total number of bytes to transmit as opposed to\n        sending the file until EOF is reached. File position is updated on\n        return or also in case of error in which case file.tell()\n        can be used to figure out the number of bytes\n        which were sent.\n\n        fallback set to True makes asyncio to manually read and send\n        the file when the platform does not support the sendfile syscall\n        (e.g. Windows or SSL socket on Unix).\n\n        Raise SendfileNotAvailableError if the system does not support\n        sendfile syscall and fallback is False.\n        \"\"\"\n        if transport.is_closing():\n            raise RuntimeError(\"Transport is closing\")\n        mode = getattr(transport, '_sendfile_compatible',\n                       constants._SendfileMode.UNSUPPORTED)\n        if mode is constants._SendfileMode.UNSUPPORTED:\n            raise RuntimeError(\n                f\"sendfile is not supported for transport {transport!r}\")\n        if mode is constants._SendfileMode.TRY_NATIVE:\n            try:\n                return await self._sendfile_native(transport, file,\n                                                   offset, count)\n            except exceptions.SendfileNotAvailableError as exc:\n                if not fallback:\n                    raise\n\n        if not fallback:\n            raise RuntimeError(\n                f\"fallback is disabled and native sendfile is not \"\n                f\"supported for transport {transport!r}\")\n\n        return await self._sendfile_fallback(transport, file,\n                                             offset, count)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        raise exceptions.SendfileNotAvailableError(\n            \"sendfile syscall is not supported\")\n\n    async def _sendfile_fallback(self, transp, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 16384) if count else 16384\n        buf = bytearray(blocksize)\n        total_sent = 0\n        proto = _SendfileFallbackProtocol(transp)\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        return total_sent\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    return total_sent  # EOF\n                await proto.drain()\n                transp.write(view[:read])\n                total_sent += read\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n            await proto.restore()\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None,\n                        ssl_shutdown_timeout=None):\n        \"\"\"Upgrade transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        if ssl is None:\n            raise RuntimeError('Python ssl module is not available')\n\n        if not isinstance(sslcontext, ssl.SSLContext):\n            raise TypeError(\n                f'sslcontext is expected to be an instance of ssl.SSLContext, '\n                f'got {sslcontext!r}')\n\n        if not getattr(transport, '_start_tls_compatible', False):\n            raise TypeError(\n                f'transport {transport!r} is not supported by start_tls()')\n\n        waiter = self.create_future()\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout,\n            call_connection_made=False)\n\n        # Pause early so that \"ssl_protocol.data_received()\" doesn't\n        # have a chance to get called before \"ssl_protocol.connection_made()\".\n        transport.pause_reading()\n\n        transport.set_protocol(ssl_protocol)\n        conmade_cb = self.call_soon(ssl_protocol.connection_made, transport)\n        resume_cb = self.call_soon(transport.resume_reading)\n\n        try:\n            await waiter\n        except BaseException:\n            transport.close()\n            conmade_cb.cancel()\n            resume_cb.cancel()\n            raise\n\n        return ssl_protocol._app_transport\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"Create datagram connection.\"\"\"\n        if sock is not None:\n            if sock.type != socket.SOCK_DGRAM:\n                raise ValueError(\n                    f'A UDP Socket was expected, got {sock!r}')\n            if (local_addr or remote_addr or\n                    family or proto or flags or\n                    reuse_port or allow_broadcast):\n                # show the problematic kwargs in exception msg\n                opts = dict(local_addr=local_addr, remote_addr=remote_addr,\n                            family=family, proto=proto, flags=flags,\n                            reuse_port=reuse_port,\n                            allow_broadcast=allow_broadcast)\n                problems = ', '.join(f'{k}={v}' for k, v in opts.items() if v)\n                raise ValueError(\n                    f'socket modifier keyword arguments can not be used '\n                    f'when sock is specified. ({problems})')\n            sock.setblocking(False)\n            r_addr = None\n        else:\n            if not (local_addr or remote_addr):\n                if family == 0:\n                    raise ValueError('unexpected address family')\n                addr_pairs_info = (((family, proto), (None, None)),)\n            elif hasattr(socket, 'AF_UNIX') and family == socket.AF_UNIX:\n                for addr in (local_addr, remote_addr):\n                    if addr is not None and not isinstance(addr, str):\n                        raise TypeError('string is expected')\n\n                if local_addr and local_addr[0] not in (0, '\\x00'):\n                    try:\n                        if stat.S_ISSOCK(os.stat(local_addr).st_mode):\n                            os.remove(local_addr)\n                    except FileNotFoundError:\n                        pass\n                    except OSError as err:\n                        # Directory may have permissions only to create socket.\n                        logger.error('Unable to check or remove stale UNIX '\n                                     'socket %r: %r',\n                                     local_addr, err)\n\n                addr_pairs_info = (((family, proto),\n                                    (local_addr, remote_addr)), )\n            else:\n                # join address by (family, protocol)\n                addr_infos = {}  # Using order preserving dict\n                for idx, addr in ((0, local_addr), (1, remote_addr)):\n                    if addr is not None:\n                        if not (isinstance(addr, tuple) and len(addr) == 2):\n                            raise TypeError('2-tuple is expected')\n\n                        infos = await self._ensure_resolved(\n                            addr, family=family, type=socket.SOCK_DGRAM,\n                            proto=proto, flags=flags, loop=self)\n                        if not infos:\n                            raise OSError('getaddrinfo() returned empty list')\n\n                        for fam, _, pro, _, address in infos:\n                            key = (fam, pro)\n                            if key not in addr_infos:\n                                addr_infos[key] = [None, None]\n                            addr_infos[key][idx] = address\n\n                # each addr has to have info for each (family, proto) pair\n                addr_pairs_info = [\n                    (key, addr_pair) for key, addr_pair in addr_infos.items()\n                    if not ((local_addr and addr_pair[0] is None) or\n                            (remote_addr and addr_pair[1] is None))]\n\n                if not addr_pairs_info:\n                    raise ValueError('can not get address information')\n\n            exceptions = []\n\n            for ((family, proto),\n                 (local_address, remote_address)) in addr_pairs_info:\n                sock = None\n                r_addr = None\n                try:\n                    sock = socket.socket(\n                        family=family, type=socket.SOCK_DGRAM, proto=proto)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if allow_broadcast:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n                    sock.setblocking(False)\n\n                    if local_addr:\n                        sock.bind(local_address)\n                    if remote_addr:\n                        if not allow_broadcast:\n                            await self.sock_connect(sock, remote_address)\n                        r_addr = remote_address\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                raise exceptions[0]\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_datagram_transport(\n            sock, protocol, r_addr, waiter)\n        if self._debug:\n            if local_addr:\n                logger.info(\"Datagram endpoint local_addr=%r remote_addr=%r \"\n                            \"created: (%r, %r)\",\n                            local_addr, remote_addr, transport, protocol)\n            else:\n                logger.debug(\"Datagram endpoint remote_addr=%r created: \"\n                             \"(%r, %r)\",\n                             remote_addr, transport, protocol)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def _ensure_resolved(self, address, *,\n                               family=0, type=socket.SOCK_STREAM,\n                               proto=0, flags=0, loop):\n        host, port = address[:2]\n        info = _ipaddr_info(host, port, family, type, proto, *address[2:])\n        if info is not None:\n            # \"host\" is already a resolved IP.\n            return [info]\n        else:\n            return await loop.getaddrinfo(host, port, family=family, type=type,\n                                          proto=proto, flags=flags)\n\n    async def _create_server_getaddrinfo(self, host, port, family, flags):\n        infos = await self._ensure_resolved((host, port), family=family,\n                                            type=socket.SOCK_STREAM,\n                                            flags=flags, loop=self)\n        if not infos:\n            raise OSError(f'getaddrinfo({host!r}) returned empty list')\n        return infos\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *,\n            family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE,\n            sock=None,\n            backlog=100,\n            ssl=None,\n            reuse_address=None,\n            reuse_port=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"Create a TCP server.\n\n        The host parameter can be a string, in that case the TCP server is\n        bound to host and port.\n\n        The host parameter can also be a sequence of strings and in that case\n        the TCP server is bound to all hosts of the sequence. If a host\n        appears multiple times (possibly indirectly e.g. when hostnames\n        resolve to the same IP address), the server is only bound once to that\n        host.\n\n        Return a Server object which can be used to stop the service.\n\n        This method is a coroutine.\n        \"\"\"\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            if reuse_address is None:\n                reuse_address = os.name == \"posix\" and sys.platform != \"cygwin\"\n            sockets = []\n            if host == '':\n                hosts = [None]\n            elif (isinstance(host, str) or\n                  not isinstance(host, collections.abc.Iterable)):\n                hosts = [host]\n            else:\n                hosts = host\n\n            fs = [self._create_server_getaddrinfo(host, port, family=family,\n                                                  flags=flags)\n                  for host in hosts]\n            infos = await tasks.gather(*fs)\n            infos = set(itertools.chain.from_iterable(infos))\n\n            completed = False\n            try:\n                for res in infos:\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                    except socket.error:\n                        # Assume it's a bad family/type/protocol combination.\n                        if self._debug:\n                            logger.warning('create_server() failed to create '\n                                           'socket.socket(%r, %r, %r)',\n                                           af, socktype, proto, exc_info=True)\n                        continue\n                    sockets.append(sock)\n                    if reuse_address:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    # Disable IPv4/IPv6 dual stack support (enabled by\n                    # default on Linux) which makes a single socket\n                    # listen on both address families.\n                    if (_HAS_IPv6 and\n                            af == socket.AF_INET6 and\n                            hasattr(socket, 'IPPROTO_IPV6')):\n                        sock.setsockopt(socket.IPPROTO_IPV6,\n                                        socket.IPV6_V6ONLY,\n                                        True)\n                    try:\n                        sock.bind(sa)\n                    except OSError as err:\n                        raise OSError(err.errno, 'error while attempting '\n                                      'to bind on address %r: %s'\n                                      % (sa, err.strerror.lower())) from None\n                completed = True\n            finally:\n                if not completed:\n                    for sock in sockets:\n                        sock.close()\n        else:\n            if sock is None:\n                raise ValueError('Neither host/port nor sock were specified')\n            if sock.type != socket.SOCK_STREAM:\n                raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n            sockets = [sock]\n\n        for sock in sockets:\n            sock.setblocking(False)\n\n        server = Server(self, sockets, protocol_factory,\n                        ssl, backlog, ssl_handshake_timeout,\n                        ssl_shutdown_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        if self._debug:\n            logger.info(\"%r is serving\", server)\n        return server\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        if sock.type != socket.SOCK_STREAM:\n            raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, '', server_side=True,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r handled: (%r, %r)\", sock, transport, protocol)\n        return transport, protocol\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_read_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Read pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_write_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Write pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    def _log_subprocess(self, msg, stdin, stdout, stderr):\n        info = [msg]\n        if stdin is not None:\n            info.append(f'stdin={_format_pipe(stdin)}')\n        if stdout is not None and stderr == subprocess.STDOUT:\n            info.append(f'stdout=stderr={_format_pipe(stdout)}')\n        else:\n            if stdout is not None:\n                info.append(f'stdout={_format_pipe(stdout)}')\n            if stderr is not None:\n                info.append(f'stderr={_format_pipe(stderr)}')\n        logger.debug(' '.join(info))\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               universal_newlines=False,\n                               shell=True, bufsize=0,\n                               encoding=None, errors=None, text=None,\n                               **kwargs):\n        if not isinstance(cmd, (bytes, str)):\n            raise ValueError(\"cmd must be a string\")\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if not shell:\n            raise ValueError(\"shell must be True\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = 'run shell command %r' % cmd\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, cmd, True, stdin, stdout, stderr, bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    async def subprocess_exec(self, protocol_factory, program, *args,\n                              stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE, universal_newlines=False,\n                              shell=False, bufsize=0,\n                              encoding=None, errors=None, text=None,\n                              **kwargs):\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if shell:\n            raise ValueError(\"shell must be False\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        popen_args = (program,) + args\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = f'execute program {program!r}'\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, popen_args, False, stdin, stdout, stderr,\n            bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    def get_exception_handler(self):\n        \"\"\"Return an exception handler, or None if the default one is in use.\n        \"\"\"\n        return self._exception_handler\n\n    def set_exception_handler(self, handler):\n        \"\"\"Set handler as the new event loop exception handler.\n\n        If handler is None, the default exception handler will\n        be set.\n\n        If handler is a callable object, it should have a\n        signature matching '(loop, context)', where 'loop'\n        will be a reference to the active event loop, 'context'\n        will be a dict object (see `call_exception_handler()`\n        documentation for details about context).\n        \"\"\"\n        if handler is not None and not callable(handler):\n            raise TypeError(f'A callable object or None is expected, '\n                            f'got {handler!r}')\n        self._exception_handler = handler\n\n    def default_exception_handler(self, context):\n        \"\"\"Default exception handler.\n\n        This is called when an exception occurs and no exception\n        handler is set, and can be called by a custom exception\n        handler that wants to defer to the default behavior.\n\n        This default handler logs the error message and other\n        context-dependent information.  In debug mode, a truncated\n        stack trace is also appended showing where the given object\n        (e.g. a handle or future or task) was created, if any.\n\n        The context parameter has the same meaning as in\n        `call_exception_handler()`.\n        \"\"\"\n        message = context.get('message')\n        if not message:\n            message = 'Unhandled exception in event loop'\n\n        exception = context.get('exception')\n        if exception is not None:\n            exc_info = (type(exception), exception, exception.__traceback__)\n        else:\n            exc_info = False\n\n        if ('source_traceback' not in context and\n                self._current_handle is not None and\n                self._current_handle._source_traceback):\n            context['handle_traceback'] = \\\n                self._current_handle._source_traceback\n\n        log_lines = [message]\n        for key in sorted(context):\n            if key in {'message', 'exception'}:\n                continue\n            value = context[key]\n            if key == 'source_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Object created at (most recent call last):\\n'\n                value += tb.rstrip()\n            elif key == 'handle_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Handle created at (most recent call last):\\n'\n                value += tb.rstrip()\n            else:\n                value = repr(value)\n            log_lines.append(f'{key}: {value}')\n\n        logger.error('\\n'.join(log_lines), exc_info=exc_info)\n\n    def call_exception_handler(self, context):\n        \"\"\"Call the current event loop's exception handler.\n\n        The context argument is a dict containing the following keys:\n\n        - 'message': Error message;\n        - 'exception' (optional): Exception object;\n        - 'future' (optional): Future instance;\n        - 'task' (optional): Task instance;\n        - 'handle' (optional): Handle instance;\n        - 'protocol' (optional): Protocol instance;\n        - 'transport' (optional): Transport instance;\n        - 'socket' (optional): Socket instance;\n        - 'asyncgen' (optional): Asynchronous generator that caused\n                                 the exception.\n\n        New keys maybe introduced in the future.\n\n        Note: do not overload this method in an event loop subclass.\n        For custom exception handling, use the\n        `set_exception_handler()` method.\n        \"\"\"\n        if self._exception_handler is None:\n            try:\n                self.default_exception_handler(context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                # Second protection layer for unexpected errors\n                # in the default implementation, as well as for subclassed\n                # event loops with overloaded \"default_exception_handler\".\n                logger.error('Exception in default exception handler',\n                             exc_info=True)\n        else:\n            try:\n                self._exception_handler(self, context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                # Exception in the user set custom exception handler.\n                try:\n                    # Let's try default handler.\n                    self.default_exception_handler({\n                        'message': 'Unhandled error in exception handler',\n                        'exception': exc,\n                        'context': context,\n                    })\n                except (SystemExit, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    # Guard 'default_exception_handler' in case it is\n                    # overloaded.\n                    logger.error('Exception in default exception handler '\n                                 'while handling an unexpected error '\n                                 'in custom exception handler',\n                                 exc_info=True)\n\n    def _add_callback(self, handle):\n        \"\"\"Add a Handle to _ready.\"\"\"\n        if not handle._cancelled:\n            self._ready.append(handle)\n\n    def _add_callback_signalsafe(self, handle):\n        \"\"\"Like _add_callback() but called from a signal handler.\"\"\"\n        self._add_callback(handle)\n        self._write_to_self()\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        if handle._scheduled:\n            self._timer_cancelled_count += 1\n\n    def _run_once(self):\n        \"\"\"Run one full iteration of the event loop.\n\n        This calls all currently ready callbacks, polls for I/O,\n        schedules the resulting callbacks, and finally schedules\n        'call_later' callbacks.\n        \"\"\"\n\n        sched_count = len(self._scheduled)\n        if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and\n            self._timer_cancelled_count / sched_count >\n                _MIN_CANCELLED_TIMER_HANDLES_FRACTION):\n            # Remove delayed calls that were cancelled if their number\n            # is too high\n            new_scheduled = []\n            for handle in self._scheduled:\n                if handle._cancelled:\n                    handle._scheduled = False\n                else:\n                    new_scheduled.append(handle)\n\n            heapq.heapify(new_scheduled)\n            self._scheduled = new_scheduled\n            self._timer_cancelled_count = 0\n        else:\n            # Remove delayed calls that were cancelled from head of queue.\n            while self._scheduled and self._scheduled[0]._cancelled:\n                self._timer_cancelled_count -= 1\n                handle = heapq.heappop(self._scheduled)\n                handle._scheduled = False\n\n        timeout = None\n        if self._ready or self._stopping:\n            timeout = 0\n        elif self._scheduled:\n            # Compute the desired timeout.\n            when = self._scheduled[0]._when\n            timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT)\n\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n        # Needed to break cycles when an exception occurs.\n        event_list = None\n\n        # Handle 'later' callbacks that are ready.\n        end_time = self.time() + self._clock_resolution\n        while self._scheduled:\n            handle = self._scheduled[0]\n            if handle._when >= end_time:\n                break\n            handle = heapq.heappop(self._scheduled)\n            handle._scheduled = False\n            self._ready.append(handle)\n\n        # This is the only place where callbacks are actually *called*.\n        # All other places just add them to ready.\n        # Note: We run all currently scheduled callbacks, but not any\n        # callbacks scheduled by callbacks run this time around --\n        # they will be run the next time (after another I/O poll).\n        # Use an idiom that is thread-safe without using locks.\n        ntodo = len(self._ready)\n        for i in range(ntodo):\n            handle = self._ready.popleft()\n            if handle._cancelled:\n                continue\n            if self._debug:\n                try:\n                    self._current_handle = handle\n                    t0 = self.time()\n                    handle._run()\n                    dt = self.time() - t0\n                    if dt >= self.slow_callback_duration:\n                        logger.warning('Executing %s took %.3f seconds',\n                                       _format_handle(handle), dt)\n                finally:\n                    self._current_handle = None\n            else:\n                handle._run()\n        handle = None  # Needed to break cycles when an exception occurs.\n\n    def _set_coroutine_origin_tracking(self, enabled):\n        if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n            return\n\n        if enabled:\n            self._coroutine_origin_tracking_saved_depth = (\n                sys.get_coroutine_origin_tracking_depth())\n            sys.set_coroutine_origin_tracking_depth(\n                constants.DEBUG_STACK_DEPTH)\n        else:\n            sys.set_coroutine_origin_tracking_depth(\n                self._coroutine_origin_tracking_saved_depth)\n\n        self._coroutine_origin_tracking_enabled = enabled\n\n    def get_debug(self):\n        return self._debug\n\n    def set_debug(self, enabled):\n        self._debug = enabled\n\n        if self.is_running():\n            self.call_soon_threadsafe(self._set_coroutine_origin_tracking, enabled)\n", 1947], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py": ["# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\nfrom types import GenericAlias\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard:\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet:\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        discard = self.data.discard\n        while True:\n            try:\n                item = pop()\n            except IndexError:\n                return\n            discard(item)\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return self.__class__, (list(self),), self.__getstate__()\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet') from None\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(map(ref, other))\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(map(ref, other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(map(ref, other))\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\n    def __repr__(self):\n        return repr(self.data)\n\n    __class_getitem__ = classmethod(GenericAlias)\n", 205], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py": ["\"\"\"Selectors module.\n\nThis module allows high-level and efficient I/O multiplexing, built upon the\n`select` module primitives.\n\"\"\"\n\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import namedtuple\nfrom collections.abc import Mapping\nimport math\nimport select\nimport sys\n\n\n# generic events, that must be mapped to implementation-specific ones\nEVENT_READ = (1 << 0)\nEVENT_WRITE = (1 << 1)\n\n\ndef _fileobj_to_fd(fileobj):\n    \"\"\"Return a file descriptor from a file object.\n\n    Parameters:\n    fileobj -- file object or file descriptor\n\n    Returns:\n    corresponding file descriptor\n\n    Raises:\n    ValueError if the object is invalid\n    \"\"\"\n    if isinstance(fileobj, int):\n        fd = fileobj\n    else:\n        try:\n            fd = int(fileobj.fileno())\n        except (AttributeError, TypeError, ValueError):\n            raise ValueError(\"Invalid file object: \"\n                             \"{!r}\".format(fileobj)) from None\n    if fd < 0:\n        raise ValueError(\"Invalid file descriptor: {}\".format(fd))\n    return fd\n\n\nSelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])\n\nSelectorKey.__doc__ = \"\"\"SelectorKey(fileobj, fd, events, data)\n\n    Object used to associate a file object to its backing\n    file descriptor, selected event mask, and attached data.\n\"\"\"\nSelectorKey.fileobj.__doc__ = 'File object registered.'\nSelectorKey.fd.__doc__ = 'Underlying file descriptor.'\nSelectorKey.events.__doc__ = 'Events that must be waited for on this file object.'\nSelectorKey.data.__doc__ = ('''Optional opaque data associated to this file object.\nFor example, this could be used to store a per-client session ID.''')\n\n\nclass _SelectorMapping(Mapping):\n    \"\"\"Mapping of file objects to selector keys.\"\"\"\n\n    def __init__(self, selector):\n        self._selector = selector\n\n    def __len__(self):\n        return len(self._selector._fd_to_key)\n\n    def __getitem__(self, fileobj):\n        try:\n            fd = self._selector._fileobj_lookup(fileobj)\n            return self._selector._fd_to_key[fd]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    def __iter__(self):\n        return iter(self._selector._fd_to_key)\n\n\nclass BaseSelector(metaclass=ABCMeta):\n    \"\"\"Selector abstract base class.\n\n    A selector supports registering file objects to be monitored for specific\n    I/O events.\n\n    A file object is a file descriptor or any object with a `fileno()` method.\n    An arbitrary object can be attached to the file object, which can be used\n    for example to store context information, a callback, etc.\n\n    A selector can use various implementations (select(), poll(), epoll()...)\n    depending on the platform. The default `Selector` class uses the most\n    efficient implementation on the current platform.\n    \"\"\"\n\n    @abstractmethod\n    def register(self, fileobj, events, data=None):\n        \"\"\"Register a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        ValueError if events is invalid\n        KeyError if fileobj is already registered\n        OSError if fileobj is closed or otherwise is unacceptable to\n                the underlying system call (if a system call is made)\n\n        Note:\n        OSError may or may not be raised\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def unregister(self, fileobj):\n        \"\"\"Unregister a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        KeyError if fileobj is not registered\n\n        Note:\n        If fileobj is registered but has since been closed this does\n        *not* raise OSError (even if the wrapped syscall does)\n        \"\"\"\n        raise NotImplementedError\n\n    def modify(self, fileobj, events, data=None):\n        \"\"\"Change a registered file object monitored events or attached data.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        Anything that unregister() or register() raises\n        \"\"\"\n        self.unregister(fileobj)\n        return self.register(fileobj, events, data)\n\n    @abstractmethod\n    def select(self, timeout=None):\n        \"\"\"Perform the actual selection, until some monitored file objects are\n        ready or a timeout expires.\n\n        Parameters:\n        timeout -- if timeout > 0, this specifies the maximum wait time, in\n                   seconds\n                   if timeout <= 0, the select() call won't block, and will\n                   report the currently ready file objects\n                   if timeout is None, select() will block until a monitored\n                   file object becomes ready\n\n        Returns:\n        list of (key, events) for ready file objects\n        `events` is a bitwise mask of EVENT_READ|EVENT_WRITE\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the selector.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        pass\n\n    def get_key(self, fileobj):\n        \"\"\"Return the key associated to a registered file object.\n\n        Returns:\n        SelectorKey for this file object\n        \"\"\"\n        mapping = self.get_map()\n        if mapping is None:\n            raise RuntimeError('Selector is closed')\n        try:\n            return mapping[fileobj]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    @abstractmethod\n    def get_map(self):\n        \"\"\"Return a mapping of file objects to selector keys.\"\"\"\n        raise NotImplementedError\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n\nclass _BaseSelectorImpl(BaseSelector):\n    \"\"\"Base selector implementation.\"\"\"\n\n    def __init__(self):\n        # this maps file descriptors to keys\n        self._fd_to_key = {}\n        # read-only mapping returned by get_map()\n        self._map = _SelectorMapping(self)\n\n    def _fileobj_lookup(self, fileobj):\n        \"\"\"Return a file descriptor from a file object.\n\n        This wraps _fileobj_to_fd() to do an exhaustive search in case\n        the object is invalid but we still have it in our map.  This\n        is used by unregister() so we can unregister an object that\n        was previously registered even if it is closed.  It is also\n        used by _SelectorMapping.\n        \"\"\"\n        try:\n            return _fileobj_to_fd(fileobj)\n        except ValueError:\n            # Do an exhaustive search.\n            for key in self._fd_to_key.values():\n                if key.fileobj is fileobj:\n                    return key.fd\n            # Raise ValueError after all.\n            raise\n\n    def register(self, fileobj, events, data=None):\n        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):\n            raise ValueError(\"Invalid events: {!r}\".format(events))\n\n        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)\n\n        if key.fd in self._fd_to_key:\n            raise KeyError(\"{!r} (FD {}) is already registered\"\n                           .format(fileobj, key.fd))\n\n        self._fd_to_key[key.fd] = key\n        return key\n\n    def unregister(self, fileobj):\n        try:\n            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        if events != key.events:\n            self.unregister(fileobj)\n            key = self.register(fileobj, events, data)\n        elif data != key.data:\n            # Use a shortcut to update the data.\n            key = key._replace(data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def close(self):\n        self._fd_to_key.clear()\n        self._map = None\n\n    def get_map(self):\n        return self._map\n\n    def _key_from_fd(self, fd):\n        \"\"\"Return the key associated to a given file descriptor.\n\n        Parameters:\n        fd -- file descriptor\n\n        Returns:\n        corresponding key, or None if not found\n        \"\"\"\n        try:\n            return self._fd_to_key[fd]\n        except KeyError:\n            return None\n\n\nclass SelectSelector(_BaseSelectorImpl):\n    \"\"\"Select-based selector.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._readers = set()\n        self._writers = set()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        if events & EVENT_READ:\n            self._readers.add(key.fd)\n        if events & EVENT_WRITE:\n            self._writers.add(key.fd)\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        self._readers.discard(key.fd)\n        self._writers.discard(key.fd)\n        return key\n\n    if sys.platform == 'win32':\n        def _select(self, r, w, _, timeout=None):\n            r, w, x = select.select(r, w, w, timeout)\n            return r, w + x, []\n    else:\n        _select = select.select\n\n    def select(self, timeout=None):\n        timeout = None if timeout is None else max(timeout, 0)\n        ready = []\n        try:\n            r, w, _ = self._select(self._readers, self._writers, [], timeout)\n        except InterruptedError:\n            return ready\n        r = set(r)\n        w = set(w)\n        for fd in r | w:\n            events = 0\n            if fd in r:\n                events |= EVENT_READ\n            if fd in w:\n                events |= EVENT_WRITE\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nclass _PollLikeSelector(_BaseSelectorImpl):\n    \"\"\"Base class shared between poll, epoll and devpoll selectors.\"\"\"\n    _selector_cls = None\n    _EVENT_READ = None\n    _EVENT_WRITE = None\n\n    def __init__(self):\n        super().__init__()\n        self._selector = self._selector_cls()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        poller_events = 0\n        if events & EVENT_READ:\n            poller_events |= self._EVENT_READ\n        if events & EVENT_WRITE:\n            poller_events |= self._EVENT_WRITE\n        try:\n            self._selector.register(key.fd, poller_events)\n        except:\n            super().unregister(fileobj)\n            raise\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        try:\n            self._selector.unregister(key.fd)\n        except OSError:\n            # This can happen if the FD was closed since it\n            # was registered.\n            pass\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(f\"{fileobj!r} is not registered\") from None\n\n        changed = False\n        if events != key.events:\n            selector_events = 0\n            if events & EVENT_READ:\n                selector_events |= self._EVENT_READ\n            if events & EVENT_WRITE:\n                selector_events |= self._EVENT_WRITE\n            try:\n                self._selector.modify(key.fd, selector_events)\n            except:\n                super().unregister(fileobj)\n                raise\n            changed = True\n        if data != key.data:\n            changed = True\n\n        if changed:\n            key = key._replace(events=events, data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def select(self, timeout=None):\n        # This is shared between poll() and epoll().\n        # epoll() has a different signature and handling of timeout parameter.\n        if timeout is None:\n            timeout = None\n        elif timeout <= 0:\n            timeout = 0\n        else:\n            # poll() has a resolution of 1 millisecond, round away from\n            # zero to wait *at least* timeout seconds.\n            timeout = math.ceil(timeout * 1e3)\n        ready = []\n        try:\n            fd_event_list = self._selector.poll(timeout)\n        except InterruptedError:\n            return ready\n        for fd, event in fd_event_list:\n            events = 0\n            if event & ~self._EVENT_READ:\n                events |= EVENT_WRITE\n            if event & ~self._EVENT_WRITE:\n                events |= EVENT_READ\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nif hasattr(select, 'poll'):\n\n    class PollSelector(_PollLikeSelector):\n        \"\"\"Poll-based selector.\"\"\"\n        _selector_cls = select.poll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n\nif hasattr(select, 'epoll'):\n\n    class EpollSelector(_PollLikeSelector):\n        \"\"\"Epoll-based selector.\"\"\"\n        _selector_cls = select.epoll\n        _EVENT_READ = select.EPOLLIN\n        _EVENT_WRITE = select.EPOLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def select(self, timeout=None):\n            if timeout is None:\n                timeout = -1\n            elif timeout <= 0:\n                timeout = 0\n            else:\n                # epoll_wait() has a resolution of 1 millisecond, round away\n                # from zero to wait *at least* timeout seconds.\n                timeout = math.ceil(timeout * 1e3) * 1e-3\n\n            # epoll_wait() expects `maxevents` to be greater than zero;\n            # we want to make sure that `select()` can be called when no\n            # FD is registered.\n            max_ev = max(len(self._fd_to_key), 1)\n\n            ready = []\n            try:\n                fd_event_list = self._selector.poll(timeout, max_ev)\n            except InterruptedError:\n                return ready\n            for fd, event in fd_event_list:\n                events = 0\n                if event & ~select.EPOLLIN:\n                    events |= EVENT_WRITE\n                if event & ~select.EPOLLOUT:\n                    events |= EVENT_READ\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'devpoll'):\n\n    class DevpollSelector(_PollLikeSelector):\n        \"\"\"Solaris /dev/poll selector.\"\"\"\n        _selector_cls = select.devpoll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'kqueue'):\n\n    class KqueueSelector(_BaseSelectorImpl):\n        \"\"\"Kqueue-based selector.\"\"\"\n\n        def __init__(self):\n            super().__init__()\n            self._selector = select.kqueue()\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def register(self, fileobj, events, data=None):\n            key = super().register(fileobj, events, data)\n            try:\n                if events & EVENT_READ:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                if events & EVENT_WRITE:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n            except:\n                super().unregister(fileobj)\n                raise\n            return key\n\n        def unregister(self, fileobj):\n            key = super().unregister(fileobj)\n            if key.events & EVENT_READ:\n                kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # This can happen if the FD was closed since it\n                    # was registered.\n                    pass\n            if key.events & EVENT_WRITE:\n                kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # See comment above.\n                    pass\n            return key\n\n        def select(self, timeout=None):\n            timeout = None if timeout is None else max(timeout, 0)\n            # If max_ev is 0, kqueue will ignore the timeout. For consistent\n            # behavior with the other selector classes, we prevent that here\n            # (using max). See https://bugs.python.org/issue29255\n            max_ev = max(len(self._fd_to_key), 1)\n            ready = []\n            try:\n                kev_list = self._selector.control(None, max_ev, timeout)\n            except InterruptedError:\n                return ready\n            for kev in kev_list:\n                fd = kev.ident\n                flag = kev.filter\n                events = 0\n                if flag == select.KQ_FILTER_READ:\n                    events |= EVENT_READ\n                if flag == select.KQ_FILTER_WRITE:\n                    events |= EVENT_WRITE\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\ndef _can_use(method):\n    \"\"\"Check if we can use the selector depending upon the\n    operating system. \"\"\"\n    # Implementation based upon https://github.com/sethmlarson/selectors2/blob/master/selectors2.py\n    selector = getattr(select, method, None)\n    if selector is None:\n        # select module does not implement method\n        return False\n    # check if the OS and Kernel actually support the method. Call may fail with\n    # OSError: [Errno 38] Function not implemented\n    try:\n        selector_obj = selector()\n        if method == 'poll':\n            # check that poll actually works\n            selector_obj.poll(0)\n        else:\n            # close epoll, kqueue, and devpoll fd\n            selector_obj.close()\n        return True\n    except OSError:\n        return False\n\n\n# Choose the best implementation, roughly:\n#    epoll|kqueue|devpoll > poll > select.\n# select() also can't accept a FD > FD_SETSIZE (usually around 1024)\nif _can_use('kqueue'):\n    DefaultSelector = KqueueSelector\nelif _can_use('epoll'):\n    DefaultSelector = EpollSelector\nelif _can_use('devpoll'):\n    DefaultSelector = DevpollSelector\nelif _can_use('poll'):\n    DefaultSelector = PollSelector\nelse:\n    DefaultSelector = SelectSelector\n", 618], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py": ["# Copyright 2001-2019 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2019 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom types import GenericAlias\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions', 'getLevelNamesMapping']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelNamesMapping():\n    return _nameToLevel.copy()\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual or numeric representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    If a string representation of the level is passed in, the corresponding\n    numeric value is returned.\n\n    If no matching numeric or string value is passed in, the string\n    'Level %s' % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\nif hasattr(sys, \"_getframe\"):\n    currentframe = lambda: sys._getframe(1)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception:\n            return sys.exc_info()[2].tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# Setting _srcfile to None will prevent findCaller() from being called. This\n# way, you can avoid the overhead of fetching caller information.\n\n# The following is based on warnings._is_internal_frame. It makes sure that\n# frames of the import mechanism are skipped when logging at module level and\n# using a stacklevel value greater than one.\ndef _is_internal_frame(frame):\n    \"\"\"Signal whether the frame is a CPython or logging module internal.\"\"\"\n    filename = os.path.normcase(frame.f_code.co_filename)\n    return filename == _srcfile or (\n        \"importlib\" in filename and \"_bootstrap\" in filename\n    )\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\"\n                        % (level,))\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a _at_fork_reinit method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        _acquireLock()\n        try:\n            _at_fork_reinit_lock_weakset.add(instance)\n        finally:\n            _releaseLock()\n\n    def _after_at_fork_child_reinit_locks():\n        for handler in _at_fork_reinit_lock_weakset:\n            handler._at_fork_reinit()\n\n        # _acquireLock() was called in the parent before forking.\n        # The lock is reinitialized to unlocked state.\n        _lock._at_fork_reinit()\n\n    os.register_at_fork(before=_acquireLock,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_releaseLock)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = int((ct - int(ct)) * 1000) + 0.0  # see gh-89047\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt, *, defaults=None):\n        self._fmt = fmt or self.default_format\n        self._defaults = defaults\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt % values\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt.format(**values)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._tpl.substitute(**values)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True, *,\n                 defaults=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt, defaults=defaults)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            s = time.strftime(self.default_time_format, ct)\n            if self.default_msec_format:\n                s = self.default_msec_format % (s, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Returns True if the record should be logged, or False otherwise.\n        If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n        \"\"\"\n        rv = True\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                rv = False\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            handlers.remove(wr)\n        except ValueError:\n            pass\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        self._closed = False\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def _at_fork_reinit(self):\n        self.lock._at_fork_reinit()\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            self._closed = True\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            t, v, tb = sys.exc_info()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(t, v, tb, None, sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = tb.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del t, v, tb\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            self.acquire()\n            try:\n                self.flush()\n                self.stream = stream\n            finally:\n                self.release()\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False, errors=None):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        if \"b\" not in mode:\n            self.encoding = io.text_encoding(encoding)\n        self.errors = errors\n        self.delay = delay\n        # bpo-26789: FileHandler keeps a reference to the builtin open()\n        # function to be able to open or reopen the file during Python\n        # finalization.\n        self._builtin_open = open\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                # Also see Issue #42378: we also rely on\n                # self._closed being set to True there\n                StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        open_func = self._builtin_open\n        return open_func(self.baseFilename, self.mode,\n                         encoding=self.encoding, errors=self.errors)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n\n        If stream is not open, current mode is 'w' and `_closed=True`, record\n        will not be emitted (see Issue #42378).\n        \"\"\"\n        if self.stream is None:\n            if self.mode != 'w' or not self._closed:\n                self.stream = self._open()\n        if self.stream:\n            StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    @property\n    def disable(self):\n        return self._disable\n\n    @disable.setter\n    def disable(self, value):\n        self._disable = _checkLevel(value)\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        _acquireLock()\n        for logger in self.loggerDict.values():\n            if isinstance(logger, Logger):\n                logger._cache.clear()\n        self.root._cache.clear()\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    def fatal(self, msg, *args, **kwargs):\n        \"\"\"\n        Don't use this method, use critical() instead.\n        \"\"\"\n        self.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is None:\n            return \"(unknown file)\", 0, \"(unknown function)\", None\n        while stacklevel > 0:\n            next_f = f.f_back\n            if next_f is None:\n                ## We've got options here.\n                ## If we want to use the last (deepest) frame:\n                break\n                ## If we want to mimic the warnings module:\n                #return (\"sys\", 1, \"(unknown function)\", None)\n                ## If we want to be pedantic:\n                #raise ValueError(\"call stack is not deep enough\")\n            f = next_f\n            if not _is_internal_frame(f):\n                stacklevel -= 1\n        co = f.f_code\n        sinfo = None\n        if stack_info:\n            with io.StringIO() as sio:\n                sio.write(\"Stack (most recent call last):\\n\")\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n        return co.co_filename, f.f_lineno, co.co_name, sinfo\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            _acquireLock()\n            try:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            finally:\n                _releaseLock()\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra=None):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(\n            level,\n            msg,\n            args,\n            exc_info=exc_info,\n            extra=extra,\n            stack_info=stack_info,\n        )\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root handler. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    encoding  If specified together with a filename, this encoding is passed to\n              the created FileHandler, causing it to be used when the file is\n              opened.\n    errors    If specified together with a filename, this value is passed to the\n              created FileHandler, causing it to be used when the file is\n              opened in text mode. If not specified, the default value is\n              `backslashreplace`.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.9\n       Added the ``encoding`` and ``errors`` parameters.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        force = kwargs.pop('force', False)\n        encoding = kwargs.pop('encoding', None)\n        errors = kwargs.pop('errors', 'backslashreplace')\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    if 'b' in mode:\n                        errors = None\n                    else:\n                        encoding = io.text_encoding(encoding)\n                    h = FileHandler(filename, mode,\n                                    encoding=encoding, errors=errors)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == root.name:\n        return root\n    return Logger.manager.getLogger(name)\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\ndef fatal(msg, *args, **kwargs):\n    \"\"\"\n    Don't use this function, use critical() instead.\n    \"\"\"\n    critical(msg, *args, **kwargs)\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n    def _at_fork_reinit(self):\n        pass\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        # bpo-46557: Log str(s) as msg instead of logger.warning(\"%s\", s)\n        # since some log aggregation tools group logs by the msg arg\n        logger.warning(str(s))\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 2273], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py": ["# Wrapper module for _socket, providing some additional facilities\n# implemented in Python.\n\n\"\"\"\\\nThis module provides socket operations and some related functions.\nOn Unix, it supports IP (Internet Protocol) and Unix domain sockets.\nOn other systems, it only supports IP. Functions specific for a\nsocket are available as methods of the socket object.\n\nFunctions:\n\nsocket() -- create a new socket object\nsocketpair() -- create a pair of new socket objects [*]\nfromfd() -- create a socket object from an open file descriptor [*]\nsend_fds() -- Send file descriptor to the socket.\nrecv_fds() -- Receive file descriptors from the socket.\nfromshare() -- create a socket object from data received from socket.share() [*]\ngethostname() -- return the current hostname\ngethostbyname() -- map a hostname to its IP number\ngethostbyaddr() -- map an IP number or hostname to DNS info\ngetservbyname() -- map a service name and a protocol name to a port number\ngetprotobyname() -- map a protocol name (e.g. 'tcp') to a number\nntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order\nhtons(), htonl() -- convert 16, 32 bit int from host to network byte order\ninet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format\ninet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)\nsocket.getdefaulttimeout() -- get the default timeout value\nsocket.setdefaulttimeout() -- set the default timeout value\ncreate_connection() -- connects to an address, with an optional timeout and\n                       optional source address.\n\n [*] not available on all platforms!\n\nSpecial objects:\n\nSocketType -- type object for socket objects\nerror -- exception raised for I/O errors\nhas_ipv6 -- boolean value indicating if IPv6 is supported\n\nIntEnum constants:\n\nAF_INET, AF_UNIX -- socket domains (first argument to socket() call)\nSOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)\n\nInteger constants:\n\nMany other constants may be defined; these may be used in calls to\nthe setsockopt() and getsockopt() methods.\n\"\"\"\n\nimport _socket\nfrom _socket import *\n\nimport os, sys, io, selectors\nfrom enum import IntEnum, IntFlag\n\ntry:\n    import errno\nexcept ImportError:\n    errno = None\nEBADF = getattr(errno, 'EBADF', 9)\nEAGAIN = getattr(errno, 'EAGAIN', 11)\nEWOULDBLOCK = getattr(errno, 'EWOULDBLOCK', 11)\n\n__all__ = [\"fromfd\", \"getfqdn\", \"create_connection\", \"create_server\",\n           \"has_dualstack_ipv6\", \"AddressFamily\", \"SocketKind\"]\n__all__.extend(os._get_exports_list(_socket))\n\n# Set up the socket.AF_* socket.SOCK_* constants as members of IntEnums for\n# nicer string representations.\n# Note that _socket only knows about the integer values. The public interface\n# in this module understands the enums and translates them back from integers\n# where needed (e.g. .family property of a socket object).\n\nIntEnum._convert_(\n        'AddressFamily',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AF_'))\n\nIntEnum._convert_(\n        'SocketKind',\n        __name__,\n        lambda C: C.isupper() and C.startswith('SOCK_'))\n\nIntFlag._convert_(\n        'MsgFlag',\n        __name__,\n        lambda C: C.isupper() and C.startswith('MSG_'))\n\nIntFlag._convert_(\n        'AddressInfo',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AI_'))\n\n_LOCALHOST    = '127.0.0.1'\n_LOCALHOST_V6 = '::1'\n\n\ndef _intenum_converter(value, enum_klass):\n    \"\"\"Convert a numeric family value to an IntEnum member.\n\n    If it's not a known member, return the numeric value itself.\n    \"\"\"\n    try:\n        return enum_klass(value)\n    except ValueError:\n        return value\n\n\n# WSA error codes\nif sys.platform.lower().startswith(\"win\"):\n    errorTab = {}\n    errorTab[6] = \"Specified event object handle is invalid.\"\n    errorTab[8] = \"Insufficient memory available.\"\n    errorTab[87] = \"One or more parameters are invalid.\"\n    errorTab[995] = \"Overlapped operation aborted.\"\n    errorTab[996] = \"Overlapped I/O event object not in signaled state.\"\n    errorTab[997] = \"Overlapped operation will complete later.\"\n    errorTab[10004] = \"The operation was interrupted.\"\n    errorTab[10009] = \"A bad file handle was passed.\"\n    errorTab[10013] = \"Permission denied.\"\n    errorTab[10014] = \"A fault occurred on the network??\"  # WSAEFAULT\n    errorTab[10022] = \"An invalid operation was attempted.\"\n    errorTab[10024] = \"Too many open files.\"\n    errorTab[10035] = \"The socket operation would block.\"\n    errorTab[10036] = \"A blocking operation is already in progress.\"\n    errorTab[10037] = \"Operation already in progress.\"\n    errorTab[10038] = \"Socket operation on nonsocket.\"\n    errorTab[10039] = \"Destination address required.\"\n    errorTab[10040] = \"Message too long.\"\n    errorTab[10041] = \"Protocol wrong type for socket.\"\n    errorTab[10042] = \"Bad protocol option.\"\n    errorTab[10043] = \"Protocol not supported.\"\n    errorTab[10044] = \"Socket type not supported.\"\n    errorTab[10045] = \"Operation not supported.\"\n    errorTab[10046] = \"Protocol family not supported.\"\n    errorTab[10047] = \"Address family not supported by protocol family.\"\n    errorTab[10048] = \"The network address is in use.\"\n    errorTab[10049] = \"Cannot assign requested address.\"\n    errorTab[10050] = \"Network is down.\"\n    errorTab[10051] = \"Network is unreachable.\"\n    errorTab[10052] = \"Network dropped connection on reset.\"\n    errorTab[10053] = \"Software caused connection abort.\"\n    errorTab[10054] = \"The connection has been reset.\"\n    errorTab[10055] = \"No buffer space available.\"\n    errorTab[10056] = \"Socket is already connected.\"\n    errorTab[10057] = \"Socket is not connected.\"\n    errorTab[10058] = \"The network has been shut down.\"\n    errorTab[10059] = \"Too many references.\"\n    errorTab[10060] = \"The operation timed out.\"\n    errorTab[10061] = \"Connection refused.\"\n    errorTab[10062] = \"Cannot translate name.\"\n    errorTab[10063] = \"The name is too long.\"\n    errorTab[10064] = \"The host is down.\"\n    errorTab[10065] = \"The host is unreachable.\"\n    errorTab[10066] = \"Directory not empty.\"\n    errorTab[10067] = \"Too many processes.\"\n    errorTab[10068] = \"User quota exceeded.\"\n    errorTab[10069] = \"Disk quota exceeded.\"\n    errorTab[10070] = \"Stale file handle reference.\"\n    errorTab[10071] = \"Item is remote.\"\n    errorTab[10091] = \"Network subsystem is unavailable.\"\n    errorTab[10092] = \"Winsock.dll version out of range.\"\n    errorTab[10093] = \"Successful WSAStartup not yet performed.\"\n    errorTab[10101] = \"Graceful shutdown in progress.\"\n    errorTab[10102] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10103] = \"Call has been canceled.\"\n    errorTab[10104] = \"Procedure call table is invalid.\"\n    errorTab[10105] = \"Service provider is invalid.\"\n    errorTab[10106] = \"Service provider failed to initialize.\"\n    errorTab[10107] = \"System call failure.\"\n    errorTab[10108] = \"Service not found.\"\n    errorTab[10109] = \"Class type not found.\"\n    errorTab[10110] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10111] = \"Call was canceled.\"\n    errorTab[10112] = \"Database query was refused.\"\n    errorTab[11001] = \"Host not found.\"\n    errorTab[11002] = \"Nonauthoritative host not found.\"\n    errorTab[11003] = \"This is a nonrecoverable error.\"\n    errorTab[11004] = \"Valid name, no data record requested type.\"\n    errorTab[11005] = \"QoS receivers.\"\n    errorTab[11006] = \"QoS senders.\"\n    errorTab[11007] = \"No QoS senders.\"\n    errorTab[11008] = \"QoS no receivers.\"\n    errorTab[11009] = \"QoS request confirmed.\"\n    errorTab[11010] = \"QoS admission error.\"\n    errorTab[11011] = \"QoS policy failure.\"\n    errorTab[11012] = \"QoS bad style.\"\n    errorTab[11013] = \"QoS bad object.\"\n    errorTab[11014] = \"QoS traffic control error.\"\n    errorTab[11015] = \"QoS generic error.\"\n    errorTab[11016] = \"QoS service type error.\"\n    errorTab[11017] = \"QoS flowspec error.\"\n    errorTab[11018] = \"Invalid QoS provider buffer.\"\n    errorTab[11019] = \"Invalid QoS filter style.\"\n    errorTab[11020] = \"Invalid QoS filter style.\"\n    errorTab[11021] = \"Incorrect QoS filter count.\"\n    errorTab[11022] = \"Invalid QoS object length.\"\n    errorTab[11023] = \"Incorrect QoS flow count.\"\n    errorTab[11024] = \"Unrecognized QoS object.\"\n    errorTab[11025] = \"Invalid QoS policy object.\"\n    errorTab[11026] = \"Invalid QoS flow descriptor.\"\n    errorTab[11027] = \"Invalid QoS provider-specific flowspec.\"\n    errorTab[11028] = \"Invalid QoS provider-specific filterspec.\"\n    errorTab[11029] = \"Invalid QoS shape discard mode object.\"\n    errorTab[11030] = \"Invalid QoS shaping rate object.\"\n    errorTab[11031] = \"Reserved policy QoS element type.\"\n    __all__.append(\"errorTab\")\n\n\nclass _GiveupOnSendfile(Exception): pass\n\n\nclass socket(_socket.socket):\n\n    \"\"\"A subclass of _socket.socket adding the makefile() method.\"\"\"\n\n    __slots__ = [\"__weakref__\", \"_io_refs\", \"_closed\"]\n\n    def __init__(self, family=-1, type=-1, proto=-1, fileno=None):\n        # For user code address family and type values are IntEnum members, but\n        # for the underlying _socket.socket they're just integers. The\n        # constructor of _socket.socket converts the given argument to an\n        # integer automatically.\n        if fileno is None:\n            if family == -1:\n                family = AF_INET\n            if type == -1:\n                type = SOCK_STREAM\n            if proto == -1:\n                proto = 0\n        _socket.socket.__init__(self, family, type, proto, fileno)\n        self._io_refs = 0\n        self._closed = False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        if not self._closed:\n            self.close()\n\n    def __repr__(self):\n        \"\"\"Wrap __repr__() to reveal the real class name and socket\n        address(es).\n        \"\"\"\n        closed = getattr(self, '_closed', False)\n        s = \"<%s.%s%s fd=%i, family=%s, type=%s, proto=%i\" \\\n            % (self.__class__.__module__,\n               self.__class__.__qualname__,\n               \" [closed]\" if closed else \"\",\n               self.fileno(),\n               self.family,\n               self.type,\n               self.proto)\n        if not closed:\n            # getsockname and getpeername may not be available on WASI.\n            try:\n                laddr = self.getsockname()\n                if laddr:\n                    s += \", laddr=%s\" % str(laddr)\n            except (error, AttributeError):\n                pass\n            try:\n                raddr = self.getpeername()\n                if raddr:\n                    s += \", raddr=%s\" % str(raddr)\n            except (error, AttributeError):\n                pass\n        s += '>'\n        return s\n\n    def __getstate__(self):\n        raise TypeError(f\"cannot pickle {self.__class__.__name__!r} object\")\n\n    def dup(self):\n        \"\"\"dup() -> socket object\n\n        Duplicate the socket. Return a new socket object connected to the same\n        system resource. The new socket is non-inheritable.\n        \"\"\"\n        fd = dup(self.fileno())\n        sock = self.__class__(self.family, self.type, self.proto, fileno=fd)\n        sock.settimeout(self.gettimeout())\n        return sock\n\n    def accept(self):\n        \"\"\"accept() -> (socket object, address info)\n\n        Wait for an incoming connection.  Return a new socket\n        representing the connection, and the address of the client.\n        For IP sockets, the address info is a pair (hostaddr, port).\n        \"\"\"\n        fd, addr = self._accept()\n        sock = socket(self.family, self.type, self.proto, fileno=fd)\n        # Issue #7995: if no default timeout is set and the listening\n        # socket had a (non-zero) timeout, force the new socket in blocking\n        # mode to override platform-specific socket flags inheritance.\n        if getdefaulttimeout() is None and self.gettimeout():\n            sock.setblocking(True)\n        return sock, addr\n\n    def makefile(self, mode=\"r\", buffering=None, *,\n                 encoding=None, errors=None, newline=None):\n        \"\"\"makefile(...) -> an I/O stream connected to the socket\n\n        The arguments are as for io.open() after the filename, except the only\n        supported mode values are 'r' (default), 'w' and 'b'.\n        \"\"\"\n        # XXX refactor to share code?\n        if not set(mode) <= {\"r\", \"w\", \"b\"}:\n            raise ValueError(\"invalid mode %r (only r, w, b allowed)\" % (mode,))\n        writing = \"w\" in mode\n        reading = \"r\" in mode or not writing\n        assert reading or writing\n        binary = \"b\" in mode\n        rawmode = \"\"\n        if reading:\n            rawmode += \"r\"\n        if writing:\n            rawmode += \"w\"\n        raw = SocketIO(self, rawmode)\n        self._io_refs += 1\n        if buffering is None:\n            buffering = -1\n        if buffering < 0:\n            buffering = io.DEFAULT_BUFFER_SIZE\n        if buffering == 0:\n            if not binary:\n                raise ValueError(\"unbuffered streams must be binary\")\n            return raw\n        if reading and writing:\n            buffer = io.BufferedRWPair(raw, raw, buffering)\n        elif reading:\n            buffer = io.BufferedReader(raw, buffering)\n        else:\n            assert writing\n            buffer = io.BufferedWriter(raw, buffering)\n        if binary:\n            return buffer\n        encoding = io.text_encoding(encoding)\n        text = io.TextIOWrapper(buffer, encoding, errors, newline)\n        text.mode = mode\n        return text\n\n    if hasattr(os, 'sendfile'):\n\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            self._check_sendfile_params(file, offset, count)\n            sockno = self.fileno()\n            try:\n                fileno = file.fileno()\n            except (AttributeError, io.UnsupportedOperation) as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            try:\n                fsize = os.fstat(fileno).st_size\n            except OSError as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            if not fsize:\n                return 0  # empty file\n            # Truncate to 1GiB to avoid OverflowError, see bpo-38319.\n            blocksize = min(count or fsize, 2 ** 30)\n            timeout = self.gettimeout()\n            if timeout == 0:\n                raise ValueError(\"non-blocking sockets are not supported\")\n            # poll/select have the advantage of not requiring any\n            # extra file descriptor, contrarily to epoll/kqueue\n            # (also, they require a single syscall).\n            if hasattr(selectors, 'PollSelector'):\n                selector = selectors.PollSelector()\n            else:\n                selector = selectors.SelectSelector()\n            selector.register(sockno, selectors.EVENT_WRITE)\n\n            total_sent = 0\n            # localize variable access to minimize overhead\n            selector_select = selector.select\n            os_sendfile = os.sendfile\n            try:\n                while True:\n                    if timeout and not selector_select(timeout):\n                        raise TimeoutError('timed out')\n                    if count:\n                        blocksize = count - total_sent\n                        if blocksize <= 0:\n                            break\n                    try:\n                        sent = os_sendfile(sockno, fileno, offset, blocksize)\n                    except BlockingIOError:\n                        if not timeout:\n                            # Block until the socket is ready to send some\n                            # data; avoids hogging CPU resources.\n                            selector_select()\n                        continue\n                    except OSError as err:\n                        if total_sent == 0:\n                            # We can get here for different reasons, the main\n                            # one being 'file' is not a regular mmap(2)-like\n                            # file, in which case we'll fall back on using\n                            # plain send().\n                            raise _GiveupOnSendfile(err)\n                        raise err from None\n                    else:\n                        if sent == 0:\n                            break  # EOF\n                        offset += sent\n                        total_sent += sent\n                return total_sent\n            finally:\n                if total_sent > 0 and hasattr(file, 'seek'):\n                    file.seek(offset)\n    else:\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            raise _GiveupOnSendfile(\n                \"os.sendfile() not available on this platform\")\n\n    def _sendfile_use_send(self, file, offset=0, count=None):\n        self._check_sendfile_params(file, offset, count)\n        if self.gettimeout() == 0:\n            raise ValueError(\"non-blocking sockets are not supported\")\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 8192) if count else 8192\n        total_sent = 0\n        # localize variable access to minimize overhead\n        file_read = file.read\n        sock_send = self.send\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                data = memoryview(file_read(blocksize))\n                if not data:\n                    break  # EOF\n                while True:\n                    try:\n                        sent = sock_send(data)\n                    except BlockingIOError:\n                        continue\n                    else:\n                        total_sent += sent\n                        if sent < len(data):\n                            data = data[sent:]\n                        else:\n                            break\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not self.type & SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n\n    def sendfile(self, file, offset=0, count=None):\n        \"\"\"sendfile(file[, offset[, count]]) -> sent\n\n        Send a file until EOF is reached by using high-performance\n        os.sendfile() and return the total number of bytes which\n        were sent.\n        *file* must be a regular file object opened in binary mode.\n        If os.sendfile() is not available (e.g. Windows) or file is\n        not a regular file socket.send() will be used instead.\n        *offset* tells from where to start reading the file.\n        If specified, *count* is the total number of bytes to transmit\n        as opposed to sending the file until EOF is reached.\n        File position is updated on return or also in case of error in\n        which case file.tell() can be used to figure out the number of\n        bytes which were sent.\n        The socket must be of SOCK_STREAM type.\n        Non-blocking sockets are not supported.\n        \"\"\"\n        try:\n            return self._sendfile_use_sendfile(file, offset, count)\n        except _GiveupOnSendfile:\n            return self._sendfile_use_send(file, offset, count)\n\n    def _decref_socketios(self):\n        if self._io_refs > 0:\n            self._io_refs -= 1\n        if self._closed:\n            self.close()\n\n    def _real_close(self, _ss=_socket.socket):\n        # This function should not reference any globals. See issue #808164.\n        _ss.close(self)\n\n    def close(self):\n        # This function should not reference any globals. See issue #808164.\n        self._closed = True\n        if self._io_refs <= 0:\n            self._real_close()\n\n    def detach(self):\n        \"\"\"detach() -> file descriptor\n\n        Close the socket object without closing the underlying file descriptor.\n        The object cannot be used after this call, but the file descriptor\n        can be reused for other purposes.  The file descriptor is returned.\n        \"\"\"\n        self._closed = True\n        return super().detach()\n\n    @property\n    def family(self):\n        \"\"\"Read-only access to the address family for this socket.\n        \"\"\"\n        return _intenum_converter(super().family, AddressFamily)\n\n    @property\n    def type(self):\n        \"\"\"Read-only access to the socket type.\n        \"\"\"\n        return _intenum_converter(super().type, SocketKind)\n\n    if os.name == 'nt':\n        def get_inheritable(self):\n            return os.get_handle_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_handle_inheritable(self.fileno(), inheritable)\n    else:\n        def get_inheritable(self):\n            return os.get_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_inheritable(self.fileno(), inheritable)\n    get_inheritable.__doc__ = \"Get the inheritable flag of the socket\"\n    set_inheritable.__doc__ = \"Set the inheritable flag of the socket\"\n\ndef fromfd(fd, family, type, proto=0):\n    \"\"\" fromfd(fd, family, type[, proto]) -> socket object\n\n    Create a socket object from a duplicate of the given file\n    descriptor.  The remaining arguments are the same as for socket().\n    \"\"\"\n    nfd = dup(fd)\n    return socket(family, type, proto, nfd)\n\nif hasattr(_socket.socket, \"sendmsg\"):\n    import array\n\n    def send_fds(sock, buffers, fds, flags=0, address=None):\n        \"\"\" send_fds(sock, buffers, fds[, flags[, address]]) -> integer\n\n        Send the list of file descriptors fds over an AF_UNIX socket.\n        \"\"\"\n        return sock.sendmsg(buffers, [(_socket.SOL_SOCKET,\n            _socket.SCM_RIGHTS, array.array(\"i\", fds))])\n    __all__.append(\"send_fds\")\n\nif hasattr(_socket.socket, \"recvmsg\"):\n    import array\n\n    def recv_fds(sock, bufsize, maxfds, flags=0):\n        \"\"\" recv_fds(sock, bufsize, maxfds[, flags]) -> (data, list of file\n        descriptors, msg_flags, address)\n\n        Receive up to maxfds file descriptors returning the message\n        data and a list containing the descriptors.\n        \"\"\"\n        # Array of ints\n        fds = array.array(\"i\")\n        msg, ancdata, flags, addr = sock.recvmsg(bufsize,\n            _socket.CMSG_LEN(maxfds * fds.itemsize))\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if (cmsg_level == _socket.SOL_SOCKET and cmsg_type == _socket.SCM_RIGHTS):\n                fds.frombytes(cmsg_data[:\n                        len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return msg, list(fds), flags, addr\n    __all__.append(\"recv_fds\")\n\nif hasattr(_socket.socket, \"share\"):\n    def fromshare(info):\n        \"\"\" fromshare(info) -> socket object\n\n        Create a socket object from the bytes object returned by\n        socket.share(pid).\n        \"\"\"\n        return socket(0, 0, 0, info)\n    __all__.append(\"fromshare\")\n\nif hasattr(_socket, \"socketpair\"):\n\n    def socketpair(family=None, type=SOCK_STREAM, proto=0):\n        \"\"\"socketpair([family[, type[, proto]]]) -> (socket object, socket object)\n\n        Create a pair of socket objects from the sockets returned by the platform\n        socketpair() function.\n        The arguments are the same as for socket() except the default family is\n        AF_UNIX if defined on the platform; otherwise, the default is AF_INET.\n        \"\"\"\n        if family is None:\n            try:\n                family = AF_UNIX\n            except NameError:\n                family = AF_INET\n        a, b = _socket.socketpair(family, type, proto)\n        a = socket(family, type, proto, a.detach())\n        b = socket(family, type, proto, b.detach())\n        return a, b\n\nelse:\n\n    # Origin: https://gist.github.com/4325783, by Geert Jansen.  Public domain.\n    def socketpair(family=AF_INET, type=SOCK_STREAM, proto=0):\n        if family == AF_INET:\n            host = _LOCALHOST\n        elif family == AF_INET6:\n            host = _LOCALHOST_V6\n        else:\n            raise ValueError(\"Only AF_INET and AF_INET6 socket address families \"\n                             \"are supported\")\n        if type != SOCK_STREAM:\n            raise ValueError(\"Only SOCK_STREAM socket type is supported\")\n        if proto != 0:\n            raise ValueError(\"Only protocol zero is supported\")\n\n        # We create a connected TCP socket. Note the trick with\n        # setblocking(False) that prevents us from having to create a thread.\n        lsock = socket(family, type, proto)\n        try:\n            lsock.bind((host, 0))\n            lsock.listen()\n            # On IPv6, ignore flow_info and scope_id\n            addr, port = lsock.getsockname()[:2]\n            csock = socket(family, type, proto)\n            try:\n                csock.setblocking(False)\n                try:\n                    csock.connect((addr, port))\n                except (BlockingIOError, InterruptedError):\n                    pass\n                csock.setblocking(True)\n                ssock, _ = lsock.accept()\n            except:\n                csock.close()\n                raise\n        finally:\n            lsock.close()\n        return (ssock, csock)\n    __all__.append(\"socketpair\")\n\nsocketpair.__doc__ = \"\"\"socketpair([family[, type[, proto]]]) -> (socket object, socket object)\nCreate a pair of socket objects from the sockets returned by the platform\nsocketpair() function.\nThe arguments are the same as for socket() except the default family is AF_UNIX\nif defined on the platform; otherwise, the default is AF_INET.\n\"\"\"\n\n_blocking_errnos = { EAGAIN, EWOULDBLOCK }\n\nclass SocketIO(io.RawIOBase):\n\n    \"\"\"Raw I/O implementation for stream sockets.\n\n    This class supports the makefile() method on sockets.  It provides\n    the raw I/O interface on top of a socket object.\n    \"\"\"\n\n    # One might wonder why not let FileIO do the job instead.  There are two\n    # main reasons why FileIO is not adapted:\n    # - it wouldn't work under Windows (where you can't used read() and\n    #   write() on a socket handle)\n    # - it wouldn't work with socket timeouts (FileIO would ignore the\n    #   timeout and consider the socket non-blocking)\n\n    # XXX More docs\n\n    def __init__(self, sock, mode):\n        if mode not in (\"r\", \"w\", \"rw\", \"rb\", \"wb\", \"rwb\"):\n            raise ValueError(\"invalid mode: %r\" % mode)\n        io.RawIOBase.__init__(self)\n        self._sock = sock\n        if \"b\" not in mode:\n            mode += \"b\"\n        self._mode = mode\n        self._reading = \"r\" in mode\n        self._writing = \"w\" in mode\n        self._timeout_occurred = False\n\n    def readinto(self, b):\n        \"\"\"Read up to len(b) bytes into the writable buffer *b* and return\n        the number of bytes read.  If the socket is non-blocking and no bytes\n        are available, None is returned.\n\n        If *b* is non-empty, a 0 return value indicates that the connection\n        was shutdown at the other end.\n        \"\"\"\n        self._checkClosed()\n        self._checkReadable()\n        if self._timeout_occurred:\n            raise OSError(\"cannot read from timed out object\")\n        while True:\n            try:\n                return self._sock.recv_into(b)\n            except timeout:\n                self._timeout_occurred = True\n                raise\n            except error as e:\n                if e.errno in _blocking_errnos:\n                    return None\n                raise\n\n    def write(self, b):\n        \"\"\"Write the given bytes or bytearray object *b* to the socket\n        and return the number of bytes written.  This can be less than\n        len(b) if not all data could be written.  If the socket is\n        non-blocking and no bytes could be written None is returned.\n        \"\"\"\n        self._checkClosed()\n        self._checkWritable()\n        try:\n            return self._sock.send(b)\n        except error as e:\n            # XXX what about EINTR?\n            if e.errno in _blocking_errnos:\n                return None\n            raise\n\n    def readable(self):\n        \"\"\"True if the SocketIO is open for reading.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._reading\n\n    def writable(self):\n        \"\"\"True if the SocketIO is open for writing.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._writing\n\n    def seekable(self):\n        \"\"\"True if the SocketIO is open for seeking.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return super().seekable()\n\n    def fileno(self):\n        \"\"\"Return the file descriptor of the underlying socket.\n        \"\"\"\n        self._checkClosed()\n        return self._sock.fileno()\n\n    @property\n    def name(self):\n        if not self.closed:\n            return self.fileno()\n        else:\n            return -1\n\n    @property\n    def mode(self):\n        return self._mode\n\n    def close(self):\n        \"\"\"Close the SocketIO object.  This doesn't close the underlying\n        socket, except if all references to it have disappeared.\n        \"\"\"\n        if self.closed:\n            return\n        io.RawIOBase.close(self)\n        self._sock._decref_socketios()\n        self._sock = None\n\n\ndef getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available and `name`\n    was given, it is returned unchanged. If `name` was empty, '0.0.0.0' or '::',\n    hostname from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name in ('0.0.0.0', '::'):\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name\n\n\n_GLOBAL_DEFAULT_TIMEOUT = object()\n\ndef create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None, *, all_errors=False):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    A host of '' or port 0 tells the OS to use the default. When a connection\n    cannot be created, raises the last error if *all_errors* is False,\n    and an ExceptionGroup of all errors if *all_errors* is True.\n    \"\"\"\n\n    host, port = address\n    exceptions = []\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            # Break explicitly a reference cycle\n            exceptions.clear()\n            return sock\n\n        except error as exc:\n            if not all_errors:\n                exceptions.clear()  # raise only the last error\n            exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n\n    if len(exceptions):\n        try:\n            if not all_errors:\n                raise exceptions[0]\n            raise ExceptionGroup(\"create_connection failed\", exceptions)\n        finally:\n            # Break explicitly a reference cycle\n            exceptions.clear()\n    else:\n        raise error(\"getaddrinfo returns an empty list\")\n\n\ndef has_dualstack_ipv6():\n    \"\"\"Return True if the platform supports creating a SOCK_STREAM socket\n    which can handle both AF_INET and AF_INET6 (IPv4 / IPv6) connections.\n    \"\"\"\n    if not has_ipv6 \\\n            or not hasattr(_socket, 'IPPROTO_IPV6') \\\n            or not hasattr(_socket, 'IPV6_V6ONLY'):\n        return False\n    try:\n        with socket(AF_INET6, SOCK_STREAM) as sock:\n            sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            return True\n    except error:\n        return False\n\n\ndef create_server(address, *, family=AF_INET, backlog=None, reuse_port=False,\n                  dualstack_ipv6=False):\n    \"\"\"Convenience function which creates a SOCK_STREAM type socket\n    bound to *address* (a 2-tuple (host, port)) and return the socket\n    object.\n\n    *family* should be either AF_INET or AF_INET6.\n    *backlog* is the queue size passed to socket.listen().\n    *reuse_port* dictates whether to use the SO_REUSEPORT socket option.\n    *dualstack_ipv6*: if true and the platform supports it, it will\n    create an AF_INET6 socket able to accept both IPv4 or IPv6\n    connections. When false it will explicitly disable this option on\n    platforms that enable it by default (e.g. Linux).\n\n    >>> with create_server(('', 8000)) as server:\n    ...     while True:\n    ...         conn, addr = server.accept()\n    ...         # handle new connection\n    \"\"\"\n    if reuse_port and not hasattr(_socket, \"SO_REUSEPORT\"):\n        raise ValueError(\"SO_REUSEPORT not supported on this platform\")\n    if dualstack_ipv6:\n        if not has_dualstack_ipv6():\n            raise ValueError(\"dualstack_ipv6 not supported on this platform\")\n        if family != AF_INET6:\n            raise ValueError(\"dualstack_ipv6 requires AF_INET6 family\")\n    sock = socket(family, SOCK_STREAM)\n    try:\n        # Note about Windows. We don't set SO_REUSEADDR because:\n        # 1) It's unnecessary: bind() will succeed even in case of a\n        # previous closed socket on the same address and still in\n        # TIME_WAIT state.\n        # 2) If set, another socket is free to bind() on the same\n        # address, effectively preventing this one from accepting\n        # connections. Also, it may set the process in a state where\n        # it'll no longer respond to any signals or graceful kills.\n        # See: https://learn.microsoft.com/windows/win32/winsock/using-so-reuseaddr-and-so-exclusiveaddruse\n        if os.name not in ('nt', 'cygwin') and \\\n                hasattr(_socket, 'SO_REUSEADDR'):\n            try:\n                sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n            except error:\n                # Fail later on bind(), for platforms which may not\n                # support this option.\n                pass\n        if reuse_port:\n            sock.setsockopt(SOL_SOCKET, SO_REUSEPORT, 1)\n        if has_ipv6 and family == AF_INET6:\n            if dualstack_ipv6:\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            elif hasattr(_socket, \"IPV6_V6ONLY\") and \\\n                    hasattr(_socket, \"IPPROTO_IPV6\"):\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 1)\n        try:\n            sock.bind(address)\n        except error as err:\n            msg = '%s (while attempting to bind on address %r)' % \\\n                (err.strerror, address)\n            raise error(err.errno, msg) from None\n        if backlog is None:\n            sock.listen()\n        else:\n            sock.listen(backlog)\n        return sock\n    except error:\n        sock.close()\n        raise\n\n\ndef getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):\n    \"\"\"Resolve host and port into list of address info entries.\n\n    Translate the host/port argument into a sequence of 5-tuples that contain\n    all the necessary arguments for creating a socket connected to that service.\n    host is a domain name, a string representation of an IPv4/v6 address or\n    None. port is a string service name such as 'http', a numeric port number or\n    None. By passing None as the value of host and port, you can pass NULL to\n    the underlying C API.\n\n    The family, type and proto arguments can be optionally specified in order to\n    narrow the list of addresses returned. Passing zero as a value for each of\n    these arguments selects the full range of results.\n    \"\"\"\n    # We override this function since we want to translate the numeric family\n    # and socket type values to enum constants.\n    addrlist = []\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n        af, socktype, proto, canonname, sa = res\n        addrlist.append((_intenum_converter(af, AddressFamily),\n                         _intenum_converter(socktype, SocketKind),\n                         proto, canonname, sa))\n    return addrlist\n", 967], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py": ["\"\"\"Event loop using a selector and related classes.\n\nA selector is a \"notify-when-ready\" multiplexer.  For a subclass which\nalso includes support for signal handling, see the unix_events sub-module.\n\"\"\"\n\n__all__ = 'BaseSelectorEventLoop',\n\nimport collections\nimport errno\nimport functools\nimport selectors\nimport socket\nimport warnings\nimport weakref\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import base_events\nfrom . import constants\nfrom . import events\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\ndef _test_selector_event(selector, fd, event):\n    # Test if the selector is monitoring 'event' events\n    # for the file descriptor 'fd'.\n    try:\n        key = selector.get_key(fd)\n    except KeyError:\n        return False\n    else:\n        return bool(key.events & event)\n\n\nclass BaseSelectorEventLoop(base_events.BaseEventLoop):\n    \"\"\"Selector event loop.\n\n    See events.EventLoop for API specification.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__()\n\n        if selector is None:\n            selector = selectors.DefaultSelector()\n        logger.debug('Using selector: %s', selector.__class__.__name__)\n        self._selector = selector\n        self._make_self_pipe()\n        self._transports = weakref.WeakValueDictionary()\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        return _SelectorSocketTransport(self, sock, protocol, waiter,\n                                        extra, server)\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT,\n    ):\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout\n        )\n        _SelectorSocketTransport(self, rawsock, ssl_protocol,\n                                 extra=extra, server=server)\n        return ssl_protocol._app_transport\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        return _SelectorDatagramTransport(self, sock, protocol,\n                                          address, waiter, extra)\n\n    def close(self):\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self.is_closed():\n            return\n        self._close_self_pipe()\n        super().close()\n        if self._selector is not None:\n            self._selector.close()\n            self._selector = None\n\n    def _close_self_pipe(self):\n        self._remove_reader(self._ssock.fileno())\n        self._ssock.close()\n        self._ssock = None\n        self._csock.close()\n        self._csock = None\n        self._internal_fds -= 1\n\n    def _make_self_pipe(self):\n        # A self-socket, really. :-)\n        self._ssock, self._csock = socket.socketpair()\n        self._ssock.setblocking(False)\n        self._csock.setblocking(False)\n        self._internal_fds += 1\n        self._add_reader(self._ssock.fileno(), self._read_from_self)\n\n    def _process_self_data(self, data):\n        pass\n\n    def _read_from_self(self):\n        while True:\n            try:\n                data = self._ssock.recv(4096)\n                if not data:\n                    break\n                self._process_self_data(data)\n            except InterruptedError:\n                continue\n            except BlockingIOError:\n                break\n\n    def _write_to_self(self):\n        # This may be called from a different thread, possibly after\n        # _close_self_pipe() has been called or even while it is\n        # running.  Guard for self._csock being None or closed.  When\n        # a socket is closed, send() raises OSError (with errno set to\n        # EBADF, but let's not rely on the exact error code).\n        csock = self._csock\n        if csock is None:\n            return\n\n        try:\n            csock.send(b'\\0')\n        except OSError:\n            if self._debug:\n                logger.debug(\"Fail to write a null byte into the \"\n                             \"self-pipe socket\",\n                             exc_info=True)\n\n    def _start_serving(self, protocol_factory, sock,\n                       sslcontext=None, server=None, backlog=100,\n                       ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n                       ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        self._add_reader(sock.fileno(), self._accept_connection,\n                         protocol_factory, sock, sslcontext, server, backlog,\n                         ssl_handshake_timeout, ssl_shutdown_timeout)\n\n    def _accept_connection(\n            self, protocol_factory, sock,\n            sslcontext=None, server=None, backlog=100,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        # This method is only called once for each event loop tick where the\n        # listening socket has triggered an EVENT_READ. There may be multiple\n        # connections waiting for an .accept() so it is called in a loop.\n        # See https://bugs.python.org/issue27906 for more details.\n        for _ in range(backlog):\n            try:\n                conn, addr = sock.accept()\n                if self._debug:\n                    logger.debug(\"%r got a new connection from %r: %r\",\n                                 server, addr, conn)\n                conn.setblocking(False)\n            except (BlockingIOError, InterruptedError, ConnectionAbortedError):\n                # Early exit because the socket accept buffer is empty.\n                return None\n            except OSError as exc:\n                # There's nowhere to send the error, so just log it.\n                if exc.errno in (errno.EMFILE, errno.ENFILE,\n                                 errno.ENOBUFS, errno.ENOMEM):\n                    # Some platforms (e.g. Linux keep reporting the FD as\n                    # ready, so we remove the read handler temporarily.\n                    # We'll try again in a while.\n                    self.call_exception_handler({\n                        'message': 'socket.accept() out of system resource',\n                        'exception': exc,\n                        'socket': trsock.TransportSocket(sock),\n                    })\n                    self._remove_reader(sock.fileno())\n                    self.call_later(constants.ACCEPT_RETRY_DELAY,\n                                    self._start_serving,\n                                    protocol_factory, sock, sslcontext, server,\n                                    backlog, ssl_handshake_timeout,\n                                    ssl_shutdown_timeout)\n                else:\n                    raise  # The event loop will catch, log and ignore it.\n            else:\n                extra = {'peername': addr}\n                accept = self._accept_connection2(\n                    protocol_factory, conn, extra, sslcontext, server,\n                    ssl_handshake_timeout, ssl_shutdown_timeout)\n                self.create_task(accept)\n\n    async def _accept_connection2(\n            self, protocol_factory, conn, extra,\n            sslcontext=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        protocol = None\n        transport = None\n        try:\n            protocol = protocol_factory()\n            waiter = self.create_future()\n            if sslcontext:\n                transport = self._make_ssl_transport(\n                    conn, protocol, sslcontext, waiter=waiter,\n                    server_side=True, extra=extra, server=server,\n                    ssl_handshake_timeout=ssl_handshake_timeout,\n                    ssl_shutdown_timeout=ssl_shutdown_timeout)\n            else:\n                transport = self._make_socket_transport(\n                    conn, protocol, waiter=waiter, extra=extra,\n                    server=server)\n\n            try:\n                await waiter\n            except BaseException:\n                transport.close()\n                raise\n                # It's now up to the protocol to handle the connection.\n\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if self._debug:\n                context = {\n                    'message':\n                        'Error on transport creation for incoming connection',\n                    'exception': exc,\n                }\n                if protocol is not None:\n                    context['protocol'] = protocol\n                if transport is not None:\n                    context['transport'] = transport\n                self.call_exception_handler(context)\n\n    def _ensure_fd_no_transport(self, fd):\n        fileno = fd\n        if not isinstance(fileno, int):\n            try:\n                fileno = int(fileno.fileno())\n            except (AttributeError, TypeError, ValueError):\n                # This code matches selectors._fileobj_to_fd function.\n                raise ValueError(f\"Invalid file object: {fd!r}\") from None\n        try:\n            transport = self._transports[fileno]\n        except KeyError:\n            pass\n        else:\n            if not transport.is_closing():\n                raise RuntimeError(\n                    f'File descriptor {fd!r} is used by transport '\n                    f'{transport!r}')\n\n    def _add_reader(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_READ,\n                                    (handle, None))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_READ,\n                                  (handle, writer))\n            if reader is not None:\n                reader.cancel()\n        return handle\n\n    def _remove_reader(self, fd):\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            mask &= ~selectors.EVENT_READ\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (None, writer))\n\n            if reader is not None:\n                reader.cancel()\n                return True\n            else:\n                return False\n\n    def _add_writer(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_WRITE,\n                                    (None, handle))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_WRITE,\n                                  (reader, handle))\n            if writer is not None:\n                writer.cancel()\n        return handle\n\n    def _remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            # Remove both writer and connector.\n            mask &= ~selectors.EVENT_WRITE\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (reader, None))\n\n            if writer is not None:\n                writer.cancel()\n                return True\n            else:\n                return False\n\n    def add_reader(self, fd, callback, *args):\n        \"\"\"Add a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_reader(fd, callback, *args)\n\n    def remove_reader(self, fd):\n        \"\"\"Remove a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_reader(fd)\n\n    def add_writer(self, fd, callback, *args):\n        \"\"\"Add a writer callback..\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_writer(fd, callback, *args)\n\n    def remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_writer(fd)\n\n    async def sock_recv(self, sock, n):\n        \"\"\"Receive data from the socket.\n\n        The return value is a bytes object representing the data received.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv, fut, sock, n)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_read_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_reader(fd)\n\n    def _sock_recv(self, fut, sock, n):\n        # _sock_recv() can add itself as an I/O callback if the operation can't\n        # be done immediately. Don't use it directly, call sock_recv().\n        if fut.done():\n            return\n        try:\n            data = sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(data)\n\n    async def sock_recv_into(self, sock, buf):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is the number of bytes written.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv_into, fut, sock, buf)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recv_into(self, fut, sock, buf):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            nbytes = sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(nbytes)\n\n    async def sock_recvfrom(self, sock, bufsize):\n        \"\"\"Receive a datagram from a datagram socket.\n\n        The return value is a tuple of (bytes, address) representing the\n        datagram received and the address it came from.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recvfrom(bufsize)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recvfrom, fut, sock, bufsize)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recvfrom(self, fut, sock, bufsize):\n        # _sock_recvfrom() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recvfrom().\n        if fut.done():\n            return\n        try:\n            result = sock.recvfrom(bufsize)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(result)\n\n    async def sock_recvfrom_into(self, sock, buf, nbytes=0):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is a tuple of (number of bytes written, address).\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        if not nbytes:\n            nbytes = len(buf)\n\n        try:\n            return sock.recvfrom_into(buf, nbytes)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recvfrom_into, fut, sock, buf,\n                                  nbytes)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recvfrom_into(self, fut, sock, buf, bufsize):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            result = sock.recvfrom_into(buf, bufsize)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(result)\n\n    async def sock_sendall(self, sock, data):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            n = sock.send(data)\n        except (BlockingIOError, InterruptedError):\n            n = 0\n\n        if n == len(data):\n            # all data sent\n            return\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        # use a trick with a list in closure to store a mutable state\n        handle = self._add_writer(fd, self._sock_sendall, fut, sock,\n                                  memoryview(data), [n])\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd, handle=handle))\n        return await fut\n\n    def _sock_sendall(self, fut, sock, view, pos):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        start = pos[0]\n        try:\n            n = sock.send(view[start:])\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n            return\n\n        start += n\n\n        if start == len(view):\n            fut.set_result(None)\n        else:\n            pos[0] = start\n\n    async def sock_sendto(self, sock, data, address):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.sendto(data, address)\n        except (BlockingIOError, InterruptedError):\n            pass\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        # use a trick with a list in closure to store a mutable state\n        handle = self._add_writer(fd, self._sock_sendto, fut, sock, data,\n                                  address)\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd, handle=handle))\n        return await fut\n\n    def _sock_sendto(self, fut, sock, data, address):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        try:\n            n = sock.sendto(data, 0, address)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(n)\n\n    async def sock_connect(self, sock, address):\n        \"\"\"Connect to a remote socket at address.\n\n        This method is a coroutine.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n\n        if sock.family == socket.AF_INET or (\n                base_events._HAS_IPv6 and sock.family == socket.AF_INET6):\n            resolved = await self._ensure_resolved(\n                address, family=sock.family, type=sock.type, proto=sock.proto,\n                loop=self,\n            )\n            _, _, _, _, address = resolved[0]\n\n        fut = self.create_future()\n        self._sock_connect(fut, sock, address)\n        try:\n            return await fut\n        finally:\n            # Needed to break cycles when an exception occurs.\n            fut = None\n\n    def _sock_connect(self, fut, sock, address):\n        fd = sock.fileno()\n        try:\n            sock.connect(address)\n        except (BlockingIOError, InterruptedError):\n            # Issue #23618: When the C function connect() fails with EINTR, the\n            # connection runs in background. We have to wait until the socket\n            # becomes writable to be notified when the connection succeed or\n            # fails.\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_writer(\n                fd, self._sock_connect_cb, fut, sock, address)\n            fut.add_done_callback(\n                functools.partial(self._sock_write_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n        finally:\n            fut = None\n\n    def _sock_write_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_writer(fd)\n\n    def _sock_connect_cb(self, fut, sock, address):\n        if fut.done():\n            return\n\n        try:\n            err = sock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n            if err != 0:\n                # Jump to any except clause below.\n                raise OSError(err, f'Connect call failed {address}')\n        except (BlockingIOError, InterruptedError):\n            # socket is still registered, the callback will be retried later\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n        finally:\n            fut = None\n\n    async def sock_accept(self, sock):\n        \"\"\"Accept a connection.\n\n        The socket must be bound to an address and listening for connections.\n        The return value is a pair (conn, address) where conn is a new socket\n        object usable to send and receive data on the connection, and address\n        is the address bound to the socket on the other end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_accept(fut, sock)\n        return await fut\n\n    def _sock_accept(self, fut, sock):\n        fd = sock.fileno()\n        try:\n            conn, address = sock.accept()\n            conn.setblocking(False)\n        except (BlockingIOError, InterruptedError):\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_reader(fd, self._sock_accept, fut, sock)\n            fut.add_done_callback(\n                functools.partial(self._sock_read_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result((conn, address))\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        del self._transports[transp._sock_fd]\n        resume_reading = transp.is_reading()\n        transp.pause_reading()\n        await transp._make_empty_waiter()\n        try:\n            return await self.sock_sendfile(transp._sock, file, offset, count,\n                                            fallback=False)\n        finally:\n            transp._reset_empty_waiter()\n            if resume_reading:\n                transp.resume_reading()\n            self._transports[transp._sock_fd] = transp\n\n    def _process_events(self, event_list):\n        for key, mask in event_list:\n            fileobj, (reader, writer) = key.fileobj, key.data\n            if mask & selectors.EVENT_READ and reader is not None:\n                if reader._cancelled:\n                    self._remove_reader(fileobj)\n                else:\n                    self._add_callback(reader)\n            if mask & selectors.EVENT_WRITE and writer is not None:\n                if writer._cancelled:\n                    self._remove_writer(fileobj)\n                else:\n                    self._add_callback(writer)\n\n    def _stop_serving(self, sock):\n        self._remove_reader(sock.fileno())\n        sock.close()\n\n\nclass _SelectorTransport(transports._FlowControlMixin,\n                         transports.Transport):\n\n    max_size = 256 * 1024  # Buffer size passed to recv().\n\n    _buffer_factory = bytearray  # Constructs initial value for self._buffer.\n\n    # Attribute used in the destructor: it must be set even if the constructor\n    # is not called (see _SelectorSslTransport which may start by raising an\n    # exception)\n    _sock = None\n\n    def __init__(self, loop, sock, protocol, extra=None, server=None):\n        super().__init__(extra, loop)\n        self._extra['socket'] = trsock.TransportSocket(sock)\n        try:\n            self._extra['sockname'] = sock.getsockname()\n        except OSError:\n            self._extra['sockname'] = None\n        if 'peername' not in self._extra:\n            try:\n                self._extra['peername'] = sock.getpeername()\n            except socket.error:\n                self._extra['peername'] = None\n        self._sock = sock\n        self._sock_fd = sock.fileno()\n\n        self._protocol_connected = False\n        self.set_protocol(protocol)\n\n        self._server = server\n        self._buffer = self._buffer_factory()\n        self._conn_lost = 0  # Set when call to connection_lost scheduled.\n        self._closing = False  # Set when close() called.\n        self._paused = False  # Set when pause_reading() called\n\n        if self._server is not None:\n            self._server._attach()\n        loop._transports[self._sock_fd] = self\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._sock is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._sock_fd}')\n        # test if the transport was closed\n        if self._loop is not None and not self._loop.is_closed():\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd, selectors.EVENT_READ)\n            if polling:\n                info.append('read=polling')\n            else:\n                info.append('read=idle')\n\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd,\n                                           selectors.EVENT_WRITE)\n            if polling:\n                state = 'polling'\n            else:\n                state = 'idle'\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'write=<{state}, bufsize={bufsize}>')\n        return '<{}>'.format(' '.join(info))\n\n    def abort(self):\n        self._force_close(None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n        self._protocol_connected = True\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def is_reading(self):\n        return not self.is_closing() and not self._paused\n\n    def pause_reading(self):\n        if not self.is_reading():\n            return\n        self._paused = True\n        self._loop._remove_reader(self._sock_fd)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._add_reader(self._sock_fd, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def close(self):\n        if self._closing:\n            return\n        self._closing = True\n        self._loop._remove_reader(self._sock_fd)\n        if not self._buffer:\n            self._conn_lost += 1\n            self._loop._remove_writer(self._sock_fd)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._sock is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._sock.close()\n\n    def _fatal_error(self, exc, message='Fatal error on transport'):\n        # Should be called from exception handler only.\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._force_close(exc)\n\n    def _force_close(self, exc):\n        if self._conn_lost:\n            return\n        if self._buffer:\n            self._buffer.clear()\n            self._loop._remove_writer(self._sock_fd)\n        if not self._closing:\n            self._closing = True\n            self._loop._remove_reader(self._sock_fd)\n        self._conn_lost += 1\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            if self._protocol_connected:\n                self._protocol.connection_lost(exc)\n        finally:\n            self._sock.close()\n            self._sock = None\n            self._protocol = None\n            self._loop = None\n            server = self._server\n            if server is not None:\n                server._detach()\n                self._server = None\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _add_reader(self, fd, callback, *args):\n        if not self.is_reading():\n            return\n        self._loop._add_reader(fd, callback, *args)\n\n\nclass _SelectorSocketTransport(_SelectorTransport):\n\n    _start_tls_compatible = True\n    _sendfile_compatible = constants._SendfileMode.TRY_NATIVE\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n\n        self._read_ready_cb = None\n        super().__init__(loop, sock, protocol, extra, server)\n        self._eof = False\n        self._empty_waiter = None\n\n        # Disable the Nagle algorithm -- small writes will be\n        # sent without waiting for the TCP ACK.  This generally\n        # decreases the latency (in some cases significantly.)\n        base_events._set_nodelay(self._sock)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def set_protocol(self, protocol):\n        if isinstance(protocol, protocols.BufferedProtocol):\n            self._read_ready_cb = self._read_ready__get_buffer\n        else:\n            self._read_ready_cb = self._read_ready__data_received\n\n        super().set_protocol(protocol)\n\n    def _read_ready(self):\n        self._read_ready_cb()\n\n    def _read_ready__get_buffer(self):\n        if self._conn_lost:\n            return\n\n        try:\n            buf = self._protocol.get_buffer(-1)\n            if not len(buf):\n                raise RuntimeError('get_buffer() returned an empty buffer')\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.get_buffer() call failed.')\n            return\n\n        try:\n            nbytes = self._sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not nbytes:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.buffer_updated(nbytes)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.buffer_updated() call failed.')\n\n    def _read_ready__data_received(self):\n        if self._conn_lost:\n            return\n        try:\n            data = self._sock.recv(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not data:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.data_received(data)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.data_received() call failed.')\n\n    def _read_ready__on_eof(self):\n        if self._loop.get_debug():\n            logger.debug(\"%r received EOF\", self)\n\n        try:\n            keep_open = self._protocol.eof_received()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.eof_received() call failed.')\n            return\n\n        if keep_open:\n            # We're keeping the connection open so the\n            # protocol can write more, but we still can't\n            # receive more, so remove the reader callback.\n            self._loop._remove_reader(self._sock_fd)\n        else:\n            self.close()\n\n    def write(self, data):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if self._eof:\n            raise RuntimeError('Cannot call write() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to write; sendfile is in progress')\n        if not data:\n            return\n\n        if self._conn_lost:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Optimization: try to send now.\n            try:\n                n = self._sock.send(data)\n            except (BlockingIOError, InterruptedError):\n                pass\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(exc, 'Fatal write error on socket transport')\n                return\n            else:\n                data = data[n:]\n                if not data:\n                    return\n            # Not all was written; register write handler.\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n        # Add it to the buffer.\n        self._buffer.extend(data)\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        if self._conn_lost:\n            return\n        try:\n            n = self._sock.send(self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            if n:\n                del self._buffer[:n]\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def write_eof(self):\n        if self._closing or self._eof:\n            return\n        self._eof = True\n        if not self._buffer:\n            self._sock.shutdown(socket.SHUT_WR)\n\n    def can_write_eof(self):\n        return True\n\n    def _call_connection_lost(self, exc):\n        super()._call_connection_lost(exc)\n        if self._empty_waiter is not None:\n            self._empty_waiter.set_exception(\n                ConnectionError(\"Connection is closed by peer\"))\n\n    def _make_empty_waiter(self):\n        if self._empty_waiter is not None:\n            raise RuntimeError(\"Empty waiter is already set\")\n        self._empty_waiter = self._loop.create_future()\n        if not self._buffer:\n            self._empty_waiter.set_result(None)\n        return self._empty_waiter\n\n    def _reset_empty_waiter(self):\n        self._empty_waiter = None\n\n\nclass _SelectorDatagramTransport(_SelectorTransport):\n\n    _buffer_factory = collections.deque\n\n    def __init__(self, loop, sock, protocol, address=None,\n                 waiter=None, extra=None):\n        super().__init__(loop, sock, protocol, extra)\n        self._address = address\n        self._buffer_size = 0\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def get_write_buffer_size(self):\n        return self._buffer_size\n\n    def _read_ready(self):\n        if self._conn_lost:\n            return\n        try:\n            data, addr = self._sock.recvfrom(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on datagram transport')\n        else:\n            self._protocol.datagram_received(data, addr)\n\n    def sendto(self, data, addr=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if not data:\n            return\n\n        if self._address:\n            if addr not in (None, self._address):\n                raise ValueError(\n                    f'Invalid address: must be None or {self._address}')\n            addr = self._address\n\n        if self._conn_lost and self._address:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n                return\n            except (BlockingIOError, InterruptedError):\n                self._loop._add_writer(self._sock_fd, self._sendto_ready)\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        # Ensure that what we buffer is immutable.\n        self._buffer.append((bytes(data), addr))\n        self._buffer_size += len(data)\n        self._maybe_pause_protocol()\n\n    def _sendto_ready(self):\n        while self._buffer:\n            data, addr = self._buffer.popleft()\n            self._buffer_size -= len(data)\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n            except (BlockingIOError, InterruptedError):\n                self._buffer.appendleft((data, addr))  # Try again later.\n                self._buffer_size += len(data)\n                break\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        self._maybe_resume_protocol()  # May append to buffer.\n        if not self._buffer:\n            self._loop._remove_writer(self._sock_fd)\n            if self._closing:\n                self._call_connection_lost(None)\n", 1242], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py": ["\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttps://peps.python.org/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType,\n     _remove_dead_weakref)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nimport _collections_abc  # Import after _weakref to avoid circular import.\nimport sys\nimport itertools\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\",\n           \"WeakSet\", \"WeakMethod\", \"finalize\"]\n\n\n_collections_abc.MutableSet.register(WeakSet)\n\nclass WeakMethod(ref):\n    \"\"\"\n    A custom `weakref.ref` subclass which simulates a weak reference to\n    a bound method, working around the lifetime problem of bound methods.\n    \"\"\"\n\n    __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\n\n    def __new__(cls, meth, callback=None):\n        try:\n            obj = meth.__self__\n            func = meth.__func__\n        except AttributeError:\n            raise TypeError(\"argument should be a bound method, not {}\"\n                            .format(type(meth))) from None\n        def _cb(arg):\n            # The self-weakref trick is needed to avoid creating a reference\n            # cycle.\n            self = self_wr()\n            if self._alive:\n                self._alive = False\n                if callback is not None:\n                    callback(self)\n        self = ref.__new__(cls, obj, _cb)\n        self._func_ref = ref(func, _cb)\n        self._meth_type = type(meth)\n        self._alive = True\n        self_wr = ref(self)\n        return self\n\n    def __call__(self):\n        obj = super().__call__()\n        func = self._func_ref()\n        if obj is None or func is None:\n            return None\n        return self._meth_type(func, obj)\n\n    def __eq__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is other\n            return ref.__eq__(self, other) and self._func_ref == other._func_ref\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is not other\n            return ref.__ne__(self, other) or self._func_ref != other._func_ref\n        return NotImplemented\n\n    __hash__ = ref.__hash__\n\n\nclass WeakValueDictionary(_collections_abc.MutableMapping):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, other=(), /, **kw):\n        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Atomic removal is necessary since this function\n                    # can be called asynchronously by the GC\n                    _atomic_removal(self.data, wr.key)\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n        self.update(other, **kw)\n\n    def _commit_removals(self, _atomic_removal=_remove_dead_weakref):\n        pop = self._pending_removals.pop\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _atomic_removal(d, key)\n\n    def __getitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        o = self.data[key]()\n        if o is None:\n            raise KeyError(key)\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __len__(self):\n        if self._pending_removals:\n            self._commit_removals()\n        return len(self.data)\n\n    def __contains__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def copy(self):\n        if self._pending_removals:\n            self._commit_removals()\n        new = WeakValueDictionary()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        if self._pending_removals:\n            self._commit_removals()\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                v = wr()\n                if v is not None:\n                    yield k, v\n\n    def keys(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                if wr() is not None:\n                    yield k\n\n    __iter__ = keys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            yield from self.data.values()\n\n    def values(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for wr in self.data.values():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            else:\n                raise KeyError(key)\n        else:\n            return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n\n    def update(self, other=None, /, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if other is not None:\n            if not hasattr(other, \"items\"):\n                other = dict(other)\n            for key, o in other.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        for key, o in kwargs.items():\n            d[key] = KeyedRef(o, self._remove, key)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        return list(self.data.values())\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super().__init__(ob, callback)\n\n\nclass WeakKeyDictionary(_collections_abc.MutableMapping):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    try:\n                        del self.data[k]\n                    except KeyError:\n                        pass\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        self._dirty_len = False\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        pop = self._pending_removals.pop\n        d = self.data\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n\n            try:\n                del d[key]\n            except KeyError:\n                pass\n\n    def _scrub_removals(self):\n        d = self.data\n        self._pending_removals = [k for k in self._pending_removals if k in d]\n        self._dirty_len = False\n\n    def __delitem__(self, key):\n        self._dirty_len = True\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __len__(self):\n        if self._dirty_len and self._pending_removals:\n            # self._pending_removals may still contain keys which were\n            # explicitly removed, we have to scrub them (see issue #21173).\n            self._scrub_removals()\n        return len(self.data) - len(self._pending_removals)\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def items(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def keys(self):\n        with _IterationGuard(self):\n            for wr in self.data:\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = keys\n\n    def values(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                if wr() is not None:\n                    yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return list(self.data)\n\n    def popitem(self):\n        self._dirty_len = True\n        while True:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        self._dirty_len = True\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, /, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass finalize:\n    \"\"\"Class for finalization of weakrefable objects\n\n    finalize(obj, func, *args, **kwargs) returns a callable finalizer\n    object which will be called when obj is garbage collected. The\n    first time the finalizer is called it evaluates func(*arg, **kwargs)\n    and returns the result. After this the finalizer is dead, and\n    calling it just returns None.\n\n    When the program exits any remaining finalizers for which the\n    atexit attribute is true will be run in reverse order of creation.\n    By default atexit is true.\n    \"\"\"\n\n    # Finalizer objects don't have any state of their own.  They are\n    # just used as keys to lookup _Info objects in the registry.  This\n    # ensures that they cannot be part of a ref-cycle.\n\n    __slots__ = ()\n    _registry = {}\n    _shutdown = False\n    _index_iter = itertools.count()\n    _dirty = False\n    _registered_with_atexit = False\n\n    class _Info:\n        __slots__ = (\"weakref\", \"func\", \"args\", \"kwargs\", \"atexit\", \"index\")\n\n    def __init__(self, obj, func, /, *args, **kwargs):\n        if not self._registered_with_atexit:\n            # We may register the exit function more than once because\n            # of a thread race, but that is harmless\n            import atexit\n            atexit.register(self._exitfunc)\n            finalize._registered_with_atexit = True\n        info = self._Info()\n        info.weakref = ref(obj, self)\n        info.func = func\n        info.args = args\n        info.kwargs = kwargs or None\n        info.atexit = True\n        info.index = next(self._index_iter)\n        self._registry[self] = info\n        finalize._dirty = True\n\n    def __call__(self, _=None):\n        \"\"\"If alive then mark as dead and return func(*args, **kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.pop(self, None)\n        if info and not self._shutdown:\n            return info.func(*info.args, **(info.kwargs or {}))\n\n    def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    def peek(self):\n        \"\"\"If alive then return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None:\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    @property\n    def alive(self):\n        \"\"\"Whether finalizer is alive\"\"\"\n        return self in self._registry\n\n    @property\n    def atexit(self):\n        \"\"\"Whether finalizer should be called at exit\"\"\"\n        info = self._registry.get(self)\n        return bool(info) and info.atexit\n\n    @atexit.setter\n    def atexit(self, value):\n        info = self._registry.get(self)\n        if info:\n            info.atexit = bool(value)\n\n    def __repr__(self):\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is None:\n            return '<%s object at %#x; dead>' % (type(self).__name__, id(self))\n        else:\n            return '<%s object at %#x; for %r at %#x>' % \\\n                (type(self).__name__, id(self), type(obj).__name__, id(obj))\n\n    @classmethod\n    def _select_for_exit(cls):\n        # Return live finalizers marked for exit, oldest first\n        L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n        L.sort(key=lambda item:item[1].index)\n        return [f for (f,i) in L]\n\n    @classmethod\n    def _exitfunc(cls):\n        # At shutdown invoke finalizers for which atexit is true.\n        # This is called once all other non-daemonic threads have been\n        # joined.\n        reenable_gc = False\n        try:\n            if cls._registry:\n                import gc\n                if gc.isenabled():\n                    reenable_gc = True\n                    gc.disable()\n                pending = None\n                while True:\n                    if pending is None or finalize._dirty:\n                        pending = cls._select_for_exit()\n                        finalize._dirty = False\n                    if not pending:\n                        break\n                    f = pending.pop()\n                    try:\n                        # gc is disabled, so (assuming no daemonic\n                        # threads) the following is the only line in\n                        # this function which might trigger creation\n                        # of a new finalizer\n                        f()\n                    except Exception:\n                        sys.excepthook(*sys.exc_info())\n                    assert f not in cls._registry\n        finally:\n            # prevent any more finalizers from executing during shutdown\n            finalize._shutdown = True\n            if reenable_gc:\n                gc.enable()\n", 674], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport functools\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs', 'gettrace', 'getprofile']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef getprofile():\n    \"\"\"Get the profiler function as set by threading.setprofile().\"\"\"\n    return _profile_hook\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\ndef gettrace():\n    \"\"\"Get the trace function as set by threading.settrace().\"\"\"\n    return _trace_hook\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        This method is deprecated, use notify_all() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('notifyAll() is deprecated, use notify_all() instead',\n                      DeprecationWarning, stacklevel=2)\n        self.notify_all()\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}>\")\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}/{self._initial_value}>\")\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def __repr__(self):\n        cls = self.__class__\n        status = 'set' if self._flag else 'unset'\n        return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: {status}>\"\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._reset_internal_locks()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    def isSet(self):\n        \"\"\"Return true if and only if the internal flag is true.\n\n        This method is deprecated, use is_set() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isSet() is deprecated, use is_set() instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.is_set()\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def __repr__(self):\n        cls = self.__class__\n        if self.broken:\n            return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: broken>\"\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" waiters={self.n_waiting}/{self.parties}>\")\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count(1).__next__\ndef _newname(name_template):\n    return name_template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\ndef _maintain_shutdown_locks():\n    \"\"\"\n    Drop any shutdown locks that don't correspond to running threads anymore.\n\n    Calling this from time to time avoids an ever-growing _shutdown_locks\n    set when Thread objects are not joined explicitly. See bpo-37788.\n\n    This must be called with _shutdown_locks_lock acquired.\n    \"\"\"\n    # If a lock was released, the corresponding thread has exited\n    to_remove = [lock for lock in _shutdown_locks if not lock.locked()]\n    _shutdown_locks.difference_update(to_remove)\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        if name:\n            name = str(name)\n        else:\n            name = _newname(\"Thread-%d\")\n            if target is not None:\n                try:\n                    target_name = target.__name__\n                    name += f\" ({target_name})\"\n                except AttributeError:\n                    pass\n\n        self._target = target\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._at_fork_reinit()\n        if is_alive:\n            # bpo-42350: If the fork happens when the thread is already stopped\n            # (ex: after threading._shutdown() has been called), _tstate_lock\n            # is None. Do nothing in this case.\n            if self._tstate_lock is not None:\n                self._tstate_lock._at_fork_reinit()\n                self._tstate_lock.acquire()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _maintain_shutdown_locks()\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            self._delete()\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                # Remove our lock and other released locks from _shutdown_locks\n                _maintain_shutdown_locks()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:\n            # already determined that the C code is done\n            assert self._is_stopped\n            return\n\n        try:\n            if lock.acquire(block, timeout):\n                lock.release()\n                self._stop()\n        except:\n            if lock.locked():\n                # bpo-45274: lock.acquire() acquired the lock, but the function\n                # was interrupted with an exception before reaching the\n                # lock.release(). It can happen if a signal handler raises an\n                # exception, like CTRL+C which raises KeyboardInterrupt.\n                lock.release()\n                self._stop()\n            raise\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        \"\"\"Return whether this thread is a daemon.\n\n        This method is deprecated, use the daemon attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        \"\"\"Set whether this thread is a daemon.\n\n        This method is deprecated, use the .daemon property instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.daemon = daemonic\n\n    def getName(self):\n        \"\"\"Return a string used for identification purposes only.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('getName() is deprecated, get the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.name\n\n    def setName(self, name):\n        \"\"\"Set the name string for this thread.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setName() is deprecated, set the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\n# Original value of threading.excepthook\n__excepthook__ = excepthook\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    This function is deprecated, use current_thread() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('currentThread() is deprecated, use current_thread() instead',\n                  DeprecationWarning, stacklevel=2)\n    return current_thread()\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    This function is deprecated, use active_count() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('activeCount() is deprecated, use active_count() instead',\n                  DeprecationWarning, stacklevel=2)\n    return active_count()\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    call = functools.partial(func, *arg, **kwargs)\n    _threading_atexits.append(call)\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    # Main thread\n    if _main_thread.ident == get_ident():\n        tlock = _main_thread._tstate_lock\n        # The main thread isn't finished yet, so its thread state lock can't\n        # have been released.\n        assert tlock is not None\n        assert tlock.locked()\n        tlock.release()\n        _main_thread._stop()\n    else:\n        # bpo-1596321: _shutdown() must be called in the main thread.\n        # If the threading module was not imported by the main thread,\n        # _main_thread is the thread which imported the threading module.\n        # In this case, ignore _main_thread, similar behavior than for threads\n        # spawned by C libraries or using _thread.start_new_thread().\n        pass\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimic Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1661], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py": ["import sys\nimport builtins as bltns\nfrom types import MappingProxyType, DynamicClassAttribute\nfrom operator import or_ as _or_\nfrom functools import reduce\n\n\n__all__ = [\n        'EnumType', 'EnumMeta',\n        'Enum', 'IntEnum', 'StrEnum', 'Flag', 'IntFlag', 'ReprEnum',\n        'auto', 'unique', 'property', 'verify', 'member', 'nonmember',\n        'FlagBoundary', 'STRICT', 'CONFORM', 'EJECT', 'KEEP',\n        'global_flag_repr', 'global_enum_repr', 'global_str', 'global_enum',\n        'EnumCheck', 'CONTINUOUS', 'NAMED_FLAGS', 'UNIQUE',\n        'pickle_by_global_name', 'pickle_by_enum_name',\n        ]\n\n\n# Dummy value for Enum and Flag as there are explicit checks for them\n# before they have been created.\n# This is also why there are checks in EnumType like `if Enum is not None`\nEnum = Flag = EJECT = _stdlib_enums = ReprEnum = None\n\nclass nonmember(object):\n    \"\"\"\n    Protects item from becoming an Enum member during class creation.\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n\nclass member(object):\n    \"\"\"\n    Forces item to become an Enum member during class creation.\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n\ndef _is_descriptor(obj):\n    \"\"\"\n    Returns True if obj is a descriptor, False otherwise.\n    \"\"\"\n    return (\n            hasattr(obj, '__get__') or\n            hasattr(obj, '__set__') or\n            hasattr(obj, '__delete__')\n            )\n\ndef _is_dunder(name):\n    \"\"\"\n    Returns True if a __dunder__ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 4 and\n            name[:2] == name[-2:] == '__' and\n            name[2] != '_' and\n            name[-3] != '_'\n            )\n\ndef _is_sunder(name):\n    \"\"\"\n    Returns True if a _sunder_ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 2 and\n            name[0] == name[-1] == '_' and\n            name[1:2] != '_' and\n            name[-2:-1] != '_'\n            )\n\ndef _is_internal_class(cls_name, obj):\n    # do not use `re` as `re` imports `enum`\n    if not isinstance(obj, type):\n        return False\n    qualname = getattr(obj, '__qualname__', '')\n    s_pattern = cls_name + '.' + getattr(obj, '__name__', '')\n    e_pattern = '.' + s_pattern\n    return qualname == s_pattern or qualname.endswith(e_pattern)\n\ndef _is_private(cls_name, name):\n    # do not use `re` as `re` imports `enum`\n    pattern = '_%s__' % (cls_name, )\n    pat_len = len(pattern)\n    if (\n            len(name) > pat_len\n            and name.startswith(pattern)\n            and name[pat_len:pat_len+1] != ['_']\n            and (name[-1] != '_' or name[-2] != '_')\n        ):\n        return True\n    else:\n        return False\n\ndef _is_single_bit(num):\n    \"\"\"\n    True if only one bit set in num (should be an int)\n    \"\"\"\n    if num == 0:\n        return False\n    num &= num - 1\n    return num == 0\n\ndef _make_class_unpicklable(obj):\n    \"\"\"\n    Make the given obj un-picklable.\n\n    obj should be either a dictionary, or an Enum\n    \"\"\"\n    def _break_on_call_reduce(self, proto):\n        raise TypeError('%r cannot be pickled' % self)\n    if isinstance(obj, dict):\n        obj['__reduce_ex__'] = _break_on_call_reduce\n        obj['__module__'] = '<unknown>'\n    else:\n        setattr(obj, '__reduce_ex__', _break_on_call_reduce)\n        setattr(obj, '__module__', '<unknown>')\n\ndef _iter_bits_lsb(num):\n    # num must be a positive integer\n    original = num\n    if isinstance(num, Enum):\n        num = num.value\n    if num < 0:\n        raise ValueError('%r is not a positive integer' % original)\n    while num:\n        b = num & (~num + 1)\n        yield b\n        num ^= b\n\ndef show_flag_values(value):\n    return list(_iter_bits_lsb(value))\n\ndef bin(num, max_bits=None):\n    \"\"\"\n    Like built-in bin(), except negative values are represented in\n    twos-compliment, and the leading bit always indicates sign\n    (0=positive, 1=negative).\n\n    >>> bin(10)\n    '0b0 1010'\n    >>> bin(~10)   # ~10 is -11\n    '0b1 0101'\n    \"\"\"\n\n    ceiling = 2 ** (num).bit_length()\n    if num >= 0:\n        s = bltns.bin(num + ceiling).replace('1', '0', 1)\n    else:\n        s = bltns.bin(~num ^ (ceiling - 1) + ceiling)\n    sign = s[:3]\n    digits = s[3:]\n    if max_bits is not None:\n        if len(digits) < max_bits:\n            digits = (sign[-1] * max_bits + digits)[-max_bits:]\n    return \"%s %s\" % (sign, digits)\n\ndef _dedent(text):\n    \"\"\"\n    Like textwrap.dedent.  Rewritten because we cannot import textwrap.\n    \"\"\"\n    lines = text.split('\\n')\n    blanks = 0\n    for i, ch in enumerate(lines[0]):\n        if ch != ' ':\n            break\n    for j, l in enumerate(lines):\n        lines[j] = l[i:]\n    return '\\n'.join(lines)\n\nclass _auto_null:\n    def __repr__(self):\n        return '_auto_null'\n_auto_null = _auto_null()\n\nclass auto:\n    \"\"\"\n    Instances are replaced with an appropriate value in Enum class suites.\n    \"\"\"\n    def __init__(self, value=_auto_null):\n        self.value = value\n\n    def __repr__(self):\n        return \"auto(%r)\" % self.value\n\nclass property(DynamicClassAttribute):\n    \"\"\"\n    This is a descriptor, used to define attributes that act differently\n    when accessed through an enum member and through an enum class.\n    Instance access is the same as property(), but access to an attribute\n    through the enum class will instead look in the class' _member_map_ for\n    a corresponding enum member.\n    \"\"\"\n\n    def __get__(self, instance, ownerclass=None):\n        if instance is None:\n            try:\n                return ownerclass._member_map_[self.name]\n            except KeyError:\n                raise AttributeError(\n                        '%r has no attribute %r' % (ownerclass, self.name)\n                        )\n        else:\n            if self.fget is None:\n                # look for a member by this name.\n                try:\n                    return ownerclass._member_map_[self.name]\n                except KeyError:\n                    raise AttributeError(\n                            '%r has no attribute %r' % (ownerclass, self.name)\n                            ) from None\n            else:\n                return self.fget(instance)\n\n    def __set__(self, instance, value):\n        if self.fset is None:\n            raise AttributeError(\n                    \"<enum %r> cannot set attribute %r\" % (self.clsname, self.name)\n                    )\n        else:\n            return self.fset(instance, value)\n\n    def __delete__(self, instance):\n        if self.fdel is None:\n            raise AttributeError(\n                    \"<enum %r> cannot delete attribute %r\" % (self.clsname, self.name)\n                    )\n        else:\n            return self.fdel(instance)\n\n    def __set_name__(self, ownerclass, name):\n        self.name = name\n        self.clsname = ownerclass.__name__\n\n\nclass _proto_member:\n    \"\"\"\n    intermediate step for enum members between class execution and final creation\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n    def __set_name__(self, enum_class, member_name):\n        \"\"\"\n        convert each quasi-member into an instance of the new enum class\n        \"\"\"\n        # first step: remove ourself from enum_class\n        delattr(enum_class, member_name)\n        # second step: create member based on enum_class\n        value = self.value\n        if not isinstance(value, tuple):\n            args = (value, )\n        else:\n            args = value\n        if enum_class._member_type_ is tuple:   # special case for tuple enums\n            args = (args, )     # wrap it one more time\n        if not enum_class._use_args_:\n            enum_member = enum_class._new_member_(enum_class)\n        else:\n            enum_member = enum_class._new_member_(enum_class, *args)\n        if not hasattr(enum_member, '_value_'):\n            if enum_class._member_type_ is object:\n                enum_member._value_ = value\n            else:\n                try:\n                    enum_member._value_ = enum_class._member_type_(*args)\n                except Exception as exc:\n                    new_exc = TypeError(\n                            '_value_ not set in __new__, unable to create it'\n                            )\n                    new_exc.__cause__ = exc\n                    raise new_exc\n        value = enum_member._value_\n        enum_member._name_ = member_name\n        enum_member.__objclass__ = enum_class\n        enum_member.__init__(*args)\n        enum_member._sort_order_ = len(enum_class._member_names_)\n\n        if Flag is not None and issubclass(enum_class, Flag):\n            enum_class._flag_mask_ |= value\n            if _is_single_bit(value):\n                enum_class._singles_mask_ |= value\n            enum_class._all_bits_ = 2 ** ((enum_class._flag_mask_).bit_length()) - 1\n\n        # If another member with the same value was already defined, the\n        # new member becomes an alias to the existing one.\n        try:\n            try:\n                # try to do a fast lookup to avoid the quadratic loop\n                enum_member = enum_class._value2member_map_[value]\n            except TypeError:\n                for name, canonical_member in enum_class._member_map_.items():\n                    if canonical_member._value_ == value:\n                        enum_member = canonical_member\n                        break\n                else:\n                    raise KeyError\n        except KeyError:\n            # this could still be an alias if the value is multi-bit and the\n            # class is a flag class\n            if (\n                    Flag is None\n                    or not issubclass(enum_class, Flag)\n                ):\n                # no other instances found, record this member in _member_names_\n                enum_class._member_names_.append(member_name)\n            elif (\n                    Flag is not None\n                    and issubclass(enum_class, Flag)\n                    and _is_single_bit(value)\n                ):\n                # no other instances found, record this member in _member_names_\n                enum_class._member_names_.append(member_name)\n        # if necessary, get redirect in place and then add it to _member_map_\n        found_descriptor = None\n        for base in enum_class.__mro__[1:]:\n            descriptor = base.__dict__.get(member_name)\n            if descriptor is not None:\n                if isinstance(descriptor, (property, DynamicClassAttribute)):\n                    found_descriptor = descriptor\n                    break\n                elif (\n                        hasattr(descriptor, 'fget') and\n                        hasattr(descriptor, 'fset') and\n                        hasattr(descriptor, 'fdel')\n                    ):\n                    found_descriptor = descriptor\n                    continue\n        if found_descriptor:\n            redirect = property()\n            redirect.member = enum_member\n            redirect.__set_name__(enum_class, member_name)\n            # earlier descriptor found; copy fget, fset, fdel to this one.\n            redirect.fget = found_descriptor.fget\n            redirect.fset = found_descriptor.fset\n            redirect.fdel = found_descriptor.fdel\n            setattr(enum_class, member_name, redirect)\n        else:\n            setattr(enum_class, member_name, enum_member)\n        # now add to _member_map_ (even aliases)\n        enum_class._member_map_[member_name] = enum_member\n        try:\n            # This may fail if value is not hashable. We can't add the value\n            # to the map, and by-value lookups for this value will be\n            # linear.\n            enum_class._value2member_map_.setdefault(value, enum_member)\n        except TypeError:\n            # keep track of the value in a list so containment checks are quick\n            enum_class._unhashable_values_.append(value)\n\n\nclass _EnumDict(dict):\n    \"\"\"\n    Track enum member order and ensure member names are not reused.\n\n    EnumType will use the names found in self._member_names as the\n    enumeration member names.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._member_names = {} # use a dict to keep insertion order\n        self._last_values = []\n        self._ignore = []\n        self._auto_called = False\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Changes anything not dundered or not a descriptor.\n\n        If an enum member name is used twice, an error is raised; duplicate\n        values are not checked for.\n\n        Single underscore (sunder) names are reserved.\n        \"\"\"\n        if _is_internal_class(self._cls_name, value):\n            import warnings\n            warnings.warn(\n                    \"In 3.13 classes created inside an enum will not become a member.  \"\n                    \"Use the `member` decorator to keep the current behavior.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                    )\n        if _is_private(self._cls_name, key):\n            # also do nothing, name will be a normal attribute\n            pass\n        elif _is_sunder(key):\n            if key not in (\n                    '_order_',\n                    '_generate_next_value_', '_numeric_repr_', '_missing_', '_ignore_',\n                    '_iter_member_', '_iter_member_by_value_', '_iter_member_by_def_',\n                    ):\n                raise ValueError(\n                        '_sunder_ names, such as %r, are reserved for future Enum use'\n                        % (key, )\n                        )\n            if key == '_generate_next_value_':\n                # check if members already defined as auto()\n                if self._auto_called:\n                    raise TypeError(\"_generate_next_value_ must be defined before members\")\n                _gnv = value.__func__ if isinstance(value, staticmethod) else value\n                setattr(self, '_generate_next_value', _gnv)\n            elif key == '_ignore_':\n                if isinstance(value, str):\n                    value = value.replace(',',' ').split()\n                else:\n                    value = list(value)\n                self._ignore = value\n                already = set(value) & set(self._member_names)\n                if already:\n                    raise ValueError(\n                            '_ignore_ cannot specify already set names: %r'\n                            % (already, )\n                            )\n        elif _is_dunder(key):\n            if key == '__order__':\n                key = '_order_'\n        elif key in self._member_names:\n            # descriptor overwriting an enum?\n            raise TypeError('%r already defined as %r' % (key, self[key]))\n        elif key in self._ignore:\n            pass\n        elif isinstance(value, nonmember):\n            # unwrap value here; it won't be processed by the below `else`\n            value = value.value\n        elif _is_descriptor(value):\n            pass\n        # TODO: uncomment next three lines in 3.13\n        # elif _is_internal_class(self._cls_name, value):\n        #     # do nothing, name will be a normal attribute\n        #     pass\n        else:\n            if key in self:\n                # enum overwriting a descriptor?\n                raise TypeError('%r already defined as %r' % (key, self[key]))\n            elif isinstance(value, member):\n                # unwrap value here -- it will become a member\n                value = value.value\n            non_auto_store = True\n            single = False\n            if isinstance(value, auto):\n                single = True\n                value = (value, )\n            if type(value) is tuple and any(isinstance(v, auto) for v in value):\n                # insist on an actual tuple, no subclasses, in keeping with only supporting\n                # top-level auto() usage (not contained in any other data structure)\n                auto_valued = []\n                for v in value:\n                    if isinstance(v, auto):\n                        non_auto_store = False\n                        if v.value == _auto_null:\n                            v.value = self._generate_next_value(\n                                    key, 1, len(self._member_names), self._last_values[:],\n                                    )\n                            self._auto_called = True\n                        v = v.value\n                        self._last_values.append(v)\n                    auto_valued.append(v)\n                if single:\n                    value = auto_valued[0]\n                else:\n                    value = tuple(auto_valued)\n            self._member_names[key] = None\n            if non_auto_store:\n                self._last_values.append(value)\n        super().__setitem__(key, value)\n\n    def update(self, members, **more_members):\n        try:\n            for name in members.keys():\n                self[name] = members[name]\n        except AttributeError:\n            for name, value in members:\n                self[name] = value\n        for name, value in more_members.items():\n            self[name] = value\n\n\nclass EnumType(type):\n    \"\"\"\n    Metaclass for Enum\n    \"\"\"\n\n    @classmethod\n    def __prepare__(metacls, cls, bases, **kwds):\n        # check that previous enum members do not exist\n        metacls._check_for_existing_members_(cls, bases)\n        # create the namespace dict\n        enum_dict = _EnumDict()\n        enum_dict._cls_name = cls\n        # inherit previous flags and _generate_next_value_ function\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        if first_enum is not None:\n            enum_dict['_generate_next_value_'] = getattr(\n                    first_enum, '_generate_next_value_', None,\n                    )\n        return enum_dict\n\n    def __new__(metacls, cls, bases, classdict, *, boundary=None, _simple=False, **kwds):\n        # an Enum class is final once enumeration items have been defined; it\n        # cannot be mixed with other types (int, float, etc.) if it has an\n        # inherited __new__ unless a new __new__ is defined (or the resulting\n        # class will fail).\n        #\n        if _simple:\n            return super().__new__(metacls, cls, bases, classdict, **kwds)\n        #\n        # remove any keys listed in _ignore_\n        classdict.setdefault('_ignore_', []).append('_ignore_')\n        ignore = classdict['_ignore_']\n        for key in ignore:\n            classdict.pop(key, None)\n        #\n        # grab member names\n        member_names = classdict._member_names\n        #\n        # check for illegal enum names (any others?)\n        invalid_names = set(member_names) & {'mro', ''}\n        if invalid_names:\n            raise ValueError('invalid enum member name(s) %s'  % (\n                    ','.join(repr(n) for n in invalid_names)\n                    ))\n        #\n        # adjust the sunders\n        _order_ = classdict.pop('_order_', None)\n        # convert to normal dict\n        classdict = dict(classdict.items())\n        #\n        # data type of member and the controlling Enum class\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        __new__, save_new, use_args = metacls._find_new_(\n                classdict, member_type, first_enum,\n                )\n        classdict['_new_member_'] = __new__\n        classdict['_use_args_'] = use_args\n        #\n        # convert future enum members into temporary _proto_members\n        for name in member_names:\n            value = classdict[name]\n            classdict[name] = _proto_member(value)\n        #\n        # house-keeping structures\n        classdict['_member_names_'] = []\n        classdict['_member_map_'] = {}\n        classdict['_value2member_map_'] = {}\n        classdict['_unhashable_values_'] = []\n        classdict['_member_type_'] = member_type\n        # now set the __repr__ for the value\n        classdict['_value_repr_'] = metacls._find_data_repr_(cls, bases)\n        #\n        # Flag structures (will be removed if final class is not a Flag\n        classdict['_boundary_'] = (\n                boundary\n                or getattr(first_enum, '_boundary_', None)\n                )\n        classdict['_flag_mask_'] = 0\n        classdict['_singles_mask_'] = 0\n        classdict['_all_bits_'] = 0\n        classdict['_inverted_'] = None\n        try:\n            exc = None\n            enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        except RuntimeError as e:\n            # any exceptions raised by member.__new__ will get converted to a\n            # RuntimeError, so get that original exception back and raise it instead\n            exc = e.__cause__ or e\n        if exc is not None:\n            raise exc\n        #\n        # update classdict with any changes made by __init_subclass__\n        classdict.update(enum_class.__dict__)\n        #\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        # however, if the method is defined in the Enum itself, don't replace\n        # it\n        #\n        # Also, special handling for ReprEnum\n        if ReprEnum is not None and ReprEnum in bases:\n            if member_type is object:\n                raise TypeError(\n                        'ReprEnum subclasses must be mixed with a data type (i.e.'\n                        ' int, str, float, etc.)'\n                        )\n            if '__format__' not in classdict:\n                enum_class.__format__ = member_type.__format__\n                classdict['__format__'] = enum_class.__format__\n            if '__str__' not in classdict:\n                method = member_type.__str__\n                if method is object.__str__:\n                    # if member_type does not define __str__, object.__str__ will use\n                    # its __repr__ instead, so we'll also use its __repr__\n                    method = member_type.__repr__\n                enum_class.__str__ = method\n                classdict['__str__'] = enum_class.__str__\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            if name not in classdict:\n                # check for mixin overrides before replacing\n                enum_method = getattr(first_enum, name)\n                found_method = getattr(enum_class, name)\n                object_method = getattr(object, name)\n                data_type_method = getattr(member_type, name)\n                if found_method in (data_type_method, object_method):\n                    setattr(enum_class, name, enum_method)\n        #\n        # for Flag, add __or__, __and__, __xor__, and __invert__\n        if Flag is not None and issubclass(enum_class, Flag):\n            for name in (\n                    '__or__', '__and__', '__xor__',\n                    '__ror__', '__rand__', '__rxor__',\n                    '__invert__'\n                ):\n                if name not in classdict:\n                    enum_method = getattr(Flag, name)\n                    setattr(enum_class, name, enum_method)\n                    classdict[name] = enum_method\n        #\n        # replace any other __new__ with our own (as long as Enum is not None,\n        # anyway) -- again, this is to support pickle\n        if Enum is not None:\n            # if the user defined their own __new__, save it before it gets\n            # clobbered in case they subclass later\n            if save_new:\n                enum_class.__new_member__ = __new__\n            enum_class.__new__ = Enum.__new__\n        #\n        # py3 support for definition order (helps keep py2/py3 code in sync)\n        #\n        # _order_ checking is spread out into three/four steps\n        # - if enum_class is a Flag:\n        #   - remove any non-single-bit flags from _order_\n        # - remove any aliases from _order_\n        # - check that _order_ and _member_names_ match\n        #\n        # step 1: ensure we have a list\n        if _order_ is not None:\n            if isinstance(_order_, str):\n                _order_ = _order_.replace(',', ' ').split()\n        #\n        # remove Flag structures if final class is not a Flag\n        if (\n                Flag is None and cls != 'Flag'\n                or Flag is not None and not issubclass(enum_class, Flag)\n            ):\n            delattr(enum_class, '_boundary_')\n            delattr(enum_class, '_flag_mask_')\n            delattr(enum_class, '_singles_mask_')\n            delattr(enum_class, '_all_bits_')\n            delattr(enum_class, '_inverted_')\n        elif Flag is not None and issubclass(enum_class, Flag):\n            # set correct __iter__\n            member_list = [m._value_ for m in enum_class]\n            if member_list != sorted(member_list):\n                enum_class._iter_member_ = enum_class._iter_member_by_def_\n            if _order_:\n                # _order_ step 2: remove any items from _order_ that are not single-bit\n                _order_ = [\n                        o\n                        for o in _order_\n                        if o not in enum_class._member_map_ or _is_single_bit(enum_class[o]._value_)\n                        ]\n        #\n        if _order_:\n            # _order_ step 3: remove aliases from _order_\n            _order_ = [\n                    o\n                    for o in _order_\n                    if (\n                        o not in enum_class._member_map_\n                        or\n                        (o in enum_class._member_map_ and o in enum_class._member_names_)\n                        )]\n            # _order_ step 4: verify that _order_ and _member_names_ match\n            if _order_ != enum_class._member_names_:\n                raise TypeError(\n                        'member order does not match _order_:\\n  %r\\n  %r'\n                        % (enum_class._member_names_, _order_)\n                        )\n\n        return enum_class\n\n    def __bool__(cls):\n        \"\"\"\n        classes/types should always be True.\n        \"\"\"\n        return True\n\n    def __call__(cls, value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None):\n        \"\"\"\n        Either returns an existing member, or creates a new enum class.\n\n        This method is used both when an enum class is given a value to match\n        to an enumeration member (i.e. Color(3)) and for the functional API\n        (i.e. Color = Enum('Color', names='RED GREEN BLUE')).\n\n        When used for the functional API:\n\n        `value` will be the name of the new class.\n\n        `names` should be either a string of white-space/comma delimited names\n        (values will start at `start`), or an iterator/mapping of name, value pairs.\n\n        `module` should be set to the module this class is being created in;\n        if it is not set, an attempt to find that module will be made, but if\n        it fails the class will not be picklable.\n\n        `qualname` should be set to the actual location this class can be found\n        at in its module; by default it is set to the global scope.  If this is\n        not correct, unpickling will fail in some circumstances.\n\n        `type`, if set, will be mixed in as the first base class.\n        \"\"\"\n        if names is None:  # simple value lookup\n            return cls.__new__(cls, value)\n        # otherwise, functional API: we're creating a new Enum type\n        return cls._create_(\n                value,\n                names,\n                module=module,\n                qualname=qualname,\n                type=type,\n                start=start,\n                boundary=boundary,\n                )\n\n    def __contains__(cls, member):\n        \"\"\"\n        Return True if member is a member of this enum\n        raises TypeError if member is not an enum member\n\n        note: in 3.12 TypeError will no longer be raised, and True will also be\n        returned if member is the value of a member in this enum\n        \"\"\"\n        if not isinstance(member, Enum):\n            import warnings\n            warnings.warn(\n                    \"in 3.12 __contains__ will no longer raise TypeError, but will return True or\\n\"\n                    \"False depending on whether the value is a member or the value of a member\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                    )\n            raise TypeError(\n                \"unsupported operand type(s) for 'in': '%s' and '%s'\" % (\n                    type(member).__qualname__, cls.__class__.__qualname__))\n        return isinstance(member, cls) and member._name_ in cls._member_map_\n\n    def __delattr__(cls, attr):\n        # nicer error message when someone tries to delete an attribute\n        # (see issue19025).\n        if attr in cls._member_map_:\n            raise AttributeError(\"%r cannot delete member %r.\" % (cls.__name__, attr))\n        super().__delattr__(attr)\n\n    def __dir__(cls):\n        interesting = set([\n                '__class__', '__contains__', '__doc__', '__getitem__',\n                '__iter__', '__len__', '__members__', '__module__',\n                '__name__', '__qualname__',\n                ]\n                + cls._member_names_\n                )\n        if cls._new_member_ is not object.__new__:\n            interesting.add('__new__')\n        if cls.__init_subclass__ is not object.__init_subclass__:\n            interesting.add('__init_subclass__')\n        if cls._member_type_ is object:\n            return sorted(interesting)\n        else:\n            # return whatever mixed-in data type has\n            return sorted(set(dir(cls._member_type_)) | interesting)\n\n    def __getattr__(cls, name):\n        \"\"\"\n        Return the enum member matching `name`\n\n        We use __getattr__ instead of descriptors or inserting into the enum\n        class' __dict__ in order to support `name` and `value` being both\n        properties for enum members (which live in the class' __dict__) and\n        enum members themselves.\n        \"\"\"\n        if _is_dunder(name):\n            raise AttributeError(name)\n        try:\n            return cls._member_map_[name]\n        except KeyError:\n            raise AttributeError(name) from None\n\n    def __getitem__(cls, name):\n        \"\"\"\n        Return the member matching `name`.\n        \"\"\"\n        return cls._member_map_[name]\n\n    def __iter__(cls):\n        \"\"\"\n        Return members in definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in cls._member_names_)\n\n    def __len__(cls):\n        \"\"\"\n        Return the number of members (no aliases)\n        \"\"\"\n        return len(cls._member_names_)\n\n    @bltns.property\n    def __members__(cls):\n        \"\"\"\n        Returns a mapping of member name->value.\n\n        This mapping lists all enum members, including aliases. Note that this\n        is a read-only view of the internal mapping.\n        \"\"\"\n        return MappingProxyType(cls._member_map_)\n\n    def __repr__(cls):\n        if Flag is not None and issubclass(cls, Flag):\n            return \"<flag %r>\" % cls.__name__\n        else:\n            return \"<enum %r>\" % cls.__name__\n\n    def __reversed__(cls):\n        \"\"\"\n        Return members in reverse definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in reversed(cls._member_names_))\n\n    def __setattr__(cls, name, value):\n        \"\"\"\n        Block attempts to reassign Enum members.\n\n        A simple assignment to the class namespace only changes one of the\n        several possible ways to get an Enum member from the Enum class,\n        resulting in an inconsistent Enumeration.\n        \"\"\"\n        member_map = cls.__dict__.get('_member_map_', {})\n        if name in member_map:\n            raise AttributeError('cannot reassign member %r' % (name, ))\n        super().__setattr__(name, value)\n\n    def _create_(cls, class_name, names, *, module=None, qualname=None, type=None, start=1, boundary=None):\n        \"\"\"\n        Convenience method to create a new Enum class.\n\n        `names` can be:\n\n        * A string containing member names, separated either with spaces or\n          commas.  Values are incremented by 1 from `start`.\n        * An iterable of member names.  Values are incremented by 1 from `start`.\n        * An iterable of (member name, value) pairs.\n        * A mapping of member name -> value pairs.\n        \"\"\"\n        metacls = cls.__class__\n        bases = (cls, ) if type is None else (type, cls)\n        _, first_enum = cls._get_mixins_(class_name, bases)\n        classdict = metacls.__prepare__(class_name, bases)\n\n        # special processing needed for names?\n        if isinstance(names, str):\n            names = names.replace(',', ' ').split()\n        if isinstance(names, (tuple, list)) and names and isinstance(names[0], str):\n            original_names, names = names, []\n            last_values = []\n            for count, name in enumerate(original_names):\n                value = first_enum._generate_next_value_(name, start, count, last_values[:])\n                last_values.append(value)\n                names.append((name, value))\n\n        # Here, names is either an iterable of (name, value) or a mapping.\n        for item in names:\n            if isinstance(item, str):\n                member_name, member_value = item, names[item]\n            else:\n                member_name, member_value = item\n            classdict[member_name] = member_value\n\n        # TODO: replace the frame hack if a blessed way to know the calling\n        # module is ever developed\n        if module is None:\n            try:\n                module = sys._getframe(2).f_globals['__name__']\n            except (AttributeError, ValueError, KeyError):\n                pass\n        if module is None:\n            _make_class_unpicklable(classdict)\n        else:\n            classdict['__module__'] = module\n        if qualname is not None:\n            classdict['__qualname__'] = qualname\n\n        return metacls.__new__(metacls, class_name, bases, classdict, boundary=boundary)\n\n    def _convert_(cls, name, module, filter, source=None, *, boundary=None, as_global=False):\n        \"\"\"\n        Create a new Enum subclass that replaces a collection of global constants\n        \"\"\"\n        # convert all constants from source (or module) that pass filter() to\n        # a new Enum called name, and export the enum and its members back to\n        # module;\n        # also, replace the __reduce_ex__ method so unpickling works in\n        # previous Python versions\n        module_globals = sys.modules[module].__dict__\n        if source:\n            source = source.__dict__\n        else:\n            source = module_globals\n        # _value2member_map_ is populated in the same order every time\n        # for a consistent reverse mapping of number to name when there\n        # are multiple names for the same number.\n        members = [\n                (name, value)\n                for name, value in source.items()\n                if filter(name)]\n        try:\n            # sort by value\n            members.sort(key=lambda t: (t[1], t[0]))\n        except TypeError:\n            # unless some values aren't comparable, in which case sort by name\n            members.sort(key=lambda t: t[0])\n        body = {t[0]: t[1] for t in members}\n        body['__module__'] = module\n        tmp_cls = type(name, (object, ), body)\n        cls = _simple_enum(etype=cls, boundary=boundary or KEEP)(tmp_cls)\n        if as_global:\n            global_enum(cls)\n        else:\n            sys.modules[cls.__module__].__dict__.update(cls.__members__)\n        module_globals[name] = cls\n        return cls\n\n    @classmethod\n    def _check_for_existing_members_(mcls, class_name, bases):\n        for chain in bases:\n            for base in chain.__mro__:\n                if isinstance(base, EnumType) and base._member_names_:\n                    raise TypeError(\n                            \"<enum %r> cannot extend %r\"\n                            % (class_name, base)\n                            )\n\n    @classmethod\n    def _get_mixins_(mcls, class_name, bases):\n        \"\"\"\n        Returns the type for creating enum members, and the first inherited\n        enum class.\n\n        bases: the tuple of bases that was given to __new__\n        \"\"\"\n        if not bases:\n            return object, Enum\n\n        mcls._check_for_existing_members_(class_name, bases)\n\n        # ensure final parent class is an Enum derivative, find any concrete\n        # data type, and check that Enum has no members\n        first_enum = bases[-1]\n        if not isinstance(first_enum, EnumType):\n            raise TypeError(\"new enumerations should be created as \"\n                    \"`EnumName([mixin_type, ...] [data_type,] enum_type)`\")\n        member_type = mcls._find_data_type_(class_name, bases) or object\n        return member_type, first_enum\n\n    @classmethod\n    def _find_data_repr_(mcls, class_name, bases):\n        for chain in bases:\n            for base in chain.__mro__:\n                if base is object:\n                    continue\n                elif isinstance(base, EnumType):\n                    # if we hit an Enum, use it's _value_repr_\n                    return base._value_repr_\n                elif '__repr__' in base.__dict__:\n                    # this is our data repr\n                    return base.__dict__['__repr__']\n        return None\n\n    @classmethod\n    def _find_data_type_(mcls, class_name, bases):\n        # a datatype has a __new__ method\n        data_types = set()\n        base_chain = set()\n        for chain in bases:\n            candidate = None\n            for base in chain.__mro__:\n                base_chain.add(base)\n                if base is object:\n                    continue\n                elif isinstance(base, EnumType):\n                    if base._member_type_ is not object:\n                        data_types.add(base._member_type_)\n                        break\n                elif '__new__' in base.__dict__ or '__dataclass_fields__' in base.__dict__:\n                    if isinstance(base, EnumType):\n                        continue\n                    data_types.add(candidate or base)\n                    break\n                else:\n                    candidate = candidate or base\n        if len(data_types) > 1:\n            raise TypeError('too many data types for %r: %r' % (class_name, data_types))\n        elif data_types:\n            return data_types.pop()\n        else:\n            return None\n\n    @classmethod\n    def _find_new_(mcls, classdict, member_type, first_enum):\n        \"\"\"\n        Returns the __new__ to be used for creating the enum members.\n\n        classdict: the class dictionary given to __new__\n        member_type: the data type whose __new__ will be used by default\n        first_enum: enumeration to check for an overriding __new__\n        \"\"\"\n        # now find the correct __new__, checking to see of one was defined\n        # by the user; also check earlier enum classes in case a __new__ was\n        # saved as __new_member__\n        __new__ = classdict.get('__new__', None)\n\n        # should __new__ be saved as __new_member__ later?\n        save_new = first_enum is not None and __new__ is not None\n\n        if __new__ is None:\n            # check all possibles for __new_member__ before falling back to\n            # __new__\n            for method in ('__new_member__', '__new__'):\n                for possible in (member_type, first_enum):\n                    target = getattr(possible, method, None)\n                    if target not in {\n                            None,\n                            None.__new__,\n                            object.__new__,\n                            Enum.__new__,\n                            }:\n                        __new__ = target\n                        break\n                if __new__ is not None:\n                    break\n            else:\n                __new__ = object.__new__\n\n        # if a non-object.__new__ is used then whatever value/tuple was\n        # assigned to the enum member name will be passed to __new__ and to the\n        # new enum member's __init__\n        if first_enum is None or __new__ in (Enum.__new__, object.__new__):\n            use_args = False\n        else:\n            use_args = True\n        return __new__, save_new, use_args\nEnumMeta = EnumType\n\n\nclass Enum(metaclass=EnumType):\n    \"\"\"\n    Create a collection of name/value pairs.\n\n    Example enumeration:\n\n    >>> class Color(Enum):\n    ...     RED = 1\n    ...     BLUE = 2\n    ...     GREEN = 3\n\n    Access them by:\n\n    - attribute access::\n\n    >>> Color.RED\n    <Color.RED: 1>\n\n    - value lookup:\n\n    >>> Color(1)\n    <Color.RED: 1>\n\n    - name lookup:\n\n    >>> Color['RED']\n    <Color.RED: 1>\n\n    Enumerations can be iterated over, and know how many members they have:\n\n    >>> len(Color)\n    3\n\n    >>> list(Color)\n    [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\n    Methods can be added to enumerations, and members can have their own\n    attributes -- see the documentation for details.\n    \"\"\"\n\n    def __new__(cls, value):\n        # all enum instances are actually created during class construction\n        # without calling this method; this method is called by the metaclass'\n        # __call__ (i.e. Color(3) ), and by pickle\n        if type(value) is cls:\n            # For lookups like Color(Color.RED)\n            return value\n        # by-value search for a matching enum member\n        # see if it's in the reverse mapping (for hashable values)\n        try:\n            return cls._value2member_map_[value]\n        except KeyError:\n            # Not found, no need to do long O(n) search\n            pass\n        except TypeError:\n            # not there, now do long search -- O(n) behavior\n            for member in cls._member_map_.values():\n                if member._value_ == value:\n                    return member\n        # still not found -- try _missing_ hook\n        try:\n            exc = None\n            result = cls._missing_(value)\n        except Exception as e:\n            exc = e\n            result = None\n        try:\n            if isinstance(result, cls):\n                return result\n            elif (\n                    Flag is not None and issubclass(cls, Flag)\n                    and cls._boundary_ is EJECT and isinstance(result, int)\n                ):\n                return result\n            else:\n                ve_exc = ValueError(\"%r is not a valid %s\" % (value, cls.__qualname__))\n                if result is None and exc is None:\n                    raise ve_exc\n                elif exc is None:\n                    exc = TypeError(\n                            'error in %s._missing_: returned %r instead of None or a valid member'\n                            % (cls.__name__, result)\n                            )\n                if not isinstance(exc, ValueError):\n                    exc.__context__ = ve_exc\n                raise exc\n        finally:\n            # ensure all variables that could hold an exception are destroyed\n            exc = None\n            ve_exc = None\n\n    def __init__(self, *args, **kwds):\n        pass\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_values: the list of values assigned\n        \"\"\"\n        if not last_values:\n            return start\n        try:\n            last = last_values[-1]\n            last_values.sort()\n            if last == last_values[-1]:\n                # no difference between old and new methods\n                return last + 1\n            else:\n                # trigger old method (with warning)\n                raise TypeError\n        except TypeError:\n            import warnings\n            warnings.warn(\n                    \"In 3.13 the default `auto()`/`_generate_next_value_` will require all values to be sortable and support adding +1\\n\"\n                    \"and the value returned will be the largest value in the enum incremented by 1\",\n                    DeprecationWarning,\n                    stacklevel=3,\n                    )\n            for v in reversed(last_values):\n                try:\n                    return v + 1\n                except TypeError:\n                    pass\n            return start\n\n    @classmethod\n    def _missing_(cls, value):\n        return None\n\n    def __repr__(self):\n        v_repr = self.__class__._value_repr_ or repr\n        return \"<%s.%s: %s>\" % (self.__class__.__name__, self._name_, v_repr(self._value_))\n\n    def __str__(self):\n        return \"%s.%s\" % (self.__class__.__name__, self._name_, )\n\n    def __dir__(self):\n        \"\"\"\n        Returns all members and all public methods\n        \"\"\"\n        if self.__class__._member_type_ is object:\n            interesting = set(['__class__', '__doc__', '__eq__', '__hash__', '__module__', 'name', 'value'])\n        else:\n            interesting = set(object.__dir__(self))\n        for name in getattr(self, '__dict__', []):\n            if name[0] != '_':\n                interesting.add(name)\n        for cls in self.__class__.mro():\n            for name, obj in cls.__dict__.items():\n                if name[0] == '_':\n                    continue\n                if isinstance(obj, property):\n                    # that's an enum.property\n                    if obj.fget is not None or name not in self._member_map_:\n                        interesting.add(name)\n                    else:\n                        # in case it was added by `dir(self)`\n                        interesting.discard(name)\n                else:\n                    interesting.add(name)\n        names = sorted(\n                set(['__class__', '__doc__', '__eq__', '__hash__', '__module__'])\n                | interesting\n                )\n        return names\n\n    def __format__(self, format_spec):\n        return str.__format__(str(self), format_spec)\n\n    def __hash__(self):\n        return hash(self._name_)\n\n    def __reduce_ex__(self, proto):\n        return self.__class__, (self._value_, )\n\n    def __deepcopy__(self,memo):\n        return self\n\n    def __copy__(self):\n        return self\n\n    # enum.property is used to provide access to the `name` and\n    # `value` attributes of enum members while keeping some measure of\n    # protection from modification, while still allowing for an enumeration\n    # to have members named `name` and `value`.  This works because enumeration\n    # members are not set directly on the enum class; they are kept in a\n    # separate structure, _member_map_, which is where enum.property looks for\n    # them\n\n    @property\n    def name(self):\n        \"\"\"The name of the Enum member.\"\"\"\n        return self._name_\n\n    @property\n    def value(self):\n        \"\"\"The value of the Enum member.\"\"\"\n        return self._value_\n\n\nclass ReprEnum(Enum):\n    \"\"\"\n    Only changes the repr(), leaving str() and format() to the mixed-in type.\n    \"\"\"\n\n\nclass IntEnum(int, ReprEnum):\n    \"\"\"\n    Enum where members are also (and must be) ints\n    \"\"\"\n\n\nclass StrEnum(str, ReprEnum):\n    \"\"\"\n    Enum where members are also (and must be) strings\n    \"\"\"\n\n    def __new__(cls, *values):\n        \"values must already be of type `str`\"\n        if len(values) > 3:\n            raise TypeError('too many arguments for str(): %r' % (values, ))\n        if len(values) == 1:\n            # it must be a string\n            if not isinstance(values[0], str):\n                raise TypeError('%r is not a string' % (values[0], ))\n        if len(values) >= 2:\n            # check that encoding argument is a string\n            if not isinstance(values[1], str):\n                raise TypeError('encoding must be a string, not %r' % (values[1], ))\n        if len(values) == 3:\n            # check that errors argument is a string\n            if not isinstance(values[2], str):\n                raise TypeError('errors must be a string, not %r' % (values[2]))\n        value = str(*values)\n        member = str.__new__(cls, value)\n        member._value_ = value\n        return member\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Return the lower-cased version of the member name.\n        \"\"\"\n        return name.lower()\n\n\ndef pickle_by_global_name(self, proto):\n    # should not be used with Flag-type enums\n    return self.name\n_reduce_ex_by_global_name = pickle_by_global_name\n\ndef pickle_by_enum_name(self, proto):\n    # should not be used with Flag-type enums\n    return getattr, (self.__class__, self._name_)\n\nclass FlagBoundary(StrEnum):\n    \"\"\"\n    control how out of range values are handled\n    \"strict\" -> error is raised             [default for Flag]\n    \"conform\" -> extra bits are discarded\n    \"eject\" -> lose flag status\n    \"keep\" -> keep flag status and all bits [default for IntFlag]\n    \"\"\"\n    STRICT = auto()\n    CONFORM = auto()\n    EJECT = auto()\n    KEEP = auto()\nSTRICT, CONFORM, EJECT, KEEP = FlagBoundary\n\n\nclass Flag(Enum, boundary=STRICT):\n    \"\"\"\n    Support for flags\n    \"\"\"\n\n    _numeric_repr_ = repr\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_values: the last value assigned or None\n        \"\"\"\n        if not count:\n            return start if start is not None else 1\n        last_value = max(last_values)\n        try:\n            high_bit = _high_bit(last_value)\n        except Exception:\n            raise TypeError('invalid flag value %r' % last_value) from None\n        return 2 ** (high_bit+1)\n\n    @classmethod\n    def _iter_member_by_value_(cls, value):\n        \"\"\"\n        Extract all members from the value in definition (i.e. increasing value) order.\n        \"\"\"\n        for val in _iter_bits_lsb(value & cls._flag_mask_):\n            yield cls._value2member_map_.get(val)\n\n    _iter_member_ = _iter_member_by_value_\n\n    @classmethod\n    def _iter_member_by_def_(cls, value):\n        \"\"\"\n        Extract all members from the value in definition order.\n        \"\"\"\n        yield from sorted(\n                cls._iter_member_by_value_(value),\n                key=lambda m: m._sort_order_,\n                )\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"\n        Create a composite member containing all canonical members present in `value`.\n\n        If non-member values are present, result depends on `_boundary_` setting.\n        \"\"\"\n        if not isinstance(value, int):\n            raise ValueError(\n                    \"%r is not a valid %s\" % (value, cls.__qualname__)\n                    )\n        # check boundaries\n        # - value must be in range (e.g. -16 <-> +15, i.e. ~15 <-> 15)\n        # - value must not include any skipped flags (e.g. if bit 2 is not\n        #   defined, then 0d10 is invalid)\n        flag_mask = cls._flag_mask_\n        singles_mask = cls._singles_mask_\n        all_bits = cls._all_bits_\n        neg_value = None\n        if (\n                not ~all_bits <= value <= all_bits\n                or value & (all_bits ^ flag_mask)\n            ):\n            if cls._boundary_ is STRICT:\n                max_bits = max(value.bit_length(), flag_mask.bit_length())\n                raise ValueError(\n                        \"%r invalid value %r\\n    given %s\\n  allowed %s\" % (\n                            cls, value, bin(value, max_bits), bin(flag_mask, max_bits),\n                            ))\n            elif cls._boundary_ is CONFORM:\n                value = value & flag_mask\n            elif cls._boundary_ is EJECT:\n                return value\n            elif cls._boundary_ is KEEP:\n                if value < 0:\n                    value = (\n                            max(all_bits+1, 2**(value.bit_length()))\n                            + value\n                            )\n            else:\n                raise ValueError(\n                        '%r unknown flag boundary %r' % (cls, cls._boundary_, )\n                        )\n        if value < 0:\n            neg_value = value\n            value = all_bits + 1 + value\n        # get members and unknown\n        unknown = value & ~flag_mask\n        aliases = value & ~singles_mask\n        member_value = value & singles_mask\n        if unknown and cls._boundary_ is not KEEP:\n            raise ValueError(\n                    '%s(%r) -->  unknown values %r [%s]'\n                    % (cls.__name__, value, unknown, bin(unknown))\n                    )\n        # normal Flag?\n        if cls._member_type_ is object:\n            # construct a singleton enum pseudo-member\n            pseudo_member = object.__new__(cls)\n        else:\n            pseudo_member = cls._member_type_.__new__(cls, value)\n        if not hasattr(pseudo_member, '_value_'):\n            pseudo_member._value_ = value\n        if member_value or aliases:\n            members = []\n            combined_value = 0\n            for m in cls._iter_member_(member_value):\n                members.append(m)\n                combined_value |= m._value_\n            if aliases:\n                value = member_value | aliases\n                for n, pm in cls._member_map_.items():\n                    if pm not in members and pm._value_ and pm._value_ & value == pm._value_:\n                        members.append(pm)\n                        combined_value |= pm._value_\n            unknown = value ^ combined_value\n            pseudo_member._name_ = '|'.join([m._name_ for m in members])\n            if not combined_value:\n                pseudo_member._name_ = None\n            elif unknown and cls._boundary_ is STRICT:\n                raise ValueError('%r: no members with value %r' % (cls, unknown))\n            elif unknown:\n                pseudo_member._name_ += '|%s' % cls._numeric_repr_(unknown)\n        else:\n            pseudo_member._name_ = None\n        # use setdefault in case another thread already created a composite\n        # with this value\n        # note: zero is a special case -- always add it\n        pseudo_member = cls._value2member_map_.setdefault(value, pseudo_member)\n        if neg_value is not None:\n            cls._value2member_map_[neg_value] = pseudo_member\n        return pseudo_member\n\n    def __contains__(self, other):\n        \"\"\"\n        Returns True if self has at least the same flags set as other.\n        \"\"\"\n        if not isinstance(other, self.__class__):\n            raise TypeError(\n                \"unsupported operand type(s) for 'in': %r and %r\" % (\n                    type(other).__qualname__, self.__class__.__qualname__))\n        return other._value_ & self._value_ == other._value_\n\n    def __iter__(self):\n        \"\"\"\n        Returns flags in definition order.\n        \"\"\"\n        yield from self._iter_member_(self._value_)\n\n    def __len__(self):\n        return self._value_.bit_count()\n\n    def __repr__(self):\n        cls_name = self.__class__.__name__\n        v_repr = self.__class__._value_repr_ or repr\n        if self._name_ is None:\n            return \"<%s: %s>\" % (cls_name, v_repr(self._value_))\n        else:\n            return \"<%s.%s: %s>\" % (cls_name, self._name_, v_repr(self._value_))\n\n    def __str__(self):\n        cls_name = self.__class__.__name__\n        if self._name_ is None:\n            return '%s(%r)' % (cls_name, self._value_)\n        else:\n            return \"%s.%s\" % (cls_name, self._name_)\n\n    def __bool__(self):\n        return bool(self._value_)\n\n    def __or__(self, other):\n        if isinstance(other, self.__class__):\n            other = other._value_\n        elif self._member_type_ is not object and isinstance(other, self._member_type_):\n            other = other\n        else:\n            return NotImplemented\n        value = self._value_\n        return self.__class__(value | other)\n\n    def __and__(self, other):\n        if isinstance(other, self.__class__):\n            other = other._value_\n        elif self._member_type_ is not object and isinstance(other, self._member_type_):\n            other = other\n        else:\n            return NotImplemented\n        value = self._value_\n        return self.__class__(value & other)\n\n    def __xor__(self, other):\n        if isinstance(other, self.__class__):\n            other = other._value_\n        elif self._member_type_ is not object and isinstance(other, self._member_type_):\n            other = other\n        else:\n            return NotImplemented\n        value = self._value_\n        return self.__class__(value ^ other)\n\n    def __invert__(self):\n        if self._inverted_ is None:\n            if self._boundary_ in (EJECT, KEEP):\n                self._inverted_ = self.__class__(~self._value_)\n            else:\n                self._inverted_ = self.__class__(self._singles_mask_ & ~self._value_)\n        return self._inverted_\n\n    __rand__ = __and__\n    __ror__ = __or__\n    __rxor__ = __xor__\n\n\nclass IntFlag(int, ReprEnum, Flag, boundary=KEEP):\n    \"\"\"\n    Support for integer-based Flags\n    \"\"\"\n\n\ndef _high_bit(value):\n    \"\"\"\n    returns index of highest bit, or -1 if value is zero or negative\n    \"\"\"\n    return value.bit_length() - 1\n\ndef unique(enumeration):\n    \"\"\"\n    Class decorator for enumerations ensuring unique member values.\n    \"\"\"\n    duplicates = []\n    for name, member in enumeration.__members__.items():\n        if name != member.name:\n            duplicates.append((name, member.name))\n    if duplicates:\n        alias_details = ', '.join(\n                [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n        raise ValueError('duplicate values found in %r: %s' %\n                (enumeration, alias_details))\n    return enumeration\n\ndef _power_of_two(value):\n    if value < 1:\n        return False\n    return value == 2 ** _high_bit(value)\n\ndef global_enum_repr(self):\n    \"\"\"\n    use module.enum_name instead of class.enum_name\n\n    the module is the last module in case of a multi-module name\n    \"\"\"\n    module = self.__class__.__module__.split('.')[-1]\n    return '%s.%s' % (module, self._name_)\n\ndef global_flag_repr(self):\n    \"\"\"\n    use module.flag_name instead of class.flag_name\n\n    the module is the last module in case of a multi-module name\n    \"\"\"\n    module = self.__class__.__module__.split('.')[-1]\n    cls_name = self.__class__.__name__\n    if self._name_ is None:\n        return \"%s.%s(%r)\" % (module, cls_name, self._value_)\n    if _is_single_bit(self):\n        return '%s.%s' % (module, self._name_)\n    if self._boundary_ is not FlagBoundary.KEEP:\n        return '|'.join(['%s.%s' % (module, name) for name in self.name.split('|')])\n    else:\n        name = []\n        for n in self._name_.split('|'):\n            if n[0].isdigit():\n                name.append(n)\n            else:\n                name.append('%s.%s' % (module, n))\n        return '|'.join(name)\n\ndef global_str(self):\n    \"\"\"\n    use enum_name instead of class.enum_name\n    \"\"\"\n    if self._name_ is None:\n        cls_name = self.__class__.__name__\n        return \"%s(%r)\" % (cls_name, self._value_)\n    else:\n        return self._name_\n\ndef global_enum(cls, update_str=False):\n    \"\"\"\n    decorator that makes the repr() of an enum member reference its module\n    instead of its class; also exports all members to the enum's module's\n    global namespace\n    \"\"\"\n    if issubclass(cls, Flag):\n        cls.__repr__ = global_flag_repr\n    else:\n        cls.__repr__ = global_enum_repr\n    if not issubclass(cls, ReprEnum) or update_str:\n        cls.__str__ = global_str\n    sys.modules[cls.__module__].__dict__.update(cls.__members__)\n    return cls\n\ndef _simple_enum(etype=Enum, *, boundary=None, use_args=None):\n    \"\"\"\n    Class decorator that converts a normal class into an :class:`Enum`.  No\n    safety checks are done, and some advanced behavior (such as\n    :func:`__init_subclass__`) is not available.  Enum creation can be faster\n    using :func:`simple_enum`.\n\n        >>> from enum import Enum, _simple_enum\n        >>> @_simple_enum(Enum)\n        ... class Color:\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> Color\n        <enum 'Color'>\n    \"\"\"\n    def convert_class(cls):\n        nonlocal use_args\n        cls_name = cls.__name__\n        if use_args is None:\n            use_args = etype._use_args_\n        __new__ = cls.__dict__.get('__new__')\n        if __new__ is not None:\n            new_member = __new__.__func__\n        else:\n            new_member = etype._member_type_.__new__\n        attrs = {}\n        body = {}\n        if __new__ is not None:\n            body['__new_member__'] = new_member\n        body['_new_member_'] = new_member\n        body['_use_args_'] = use_args\n        body['_generate_next_value_'] = gnv = etype._generate_next_value_\n        body['_member_names_'] = member_names = []\n        body['_member_map_'] = member_map = {}\n        body['_value2member_map_'] = value2member_map = {}\n        body['_unhashable_values_'] = []\n        body['_member_type_'] = member_type = etype._member_type_\n        body['_value_repr_'] = etype._value_repr_\n        if issubclass(etype, Flag):\n            body['_boundary_'] = boundary or etype._boundary_\n            body['_flag_mask_'] = None\n            body['_all_bits_'] = None\n            body['_singles_mask_'] = None\n            body['_inverted_'] = None\n            body['__or__'] = Flag.__or__\n            body['__xor__'] = Flag.__xor__\n            body['__and__'] = Flag.__and__\n            body['__ror__'] = Flag.__ror__\n            body['__rxor__'] = Flag.__rxor__\n            body['__rand__'] = Flag.__rand__\n            body['__invert__'] = Flag.__invert__\n        for name, obj in cls.__dict__.items():\n            if name in ('__dict__', '__weakref__'):\n                continue\n            if _is_dunder(name) or _is_private(cls_name, name) or _is_sunder(name) or _is_descriptor(obj):\n                body[name] = obj\n            else:\n                attrs[name] = obj\n        if cls.__dict__.get('__doc__') is None:\n            body['__doc__'] = 'An enumeration.'\n        #\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        # however, if the method is defined in the Enum itself, don't replace\n        # it\n        enum_class = type(cls_name, (etype, ), body, boundary=boundary, _simple=True)\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            if name not in body:\n                # check for mixin overrides before replacing\n                enum_method = getattr(etype, name)\n                found_method = getattr(enum_class, name)\n                object_method = getattr(object, name)\n                data_type_method = getattr(member_type, name)\n                if found_method in (data_type_method, object_method):\n                    setattr(enum_class, name, enum_method)\n        gnv_last_values = []\n        if issubclass(enum_class, Flag):\n            # Flag / IntFlag\n            single_bits = multi_bits = 0\n            for name, value in attrs.items():\n                if isinstance(value, auto) and auto.value is _auto_null:\n                    value = gnv(name, 1, len(member_names), gnv_last_values)\n                if value in value2member_map:\n                    # an alias to an existing member\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = value2member_map[value]\n                else:\n                    # create the member\n                    if use_args:\n                        if not isinstance(value, tuple):\n                            value = (value, )\n                        member = new_member(enum_class, *value)\n                        value = value[0]\n                    else:\n                        member = new_member(enum_class)\n                    if __new__ is None:\n                        member._value_ = value\n                    member._name_ = name\n                    member.__objclass__ = enum_class\n                    member.__init__(value)\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = member\n                    member._sort_order_ = len(member_names)\n                    value2member_map[value] = member\n                    if _is_single_bit(value):\n                        # not a multi-bit alias, record in _member_names_ and _flag_mask_\n                        member_names.append(name)\n                        single_bits |= value\n                    else:\n                        multi_bits |= value\n                    gnv_last_values.append(value)\n            enum_class._flag_mask_ = single_bits | multi_bits\n            enum_class._singles_mask_ = single_bits\n            enum_class._all_bits_ = 2 ** ((single_bits|multi_bits).bit_length()) - 1\n            # set correct __iter__\n            member_list = [m._value_ for m in enum_class]\n            if member_list != sorted(member_list):\n                enum_class._iter_member_ = enum_class._iter_member_by_def_\n        else:\n            # Enum / IntEnum / StrEnum\n            for name, value in attrs.items():\n                if isinstance(value, auto):\n                    if value.value is _auto_null:\n                        value.value = gnv(name, 1, len(member_names), gnv_last_values)\n                    value = value.value\n                if value in value2member_map:\n                    # an alias to an existing member\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = value2member_map[value]\n                else:\n                    # create the member\n                    if use_args:\n                        if not isinstance(value, tuple):\n                            value = (value, )\n                        member = new_member(enum_class, *value)\n                        value = value[0]\n                    else:\n                        member = new_member(enum_class)\n                    if __new__ is None:\n                        member._value_ = value\n                    member._name_ = name\n                    member.__objclass__ = enum_class\n                    member.__init__(value)\n                    member._sort_order_ = len(member_names)\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = member\n                    value2member_map[value] = member\n                    member_names.append(name)\n                    gnv_last_values.append(value)\n        if '__new__' in body:\n            enum_class.__new_member__ = enum_class.__new__\n        enum_class.__new__ = Enum.__new__\n        return enum_class\n    return convert_class\n\n@_simple_enum(StrEnum)\nclass EnumCheck:\n    \"\"\"\n    various conditions to check an enumeration for\n    \"\"\"\n    CONTINUOUS = \"no skipped integer values\"\n    NAMED_FLAGS = \"multi-flag aliases may not contain unnamed flags\"\n    UNIQUE = \"one name per value\"\nCONTINUOUS, NAMED_FLAGS, UNIQUE = EnumCheck\n\n\nclass verify:\n    \"\"\"\n    Check an enumeration for various constraints. (see EnumCheck)\n    \"\"\"\n    def __init__(self, *checks):\n        self.checks = checks\n    def __call__(self, enumeration):\n        checks = self.checks\n        cls_name = enumeration.__name__\n        if Flag is not None and issubclass(enumeration, Flag):\n            enum_type = 'flag'\n        elif issubclass(enumeration, Enum):\n            enum_type = 'enum'\n        else:\n            raise TypeError(\"the 'verify' decorator only works with Enum and Flag\")\n        for check in checks:\n            if check is UNIQUE:\n                # check for duplicate names\n                duplicates = []\n                for name, member in enumeration.__members__.items():\n                    if name != member.name:\n                        duplicates.append((name, member.name))\n                if duplicates:\n                    alias_details = ', '.join(\n                            [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n                    raise ValueError('aliases found in %r: %s' %\n                            (enumeration, alias_details))\n            elif check is CONTINUOUS:\n                values = set(e.value for e in enumeration)\n                if len(values) < 2:\n                    continue\n                low, high = min(values), max(values)\n                missing = []\n                if enum_type == 'flag':\n                    # check for powers of two\n                    for i in range(_high_bit(low)+1, _high_bit(high)):\n                        if 2**i not in values:\n                            missing.append(2**i)\n                elif enum_type == 'enum':\n                    # check for powers of one\n                    for i in range(low+1, high):\n                        if i not in values:\n                            missing.append(i)\n                else:\n                    raise Exception('verify: unknown type %r' % enum_type)\n                if missing:\n                    raise ValueError(('invalid %s %r: missing values %s' % (\n                            enum_type, cls_name, ', '.join((str(m) for m in missing)))\n                            )[:256])\n                            # limit max length to protect against DOS attacks\n            elif check is NAMED_FLAGS:\n                # examine each alias and check for unnamed flags\n                member_names = enumeration._member_names_\n                member_values = [m.value for m in enumeration]\n                missing_names = []\n                missing_value = 0\n                for name, alias in enumeration._member_map_.items():\n                    if name in member_names:\n                        # not an alias\n                        continue\n                    if alias.value < 0:\n                        # negative numbers are not checked\n                        continue\n                    values = list(_iter_bits_lsb(alias.value))\n                    missed = [v for v in values if v not in member_values]\n                    if missed:\n                        missing_names.append(name)\n                        missing_value |= reduce(_or_, missed)\n                if missing_names:\n                    if len(missing_names) == 1:\n                        alias = 'alias %s is missing' % missing_names[0]\n                    else:\n                        alias = 'aliases %s and %s are missing' % (\n                                ', '.join(missing_names[:-1]), missing_names[-1]\n                                )\n                    if _is_single_bit(missing_value):\n                        value = 'value 0x%x' % missing_value\n                    else:\n                        value = 'combined values of 0x%x' % missing_value\n                    raise ValueError(\n                            'invalid Flag %r: %s %s [use enum.show_flag_values(value) for details]'\n                            % (cls_name, alias, value)\n                            )\n        return enumeration\n\ndef _test_simple_enum(checked_enum, simple_enum):\n    \"\"\"\n    A function that can be used to test an enum created with :func:`_simple_enum`\n    against the version created by subclassing :class:`Enum`::\n\n        >>> from enum import Enum, _simple_enum, _test_simple_enum\n        >>> @_simple_enum(Enum)\n        ... class Color:\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> class CheckedColor(Enum):\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> _test_simple_enum(CheckedColor, Color)\n\n    If differences are found, a :exc:`TypeError` is raised.\n    \"\"\"\n    failed = []\n    if checked_enum.__dict__ != simple_enum.__dict__:\n        checked_dict = checked_enum.__dict__\n        checked_keys = list(checked_dict.keys())\n        simple_dict = simple_enum.__dict__\n        simple_keys = list(simple_dict.keys())\n        member_names = set(\n                list(checked_enum._member_map_.keys())\n                + list(simple_enum._member_map_.keys())\n                )\n        for key in set(checked_keys + simple_keys):\n            if key in ('__module__', '_member_map_', '_value2member_map_', '__doc__'):\n                # keys known to be different, or very long\n                continue\n            elif key in member_names:\n                # members are checked below\n                continue\n            elif key not in simple_keys:\n                failed.append(\"missing key: %r\" % (key, ))\n            elif key not in checked_keys:\n                failed.append(\"extra key:   %r\" % (key, ))\n            else:\n                checked_value = checked_dict[key]\n                simple_value = simple_dict[key]\n                if callable(checked_value) or isinstance(checked_value, bltns.property):\n                    continue\n                if key == '__doc__':\n                    # remove all spaces/tabs\n                    compressed_checked_value = checked_value.replace(' ','').replace('\\t','')\n                    compressed_simple_value = simple_value.replace(' ','').replace('\\t','')\n                    if compressed_checked_value != compressed_simple_value:\n                        failed.append(\"%r:\\n         %s\\n         %s\" % (\n                                key,\n                                \"checked -> %r\" % (checked_value, ),\n                                \"simple  -> %r\" % (simple_value, ),\n                                ))\n                elif checked_value != simple_value:\n                    failed.append(\"%r:\\n         %s\\n         %s\" % (\n                            key,\n                            \"checked -> %r\" % (checked_value, ),\n                            \"simple  -> %r\" % (simple_value, ),\n                            ))\n        failed.sort()\n        for name in member_names:\n            failed_member = []\n            if name not in simple_keys:\n                failed.append('missing member from simple enum: %r' % name)\n            elif name not in checked_keys:\n                failed.append('extra member in simple enum: %r' % name)\n            else:\n                checked_member_dict = checked_enum[name].__dict__\n                checked_member_keys = list(checked_member_dict.keys())\n                simple_member_dict = simple_enum[name].__dict__\n                simple_member_keys = list(simple_member_dict.keys())\n                for key in set(checked_member_keys + simple_member_keys):\n                    if key in ('__module__', '__objclass__', '_inverted_'):\n                        # keys known to be different or absent\n                        continue\n                    elif key not in simple_member_keys:\n                        failed_member.append(\"missing key %r not in the simple enum member %r\" % (key, name))\n                    elif key not in checked_member_keys:\n                        failed_member.append(\"extra key %r in simple enum member %r\" % (key, name))\n                    else:\n                        checked_value = checked_member_dict[key]\n                        simple_value = simple_member_dict[key]\n                        if checked_value != simple_value:\n                            failed_member.append(\"%r:\\n         %s\\n         %s\" % (\n                                    key,\n                                    \"checked member -> %r\" % (checked_value, ),\n                                    \"simple member  -> %r\" % (simple_value, ),\n                                    ))\n            if failed_member:\n                failed.append('%r member mismatch:\\n      %s' % (\n                        name, '\\n      '.join(failed_member),\n                        ))\n        for method in (\n                '__str__', '__repr__', '__reduce_ex__', '__format__',\n                '__getnewargs_ex__', '__getnewargs__', '__reduce_ex__', '__reduce__'\n            ):\n            if method in simple_keys and method in checked_keys:\n                # cannot compare functions, and it exists in both, so we're good\n                continue\n            elif method not in simple_keys and method not in checked_keys:\n                # method is inherited -- check it out\n                checked_method = getattr(checked_enum, method, None)\n                simple_method = getattr(simple_enum, method, None)\n                if hasattr(checked_method, '__func__'):\n                    checked_method = checked_method.__func__\n                    simple_method = simple_method.__func__\n                if checked_method != simple_method:\n                    failed.append(\"%r:  %-30s %s\" % (\n                            method,\n                            \"checked -> %r\" % (checked_method, ),\n                            \"simple -> %r\" % (simple_method, ),\n                            ))\n            else:\n                # if the method existed in only one of the enums, it will have been caught\n                # in the first checks above\n                pass\n    if failed:\n        raise TypeError('enum mismatch:\\n   %s' % '\\n   '.join(failed))\n\ndef _old_convert_(etype, name, module, filter, source=None, *, boundary=None):\n    \"\"\"\n    Create a new Enum subclass that replaces a collection of global constants\n    \"\"\"\n    # convert all constants from source (or module) that pass filter() to\n    # a new Enum called name, and export the enum and its members back to\n    # module;\n    # also, replace the __reduce_ex__ method so unpickling works in\n    # previous Python versions\n    module_globals = sys.modules[module].__dict__\n    if source:\n        source = source.__dict__\n    else:\n        source = module_globals\n    # _value2member_map_ is populated in the same order every time\n    # for a consistent reverse mapping of number to name when there\n    # are multiple names for the same number.\n    members = [\n            (name, value)\n            for name, value in source.items()\n            if filter(name)]\n    try:\n        # sort by value\n        members.sort(key=lambda t: (t[1], t[0]))\n    except TypeError:\n        # unless some values aren't comparable, in which case sort by name\n        members.sort(key=lambda t: t[0])\n    cls = etype(name, members, module=module, boundary=boundary or KEEP)\n    return cls\n\n_stdlib_enums = IntEnum, StrEnum, IntFlag\n", 2042], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py": ["import _signal\nfrom _signal import *\nfrom enum import IntEnum as _IntEnum\n\n_globals = globals()\n\n_IntEnum._convert_(\n        'Signals', __name__,\n        lambda name:\n            name.isupper()\n            and (name.startswith('SIG') and not name.startswith('SIG_'))\n            or name.startswith('CTRL_'))\n\n_IntEnum._convert_(\n        'Handlers', __name__,\n        lambda name: name in ('SIG_DFL', 'SIG_IGN'))\n\nif 'pthread_sigmask' in _globals:\n    _IntEnum._convert_(\n            'Sigmasks', __name__,\n            lambda name: name in ('SIG_BLOCK', 'SIG_UNBLOCK', 'SIG_SETMASK'))\n\n\ndef _int_to_enum(value, enum_klass):\n    \"\"\"Convert a numeric value to an IntEnum member.\n    If it's not a known member, return the numeric value itself.\n    \"\"\"\n    try:\n        return enum_klass(value)\n    except ValueError:\n        return value\n\n\ndef _enum_to_int(value):\n    \"\"\"Convert an IntEnum member to a numeric value.\n    If it's not an IntEnum member return the value itself.\n    \"\"\"\n    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return value\n\n\n# Similar to functools.wraps(), but only assign __doc__.\n# __module__ should be preserved,\n# __name__ and __qualname__ are already fine,\n# __annotations__ is not set.\ndef _wraps(wrapped):\n    def decorator(wrapper):\n        wrapper.__doc__ = wrapped.__doc__\n        return wrapper\n    return decorator\n\n@_wraps(_signal.signal)\ndef signal(signalnum, handler):\n    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n    return _int_to_enum(handler, Handlers)\n\n\n@_wraps(_signal.getsignal)\ndef getsignal(signalnum):\n    handler = _signal.getsignal(signalnum)\n    return _int_to_enum(handler, Handlers)\n\n\nif 'pthread_sigmask' in _globals:\n    @_wraps(_signal.pthread_sigmask)\n    def pthread_sigmask(how, mask):\n        sigs_set = _signal.pthread_sigmask(how, mask)\n        return set(_int_to_enum(x, Signals) for x in sigs_set)\n\n\nif 'sigpending' in _globals:\n    @_wraps(_signal.sigpending)\n    def sigpending():\n        return {_int_to_enum(x, Signals) for x in _signal.sigpending()}\n\n\nif 'sigwait' in _globals:\n    @_wraps(_signal.sigwait)\n    def sigwait(sigset):\n        retsig = _signal.sigwait(sigset)\n        return _int_to_enum(retsig, Signals)\n\n\nif 'valid_signals' in _globals:\n    @_wraps(_signal.valid_signals)\n    def valid_signals():\n        return {_int_to_enum(x, Signals) for x in _signal.valid_signals()}\n\n\ndel _globals, _wraps\n", 92], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py": ["__all__ = ()\n\nimport reprlib\nfrom _thread import get_ident\n\nfrom . import format_helpers\n\n# States for Future.\n_PENDING = 'PENDING'\n_CANCELLED = 'CANCELLED'\n_FINISHED = 'FINISHED'\n\n\ndef isfuture(obj):\n    \"\"\"Check for a Future.\n\n    This returns True when obj is a Future instance or is advertising\n    itself as duck-type compatible by setting _asyncio_future_blocking.\n    See comment in Future for more details.\n    \"\"\"\n    return (hasattr(obj.__class__, '_asyncio_future_blocking') and\n            obj._asyncio_future_blocking is not None)\n\n\ndef _format_callbacks(cb):\n    \"\"\"helper function for Future.__repr__\"\"\"\n    size = len(cb)\n    if not size:\n        cb = ''\n\n    def format_cb(callback):\n        return format_helpers._format_callback_source(callback, ())\n\n    if size == 1:\n        cb = format_cb(cb[0][0])\n    elif size == 2:\n        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))\n    elif size > 2:\n        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),\n                                        size - 2,\n                                        format_cb(cb[-1][0]))\n    return f'cb=[{cb}]'\n\n\ndef _future_repr_info(future):\n    # (Future) -> str\n    \"\"\"helper function for Future.__repr__\"\"\"\n    info = [future._state.lower()]\n    if future._state == _FINISHED:\n        if future._exception is not None:\n            info.append(f'exception={future._exception!r}')\n        else:\n            # use reprlib to limit the length of the output, especially\n            # for very long strings\n            result = reprlib.repr(future._result)\n            info.append(f'result={result}')\n    if future._callbacks:\n        info.append(_format_callbacks(future._callbacks))\n    if future._source_traceback:\n        frame = future._source_traceback[-1]\n        info.append(f'created at {frame[0]}:{frame[1]}')\n    return info\n\n\n@reprlib.recursive_repr()\ndef _future_repr(future):\n    info = ' '.join(_future_repr_info(future))\n    return f'<{future.__class__.__name__} {info}>'\n", 68], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\nfrom types import GenericAlias\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n    _cancel_message = None\n    # A saved CancelledError for later chaining as an exception context.\n    _cancelled_exc = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events._get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    def __repr__(self):\n        return base_futures._future_repr(self)\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if val:\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def _make_cancelled_error(self):\n        \"\"\"Create the CancelledError to raise if the Future is cancelled.\n\n        This should only be called once when handling a cancellation since\n        it erases the saved context exception value.\n        \"\"\"\n        if self._cancelled_exc is not None:\n            exc = self._cancelled_exc\n            self._cancelled_exc = None\n            return exc\n\n        if self._cancel_message is None:\n            exc = exceptions.CancelledError()\n        else:\n            exc = exceptions.CancelledError(self._cancel_message)\n        exc.__context__ = self._cancelled_exc\n        # Remove the reference since we don't need this anymore.\n        self._cancelled_exc = None\n        return exc\n\n    def cancel(self, msg=None):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self._cancel_message = msg\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception.with_traceback(self._exception_tb)\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if type(exception) is StopIteration:\n            raise TypeError(\"StopIteration interacts badly with generators \"\n                            \"and cannot be raised into a Future\")\n        self._exception = exception\n        self._exception_tb = exception.__traceback__\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args)\n    elif exc_class is concurrent.futures.TimeoutError:\n        return exceptions.TimeoutError(*exc.args)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            if dest_loop.is_closed():\n                return\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events._get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 428], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport types\nimport warnings\nimport weakref\nfrom types import GenericAlias\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom .coroutines import _is_coroutine\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks\n            if futures._get_loop(t) is loop and not t.done()}\n\n\ndef _set_task_name(task, name):\n    if name is not None:\n        try:\n            set_name = task.set_name\n        except AttributeError:\n            warnings.warn(\"Task.set_name() was added in Python 3.8, \"\n                      \"the method support will be mandatory for third-party \"\n                      \"task implementations since 3.13.\",\n                      DeprecationWarning, stacklevel=3)\n        else:\n            set_name(name)\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    #\n    # - Either _fut_waiter is None, and _step() is scheduled;\n    # - or _fut_waiter is some Future, and _step() is *not* scheduled.\n    #\n    # The only transition from the latter to the former is through\n    # _wakeup().  When _fut_waiter is not None, one of its callbacks\n    # must be _wakeup().\n\n    # If False, don't log a message if the task is destroyed whereas its\n    # status is still pending\n    _log_destroy_pending = True\n\n    def __init__(self, coro, *, loop=None, name=None, context=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._num_cancels_requested = 0\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        if context is None:\n            self._context = contextvars.copy_context()\n        else:\n            self._context = context\n\n        self._loop.call_soon(self.__step, context=self._context)\n        _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __repr__(self):\n        return base_tasks._task_repr(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self, msg=None):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n\n        This also increases the task's count of cancellation requests.\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        self._num_cancels_requested += 1\n        # These two lines are controversial.  See discussion starting at\n        # https://github.com/python/cpython/pull/31394#issuecomment-1053545331\n        # Also remember that this is duplicated in _asynciomodule.c.\n        # if self._num_cancels_requested > 1:\n        #     return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel(msg=msg):\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        self._cancel_message = msg\n        return True\n\n    def cancelling(self):\n        \"\"\"Return the count of the task's cancellation requests.\n\n        This count is incremented when .cancel() is called\n        and may be decremented using .uncancel().\n        \"\"\"\n        return self._num_cancels_requested\n\n    def uncancel(self):\n        \"\"\"Decrement the task's count of cancellation requests.\n\n        This should be called by the party that called `cancel()` on the task\n        beforehand.\n\n        Returns the remaining number of cancellation requests.\n        \"\"\"\n        if self._num_cancels_requested > 0:\n            self._num_cancels_requested -= 1\n        return self._num_cancels_requested\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = self._make_cancelled_error()\n            self._must_cancel = False\n        coro = self._coro\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        # Call either coro.throw(exc) or coro.send(None).\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel(msg=self._cancel_message)\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError as exc:\n            # Save the original exception so we can chain it later.\n            self._cancelled_exc = exc\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel(\n                                    msg=self._cancel_message):\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None, context=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    if context is None:\n        # Use legacy API if context is not needed\n        task = loop.create_task(coro)\n    else:\n        task = loop.create_task(coro, context=context)\n\n    _set_task_name(task, name)\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures or Tasks given by fs to complete.\n\n    The fs iterable must not be empty.\n\n    Coroutines will be wrapped in Tasks.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of Tasks/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    fs = set(fs)\n\n    if any(coroutines.iscoroutine(f) for f in fs):\n        raise TypeError(\"Passing coroutines is forbidden, use tasks explicitly.\")\n\n    loop = events.get_running_loop()\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    This function is a coroutine.\n    \"\"\"\n    loop = events.get_running_loop()\n\n    if timeout is None:\n        return await fut\n\n    if timeout <= 0:\n        fut = ensure_future(fut, loop=loop)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut, loop=loop)\n        try:\n            return fut.result()\n        except exceptions.CancelledError as exc:\n            raise exceptions.TimeoutError() from exc\n\n    waiter = loop.create_future()\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    cb = functools.partial(_release_waiter, waiter)\n\n    fut = ensure_future(fut, loop=loop)\n    fut.add_done_callback(cb)\n\n    try:\n        # wait until the future completes or the timeout\n        try:\n            await waiter\n        except exceptions.CancelledError:\n            if fut.done():\n                return fut.result()\n            else:\n                fut.remove_done_callback(cb)\n                # We must ensure that the task is not running\n                # after wait_for() returns.\n                # See https://bugs.python.org/issue32751\n                await _cancel_and_wait(fut, loop=loop)\n                raise\n\n        if fut.done():\n            return fut.result()\n        else:\n            fut.remove_done_callback(cb)\n            # We must ensure that the task is not running\n            # after wait_for() returns.\n            # See https://bugs.python.org/issue32751\n            await _cancel_and_wait(fut, loop=loop)\n            # In case task cancellation failed with some\n            # exception, we should re-raise it\n            # See https://bugs.python.org/issue40607\n            try:\n                return fut.result()\n            except exceptions.CancelledError as exc:\n                raise exceptions.TimeoutError() from exc\n    finally:\n        timeout_handle.cancel()\n\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut, loop):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\ndef as_completed(fs, *, timeout=None):\n    \"\"\"Return an iterator whose values are coroutines.\n\n    When waiting for the yielded coroutines you'll get the results (or\n    exceptions!) of the original Futures (or coroutines), in the order\n    in which and as soon as they complete.\n\n    This differs from PEP 3148; the proper way to use this is:\n\n        for f in as_completed(fs):\n            result = await f  # The 'await' may raise.\n            # Use result.\n\n    If a timeout is specified, the 'await' will raise\n    TimeoutError when the timeout occurs before all Futures are done.\n\n    Note: The futures 'f' are not necessarily members of fs.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect an iterable of futures, not {type(fs).__name__}\")\n\n    from .queues import Queue  # Import here to avoid circular import problem.\n    done = Queue()\n\n    loop = events._get_event_loop()\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\n    timeout_handle = None\n\n    def _on_timeout():\n        for f in todo:\n            f.remove_done_callback(_on_completion)\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\n        todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _on_completion(f):\n        if not todo:\n            return  # _on_timeout() was here first.\n        todo.remove(f)\n        done.put_nowait(f)\n        if not todo and timeout_handle is not None:\n            timeout_handle.cancel()\n\n    async def _wait_for_one():\n        f = await done.get()\n        if f is None:\n            # Dummy value from _on_timeout().\n            raise exceptions.TimeoutError\n        return f.result()  # May raise f.exception().\n\n    for f in todo:\n        f.add_done_callback(_on_completion)\n    if todo and timeout is not None:\n        timeout_handle = loop.call_later(timeout, _on_timeout)\n    for _ in range(len(todo)):\n        yield _wait_for_one()\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    loop = events.get_running_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    return _ensure_future(coro_or_future, loop=loop)\n\n\ndef _ensure_future(coro_or_future, *, loop=None):\n    if futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                            'the one specified as the loop argument')\n        return coro_or_future\n    called_wrap_awaitable = False\n    if not coroutines.iscoroutine(coro_or_future):\n        if inspect.isawaitable(coro_or_future):\n            coro_or_future = _wrap_awaitable(coro_or_future)\n            called_wrap_awaitable = True\n        else:\n            raise TypeError('An asyncio.Future, a coroutine or an awaitable '\n                            'is required')\n\n    if loop is None:\n        loop = events._get_event_loop(stacklevel=4)\n    try:\n        return loop.create_task(coro_or_future)\n    except RuntimeError:\n        if not called_wrap_awaitable:\n            coro_or_future.close()\n        raise\n\n\n@types.coroutine\ndef _wrap_awaitable(awaitable):\n    \"\"\"Helper for asyncio.ensure_future().\n\n    Wraps awaitable (an object with __await__) into a coroutine\n    that will later be wrapped in a Task by ensure_future().\n    \"\"\"\n    return (yield from awaitable.__await__())\n\n_wrap_awaitable._is_coroutine = _is_coroutine\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop):\n        assert loop is not None\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self, msg=None):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel(msg=msg):\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if not coros_or_futures:\n        loop = events._get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer is None or outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\n                    # will *raise* a CancelledError instead of returning it.\n                    # Also, since we're adding the exception return value\n                    # to 'results' instead of raising it, don't bother\n                    # setting __context__.  This also lets us preserve\n                    # calling '_make_cancelled_error()' at most once.\n                    res = exceptions.CancelledError(\n                        '' if fut._cancel_message is None else\n                        fut._cancel_message)\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    loop = None\n    outer = None  # bpo-46672\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = _ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    return outer\n\n\ndef shield(arg):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        task = asyncio.create_task(something())\n        res = await shield(task)\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        task = asyncio.create_task(something())\n        try:\n            res = await shield(task)\n        except CancelledError:\n            res = None\n\n    Save a reference to tasks passed to this function, to avoid\n    a task disappearing mid-execution. The event loop only keeps\n    weak references to tasks. A task that isn't referenced elsewhere\n    may get garbage collected at any time, even before it's done.\n    \"\"\"\n    inner = _ensure_future(arg)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\n# WeakSet containing all alive tasks.\n_all_tasks = weakref.WeakSet()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register a new task in asyncio as executed by loop.\"\"\"\n    _all_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a task.\"\"\"\n    _all_tasks.discard(task)\n\n\n_py_register_task = _register_task\n_py_unregister_task = _unregister_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n\n\ntry:\n    from _asyncio import (_register_task, _unregister_task,\n                          _enter_task, _leave_task,\n                          _all_tasks, _current_tasks)\nexcept ImportError:\n    pass\nelse:\n    _c_register_task = _register_task\n    _c_unregister_task = _unregister_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n", 980], "/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py": ["import asyncio\n\n\nasync def main():\n    await asyncio.sleep(1)\n    print(1)\n\n\nasyncio.run(main())\n", 9], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py": ["\"\"\"Redo the builtin repr() (representation) but with limits on most sizes.\"\"\"\n\n__all__ = [\"Repr\", \"repr\", \"recursive_repr\"]\n\nimport builtins\nfrom itertools import islice\nfrom _thread import get_ident\n\ndef recursive_repr(fillvalue='...'):\n    'Decorator to make a repr function return fillvalue for a recursive call'\n\n    def decorating_function(user_function):\n        repr_running = set()\n\n        def wrapper(self):\n            key = id(self), get_ident()\n            if key in repr_running:\n                return fillvalue\n            repr_running.add(key)\n            try:\n                result = user_function(self)\n            finally:\n                repr_running.discard(key)\n            return result\n\n        # Can't use functools.wraps() here because of bootstrap issues\n        wrapper.__module__ = getattr(user_function, '__module__')\n        wrapper.__doc__ = getattr(user_function, '__doc__')\n        wrapper.__name__ = getattr(user_function, '__name__')\n        wrapper.__qualname__ = getattr(user_function, '__qualname__')\n        wrapper.__annotations__ = getattr(user_function, '__annotations__', {})\n        return wrapper\n\n    return decorating_function\n\nclass Repr:\n\n    def __init__(self):\n        self.fillvalue = '...'\n        self.maxlevel = 6\n        self.maxtuple = 6\n        self.maxlist = 6\n        self.maxarray = 5\n        self.maxdict = 4\n        self.maxset = 6\n        self.maxfrozenset = 6\n        self.maxdeque = 6\n        self.maxstring = 30\n        self.maxlong = 40\n        self.maxother = 30\n\n    def repr(self, x):\n        return self.repr1(x, self.maxlevel)\n\n    def repr1(self, x, level):\n        typename = type(x).__name__\n        if ' ' in typename:\n            parts = typename.split()\n            typename = '_'.join(parts)\n        if hasattr(self, 'repr_' + typename):\n            return getattr(self, 'repr_' + typename)(x, level)\n        else:\n            return self.repr_instance(x, level)\n\n    def _repr_iterable(self, x, level, left, right, maxiter, trail=''):\n        n = len(x)\n        if level <= 0 and n:\n            s = self.fillvalue\n        else:\n            newlevel = level - 1\n            repr1 = self.repr1\n            pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)]\n            if n > maxiter:\n                pieces.append(self.fillvalue)\n            s = ', '.join(pieces)\n            if n == 1 and trail:\n                right = trail + right\n        return '%s%s%s' % (left, s, right)\n\n    def repr_tuple(self, x, level):\n        return self._repr_iterable(x, level, '(', ')', self.maxtuple, ',')\n\n    def repr_list(self, x, level):\n        return self._repr_iterable(x, level, '[', ']', self.maxlist)\n\n    def repr_array(self, x, level):\n        if not x:\n            return \"array('%s')\" % x.typecode\n        header = \"array('%s', [\" % x.typecode\n        return self._repr_iterable(x, level, header, '])', self.maxarray)\n\n    def repr_set(self, x, level):\n        if not x:\n            return 'set()'\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, '{', '}', self.maxset)\n\n    def repr_frozenset(self, x, level):\n        if not x:\n            return 'frozenset()'\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'frozenset({', '})',\n                                   self.maxfrozenset)\n\n    def repr_deque(self, x, level):\n        return self._repr_iterable(x, level, 'deque([', '])', self.maxdeque)\n\n    def repr_dict(self, x, level):\n        n = len(x)\n        if n == 0:\n            return '{}'\n        if level <= 0:\n            return '{' + self.fillvalue + '}'\n        newlevel = level - 1\n        repr1 = self.repr1\n        pieces = []\n        for key in islice(_possibly_sorted(x), self.maxdict):\n            keyrepr = repr1(key, newlevel)\n            valrepr = repr1(x[key], newlevel)\n            pieces.append('%s: %s' % (keyrepr, valrepr))\n        if n > self.maxdict:\n            pieces.append(self.fillvalue)\n        s = ', '.join(pieces)\n        return '{%s}' % (s,)\n\n    def repr_str(self, x, level):\n        s = builtins.repr(x[:self.maxstring])\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = builtins.repr(x[:i] + x[len(x)-j:])\n            s = s[:i] + self.fillvalue + s[len(s)-j:]\n        return s\n\n    def repr_int(self, x, level):\n        s = builtins.repr(x) # XXX Hope this isn't too slow...\n        if len(s) > self.maxlong:\n            i = max(0, (self.maxlong-3)//2)\n            j = max(0, self.maxlong-3-i)\n            s = s[:i] + self.fillvalue + s[len(s)-j:]\n        return s\n\n    def repr_instance(self, x, level):\n        try:\n            s = builtins.repr(x)\n            # Bugs in x.__repr__() can cause arbitrary\n            # exceptions -- then make up something\n        except Exception:\n            return '<%s instance at %#x>' % (x.__class__.__name__, id(x))\n        if len(s) > self.maxother:\n            i = max(0, (self.maxother-3)//2)\n            j = max(0, self.maxother-3-i)\n            s = s[:i] + self.fillvalue + s[len(s)-j:]\n        return s\n\n\ndef _possibly_sorted(x):\n    # Since not all sequences of items can be sorted and comparison\n    # functions may raise arbitrary exceptions, return an unsorted\n    # sequence in that case.\n    try:\n        return sorted(x)\n    except Exception:\n        return list(x)\n\naRepr = Repr()\nrepr = aRepr.repr\n", 167], "/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py": ["import linecache\nimport reprlib\nimport traceback\n\nfrom . import base_futures\nfrom . import coroutines\n\n\ndef _task_repr_info(task):\n    info = base_futures._future_repr_info(task)\n\n    if task.cancelling() and not task.done():\n        # replace status\n        info[0] = 'cancelling'\n\n    info.insert(1, 'name=%r' % task.get_name())\n\n    coro = coroutines._format_coroutine(task._coro)\n    info.insert(2, f'coro=<{coro}>')\n\n    if task._fut_waiter is not None:\n        info.insert(3, f'wait_for={task._fut_waiter!r}')\n    return info\n\n\n@reprlib.recursive_repr()\ndef _task_repr(task):\n    info = ' '.join(_task_repr_info(task))\n    return f'<{task.__class__.__name__} {info}>'\n\n\ndef _task_get_stack(task, limit):\n    frames = []\n    if hasattr(task._coro, 'cr_frame'):\n        # case 1: 'async def' coroutines\n        f = task._coro.cr_frame\n    elif hasattr(task._coro, 'gi_frame'):\n        # case 2: legacy coroutines\n        f = task._coro.gi_frame\n    elif hasattr(task._coro, 'ag_frame'):\n        # case 3: async generators\n        f = task._coro.ag_frame\n    else:\n        # case 4: unknown objects\n        f = None\n    if f is not None:\n        while f is not None:\n            if limit is not None:\n                if limit <= 0:\n                    break\n                limit -= 1\n            frames.append(f)\n            f = f.f_back\n        frames.reverse()\n    elif task._exception is not None:\n        tb = task._exception.__traceback__\n        while tb is not None:\n            if limit is not None:\n                if limit <= 0:\n                    break\n                limit -= 1\n            frames.append(tb.tb_frame)\n            tb = tb.tb_next\n    return frames\n\n\ndef _task_print_stack(task, limit, file):\n    extracted_list = []\n    checked = set()\n    for f in task.get_stack(limit=limit):\n        lineno = f.f_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        if filename not in checked:\n            checked.add(filename)\n            linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        extracted_list.append((filename, lineno, name, line))\n\n    exc = task._exception\n    if not extracted_list:\n        print(f'No stack for {task!r}', file=file)\n    elif exc is not None:\n        print(f'Traceback for {task!r} (most recent call last):', file=file)\n    else:\n        print(f'Stack for {task!r} (most recent call last):', file=file)\n\n    traceback.print_list(extracted_list, file=file)\n    if exc is not None:\n        for line in traceback.format_exception_only(exc.__class__, exc):\n            print(line, file=file, end='')\n", 92]}, "functions": {"__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:49)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 49], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:663)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 663], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:1438)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py", 1438], "_init_event_loop_policy (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:750)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 750], "get_event_loop_policy (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:758)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 758], "_is_debug_mode (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:11)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py", 11], "is_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:696)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 696], "set_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1943)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 1943], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:37)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 37], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:389)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 389], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:63)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 63], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:209)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 209], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:509)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 509], "_acquireLock (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:228)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py", 228], "disable (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1319)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py", 1319], "getEffectiveLevel (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1720)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py", 1720], "_releaseLock (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:237)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py", 237], "isEnabledFor (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1734)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py", 1734], "debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py:1467)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py", 1467], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:220)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", 220], "socketpair (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:595)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", 595], "_check_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:517)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 517], "get_debug (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1940)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 1940], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:31)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 31], "get_map (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:272)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 272], "_fileobj_to_fd (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:21)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 21], "_fileobj_lookup (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:215)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 215], "__getitem__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:69)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 69], "get_key (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:180)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 180], "register (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:234)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 234], "register (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:516)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 516], "_add_reader (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:261)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", 261], "_make_self_pipe (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:105)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", 105], "update (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py:289)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py", 289], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py:104)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/weakref.py", 104], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:49)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", 49], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:63)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py", 63], "new_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:689)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 689], "new_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:804)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 804], "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:682)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 682], "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:1449)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py", 1449], "set_event_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:799)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 799], "_lazy_init (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:131)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 131], "__enter__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:58)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 58], "iscoroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:34)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py", 34], "_call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:780)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 780], "call_soon (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:751)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 751], "add (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:85)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 85], "create_task (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:429)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 429], "current_thread (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1446)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py", 1446], "main_thread (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1590)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py", 1590], "_missing_ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1180)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py", 1180], "__new__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:1091)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py", 1091], "__call__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py:686)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/enum.py", 686], "_int_to_enum (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:24)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py", 24], "getsignal (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:60)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py", 60], "_enum_to_int (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:34)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py", 34], "signal (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py:54)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/signal.py", 54], "_check_running (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:586)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 586], "isfuture (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:14)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py", 14], "_get_loop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:299)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", 299], "_ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:652)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", 652], "ensure_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:644)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", 644], "_set_coroutine_origin_tracking (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1925)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 1925], "select (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:553)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 553], "_process_events (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:733)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", 733], "time (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:700)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 700], "create_future (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:425)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 425], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:103)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 103], "call_at (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:733)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 733], "call_later (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:709)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 709], "sleep (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:627)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", 627], "main (/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py:4)": ["/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py", 4], "_run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:78)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 78], "_run_once (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1845)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 1845], "_set_result_unless_cancelled (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:311)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py", 311], "_timer_handle_cancelled (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:1840)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 1840], "cancel (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:64)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 64], "cancel (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py:147)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", 147], "stop (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:655)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 655], "_run_until_complete_cb (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:180)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 180], "run_forever (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:593)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 593], "run_until_complete (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:617)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 617], "repr_instance (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:143)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py", 143], "repr1 (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:55)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py", 55], "repr (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:52)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py", 52], "_future_repr_info (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py:45)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_futures.py", 45], "get_name (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:53)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py", 53], "_format_coroutine (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py:50)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/coroutines.py", 50], "_task_repr_info (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py:9)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py", 9], "_task_repr (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py:26)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_tasks.py", 26], "wrapper (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py:15)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/reprlib.py", 15], "run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:86)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 86], "_remove (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:39)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 39], "__len__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:72)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 72], "__init__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:17)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 17], "__enter__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:21)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 21], "_commit_removals (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:53)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 53], "__exit__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:27)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 27], "__iter__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py:63)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/_weakrefset.py", 63], "<setcomp> (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:61)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", 61], "all_tasks (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:42)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", 42], "_cancel_all_tasks (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:193)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 193], "shutdown_asyncgens (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:539)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 539], "shutdown_default_executor (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:564)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 564], "is_closed (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:686)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 686], "unregister (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:247)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 247], "unregister (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:532)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 532], "_remove_reader (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:277)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", 277], "_real_close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:495)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", 495], "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:499)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", 499], "_close_self_pipe (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:97)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", 97], "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:663)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 663], "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:268)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 268], "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:578)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py", 578], "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py:86)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", 86], "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py:67)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/unix_events.py", 67], "close (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:65)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 65], "__del__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:690)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", 690], "__exit__ (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:62)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 62], "run (/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:160)": ["/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py", 160], "<module> (/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py:1)": ["/Users/networkcavalry/Documents/GitHub/Framework/Language/Python/py/viztracers/tracer_demo.py", 1]}}}