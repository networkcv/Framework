## UNIX下5种I/O模型

一个输入操作通常包括两个不同的阶段。

1. 等待数据准备好
2. 将数据从内核缓冲区复制到用户空间

### 阻塞式I/O

阻塞I/O模型：最常用的I/O模型就是阻塞I/O模型，缺省情形下，所有文件操作都是阻塞的。在进程空间中调用recvfrom，其系统调用直到数据包到达且被复制到应用进程的缓冲区中或者发生错误时才返回（最常见的错误是系统调用被信号中断），在此期间一直会等待，进程在从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此被称为阻塞I/O模型。

<img src="https://pic.networkcv.top/2022/01/19/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_6226df70-f84a-4e7a-9665-2fc198af5bf0.png" alt="企业微信截图_6226df70-f84a-4e7a-9665-2fc198af5bf0" style="zoom:50%;" />

### 非阻塞式I/O

非阻塞I/O模型：进程把一个套接字设置成非阻塞是在通知内核：当系统调用recvfrom从应用层到内核的时候，如果该缓冲区没有数据的话，就直接返回一个EWOULDBLOCK错误，而不是让该进程睡眠。一般都对非阻塞I/O模型进行轮询检查这个状态，看内核是不是有数据到来。

<img src="https://pic.networkcv.top/2022/01/19/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_bc52f7a8-de81-4691-bbac-0f6a9b0c3ce6.png" alt="企业微信截图_bc52f7a8-de81-4691-bbac-0f6a9b0c3ce6" style="zoom:50%;" />

前两次调用recvfrom 时没有数据可返回，因此内核转而立即返回一个EWOULDBLOCK 错误。第三次调用recvfrom 时已有一个数据报准备好，它被复制到应用进程缓冲区，于是recvfrom 成功返回。我们接着处理数据。

当一个应用进程像这样对一个非阻塞描述符循环调用recvfrom 时，我们称之为轮询 （polling）。应用进程持续轮询内核，以查看某个操作是否就绪。这么做往往耗费大量CPU时间，不过这种模型偶尔也会遇到，通常是在专门提供某一种功能的系统中才有

### I/O复用（select、poll和epoll）

I/O复用模型：Linux提供select/poll，进程通过将一个或多个fd传递给select或poll系统调用，阻塞在select操作上，而不是阻塞在真正的I/O系统调用上。这样select/poll可以帮我们侦测多个fd是否处于就绪状态，等待数据报套接字变为可读。当select 返回套接字可读这一条件时，我们调用recvfrom 把所读数据报复制到应用进程缓冲区。

select/poll是顺序扫描fd是否就绪，而且支持的fd数量有限（默认1024），因此它的使用受到了一些制约。Linux还提供了一个epoll系统调用，epoll使用基于事件驱动方式代替顺序扫描，因此性能更高。当有fd就绪时，立即回调函数rollback。

<img src="https://pic.networkcv.top/2022/01/19/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_a2b6a3c2-fa4d-43f0-adee-15ffb24c2aac.png" alt="企业微信截图_a2b6a3c2-fa4d-43f0-adee-15ffb24c2aac" style="zoom:50%;" />

与I/O复用密切相关的另一种I/O模型是在多线程中使用阻塞式I/O，这里暂称其为伪异步I/O。这种模型与上述模型极为相似，但它没有使用select 阻塞在多个文件描述符上，而是使用多个线程（每个文件描述符一个线程），这样每个线程都可以自由地调用诸如recvfrom 之类的阻塞式I/O系统调用了。

由于使用的是阻塞式I/O，当调用OutputStream的write方法写输出流的时候，它将会被阻塞，直到所有要发送的字节全部写入完毕，或者发生异常。这其中存在一定风险，当消息的接收方处理缓慢的时候，将不能及时地从TCP缓冲区读取数据，由接收方控制的滑动窗口也会越来越小，同时将会导致发送方的TCP window size不断减小，直到为0，双方处于Keep-Alive状态，消息发送方将不能再向TCP缓冲区写入消息，这时如果采用的是同步阻塞I/O，write操作将会被无限期阻塞，直到TCP window size大于0或者发生I/O异常。

我们了解到读和写操作都是同步阻塞的，阻塞的时间取决于对方I/O线程的处理速度和网络I/O的传输速度。本质上来讲，我们无法保证生产环境的网络状况和对端的应用程序能足够快，如果我们的应用程序依赖对方的处理速度，它的可靠性就非常差。也许在实验室进行的性能测试结果令人满意，但是一旦上线运行，面对恶劣的网络环境和良莠不齐的第三方系统，问题就会如火山一样喷发。

伪异步I/O实际上仅仅是对之前阻塞I/O线程模型的一个简单优化，它无法从根本上解决同步I/O导致的通信线程阻塞问题。

### 信号驱动式I/O

信号驱动I/O模型：首先开启套接口信号驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数（此系统调用立即返回，进程继续工作，它是非阻塞的）。当数据准备就绪时，就为该进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主循环函数处理数据。

<img src="https://pic.networkcv.top/2022/01/19/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_f7e58802-7544-4430-b4e3-c98447a6284a.png" alt="企业微信截图_f7e58802-7544-4430-b4e3-c98447a6284a" style="zoom:50%;" />

### 异步I/O

异步I/O：告知内核启动某个操作，并让内核在整个操作完成后（包括将数据从内核复制到用户自己的缓冲区）通知我们。这种模型与信号驱动模型的主要区别是：信号驱动I/O由内核通知我们何时可以开始一个I/O操作；异步I/O模型由内核通知我们I/O操作何时已经完成。

<img src="https://pic.networkcv.top/2022/01/19/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_fc286666-9240-4fd7-acbc-3878bf4839f9.png" alt="企业微信截图_fc286666-9240-4fd7-acbc-3878bf4839f9" style="zoom:50%;" />



## 同步I/O和异步I/O对比

POSIX把这两个术语定义如下：

同步 I/O操作 （synchronous I/O opetation）导致请求进程阻塞，直到I/O操作完成；
异步 I/O操作 （asynchronous I/O opetation）不导致请求进程阻塞。
根据上述定义，我们的前4种模型——阻塞式I/O模型、非阻塞式I/O模型、I/O复用模型和信号驱动式I/O模型都是同步I/O模型，因为其中真正的I/O操作（recvfrom ）将阻塞进程。只有异步I/O模型与POSIX定义的异步I/O相匹配。

## **为什么Netty使用NIO而不是AIO？**

1. Netty不看重Windows上的使用，在Linux系统上，AIO的底层实现仍使用EPOLL，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化
2. Netty整体架构是reactor模型, 而AIO是proactor模型, 混合在一起会非常混乱,把AIO也改造成reactor模型看起来是把epoll绕个弯又绕回来
3. AIO还有个缺点是接收数据需要预先分配缓存, 而不是NIO那种需要接收时才需要分配缓存, 所以对连接数量非常大但流量小的情况, 内存浪费很多
4. Linux上AIO不够成熟，处理回调结果速度跟不到处理需求，比如外卖员太少，顾客太多，供不应求，造成处理速度有瓶颈

## protobuf 优点

1. 在谷歌内部长期使用，产品成熟度高；
2. 跨语言、支持多种语言，包括C++、Java和Python；
3. 编码后的消息更小，更加有利于存储和传输；
4. 编解码的性能非常高；
5. 支持不同协议版本的前向兼容；



## Netty的历史

### Netty3

1. 创建过多对象，在JVM堆上创建，然后进行内存复制到直接内存里，再交给socket，虽然会有垃圾回收器来处理，但也会给服务器带来系统压力。
2. 只使用了Java 提供的NIO接口，无法满足一些个性化的需求，比如只在Linux上才支持的某些特性。
3. 线程模型也不合理。InBound发生在同一个线程里，但OutBound发生了调用线程里。

#### Netty4

1. 产生更少的内存垃圾。
2. 针对Linux操作系统做了传输层的优化。通过JNI来实现。
3. 实现了高性能的BufferPool，用于直接内存。
4. InBound和OutBound发生在同一个线程里。

## Netty核心组件

Channel 是对 Socket 的抽象，Channel 可以写操作，数据会走到 socket，然后调用 write 系统操作将数据发送出去。 

每个 Channel 都拥有一个 ChannelPipline，ChannelPipline 是一个包含不同 ChannelHandler 的双向链表。

